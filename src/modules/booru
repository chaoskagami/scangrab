#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# This is another generic; also, there's a bit of extra here. You can provide either a URL
# or a known Booru's name, and use the filter for a tag.

# e.g. scangrab booru danbooru touhou
# will fetch everything from the touhou tag. (which is expensive. How many damned pages are there?)

# Past that, you can also specify how many pages, like so:
#   scangrab booru danbooru touhou:50
#   (downloads 50 pages)
# Or how many images you want:
#   scangrab booru danbooru touhou::100
#   (downloads 100 images)
# You can also specify ranges if you want to get specific; like so:
#   scangrab booru danbooru touhou:20-30:10-150
#   (starts at page twenty, downloads till page 30. Skips 10 images, and downloads 140.
# Because of this, it's recommended to avoid passing URLs. They get cut up for parameters, anyways.

# Danbooru also functions differently; it is the only grabber which DOES NOT zip up things.
# By default, things are stored in folders by the tag you grab by, and named according to the image's
# MD5 (as danbooru does.) A file is stored with it that has meta-info from danbooru.

booru_longname="*Booru"
booru_url="Generic"
booru_state=2
booru_filt=1

auto_booru() {
	if [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
		# Danbooru / Safebooru
		return 1
	elif [ "$1" = "danbooru" ]; then
		# Danbooru direct.
		return 1
	elif [ "$1" = "testbooru" ]; then
		# Danbooru direct.
		return 1
	elif [ "$1" = "safebooru" ]; then
		# Safebooru direct.
		return 1
	fi

	return 0
}

# Because the syntax is hell.
booru_help() {
	echo "Usage: BOORU_NAME TAGLIST SPEC [FOLDERNAME]"
	echo "BOORU_NAME:"
	echo "  The following boorus are currently supported:"
	echo "    danbooru   (danbooru.donmai.us)"
	echo "    safebooru  (safebooru.donmai.us)"
	echo "    testbooru  (testbooru.donmai.us)"
	echo ""
	echo "TAGLIST:"
	echo "    tagname                   - Specify a tag"
	echo "    tagname1,tagname2,...     - Specify multiple tags"
	echo "    pool:poolname             - Download from a pool"
	echo ""
	echo "  Specifiers:"
	echo "      art:tagname               - Search by artist"
	echo "      char:tagname              - Search by character"
	echo "      copy:tagname              - Search by copyright"
	echo "      user:username             - Anything uploaded by username"
	echo "      fav:username              - Anything favorited by username"
	echo "      ordfav:username           - Anything favorited by username (chrono)"
	echo "      md5:hash                  - Posts with a specific MD5 hash."
	echo "      rating:rate               - Safety rating, e.g. s, q, e"
	echo "      See http://safebooru.donmai.us/wiki_pages/43049 for a more"
	echo "      exhaustive reference."
	echo ""
	echo "  Modifers:"
	echo "      tag1,-tag2        - Anything matching tag1, but not tag2"
	echo "      ~tag1,~tag2       - Anything marked tag1 or tag2"
	echo "      tag1,tag2         - Anything marked with both tag1 AND tag2"
	echo "      *tag*             - All tags containing 'tag'"
	echo ""
	echo "SPEC:"
	echo "    p10                - Download page 10."
	echo "    l50,p10            - 50 images per page, download page 10. (This is a"
	echo "                         lowercase L if your font doesn't distinguish)"
	echo "    i100               - Download 100 images"
	echo "    i5-100             - Download images 5-100"
	echo "    p10-50             - Download from page 10 to 50 inclusive"
	echo "    l50,p10,i100       - Pages with 50 images, save the first 100 images,"
	echo "                         starting at page 10"
	echo "    l50,p5-10,i100     - Pages with 50 images, save the first 100 images,"
	echo "                         starting at page 5"
	echo "                         (This syntax is legal, but -10 is ignored.)"
	echo "    pages:50           - Download 50 pages, Long syntax"
	echo "    images:100         - Download 100 images, Long syntax"
	echo "    limit:50           - 50 images on a page, Long syntax"
	echo "    all                - Download ALL images for tag."
	echo "    resume             - Resume a previous Ctrl+C'd download,"
	echo "    r                  - Short for resume"
	echo ""
	echo "  Minor note is that *technically* limit:N/lN is a tag."
	echo ""
	echo "FOLDERNAME (optional):"
	echo "    Output folder. If not specified, it uses the format:"
	echo "    'NAME - TAGLIST'"
	echo ""
	echo "Also, please note that the following sites are either alternate"
	echo "source bases or NOT boorus, but will eventually be supported"
	echo "here due to some structural similiarities:"
	echo ""
	echo "    Moebooru-based (https://github.com/moebooru/moebooru)"
	echo "        yande.re (yandere)"
	echo "        konachan.com (konachan_g) / konachan.net (konachan)"
	echo ""
	echo "    Shimmie-based (https://github.com/shish/shimmie2)"
	echo "        shimmie.katawa-shoujo.com (mishimme)"
	echo ""
	echo "    Custom / Needs research"
	echo "        zerochan.net (zerochan)"
	echo ""
	echo "    All the same, dunno what they run"
	echo "        *.booru.net (Many different things)"
	echo "        safebooru.net (Not the same as safebooru)"
	echo "        gelbooru.net"
	exit 1
}

# Range to download.
declare -a pagerange
declare -a imagerange
declare -a tagarray
declare -a sizearray
declare -a final
TAGCOUNT=0
page_index=0

# base URL
# Tags
# Images per page. Blank if no.
# page=
# Page Number
final=("$source/posts?tags=" "" "" "&page=" 1)

RESUME=0

pagerange=(1 1)

# Paged mode by default.
# 1=paged, 2=images
mode=1
source=""
name=""

is_pool=0

# Computes final tags.
booru_tag_crunch() {
	# Tag array can be pasted together.
	for (( i=0 ; i < ${#tagarray[@]} ; i++ )); do
		if [ ! "$i" = "0" ] && [ ! "$((i + 1))" = "${#tagarray}" ]; then
			final[1]="${final[1]}+"
		fi

		# This is a pool. Ignore the other things.
		if [[ "${tagarray[i]}" == pool:* ]]; then
			echo "[Booru] This is apparently a pool."
			is_pool=1
			TAGCOUNT=1
			final[1]="$(echo ${tagarray[i]} | sed -e 's|pool:||g')"
			break
		fi

		tag="$(echo ${tagarray[i]} | sed -e 's|:|%3A|g')"

		final[1]="${final[1]}${tag}"
		if [[ ! "$tag" == *:* ]]; then
			TAGCOUNT=$((TAGCOUNT + 1))
		fi
	done
}

booru_size_crunch() {
	# No settings for a pool.
	if [ $is_pool = 1 ]; then
		return
	fi

	for (( i=0 ; i < ${#sizearray[@]} ; i++ )); do
		if [[ "${sizearray[i]}" == pages:* ]] || [[ "${sizearray[i]}" == p* ]]; then
			pagerange=($(echo "${sizearray[i]}" | tr -d 'p' | tr '-' ' '))
			if [ "${pagerange[1]}" = "" ]; then
				pagerange[1]=${pagerange[0]}
			fi
			final[4]=${pagerange[0]}
		elif [[ "${sizearray[i]}" == limit:* ]] || [[ "${sizearray[i]}" == l* ]]; then
			if [[ ! "${sizearray[i]}" == "limit:*" ]]; then
				final[2]="+$(echo ${sizearray[i]} | sed 's|l|limit:|')"
			else
				final[2]="+${sizearray[i]}"
			fi
			final[2]="$(echo ${final[2]} | sed -e 's|:|%3A|g')"
		elif [[ "${sizearray[i]}" == images:* ]] || [[ "${sizearray[i]}" == i* ]]; then
			imagerange=($(echo "${sizearray[i]}" | tr -d 'i' | tr '-' ' '))
			if [ "${imagerange[1]}" = "" ]; then
				imagerange[1]=${imagerange[0]}
				imagerange[0]=1
			fi
			mode=2
		elif [[ "${sizearray[i]}" == "all" ]]; then
			# 1st page.
			pagerange[0]=1
			# Arbitrarily high number that will never be reached. Maybe. Dunno.
			# Point is, we stop when no images can be extracted from the page.
			pagerange[1]=$MAX_INT

			final[4]=${pagerange[0]}
		elif [[ "${sizearray[i]}" == "resume" ]] || [[ "${sizearray[i]}" == "r" ]]; then
			# Resume.
			RESUME=1
		fi
	done
}

dl_booru_page() {
	url="$1"
	name_out="$2"

	FETCH_RESULT=1
	while [ ! $FETCH_RESULT = 0 ]; do
		fetch "$url" "$name_out"

		if [ ! $FETCH_RESULT = 0 ]; then
			FAILS=$((FAILS + 1))
			message=" :<"
			if [ ! $FAILS = 0 ]; then
				spinner_done "!!!"

				echo -en "[Booru] Server refused us. Waiting a bit... "
				SLEEP=$((10 * FAILS))
				while [ ! $SLEEP = 0 ]; do
					sleep 1s
					spinner "${SLEEP}s"
					SLEEP=$((SLEEP - 1))
				done

				spinner_done "DONE"

				echo -en "[Booru] Continuing... "
			fi
		fi

		# This is for safety.
		# A lot of boorus get mad at you for hammering the database repeatedly and punish you
		# by denying connections for a while. This ensures we aren't hammering *too* hard.
		sleep 1s
	done
	FAILS=0
}

dl_booru() {
	if [ "$2" = "" ] && [ "$3" = "" ] || [ "$1" = "" ]; then
		booru_syntax_help
	fi

	if [ -n "$(echo $1 | grep 'safebooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "safebooru" ]; then
		# Safebooru directly. NEVER MOVE THIS CHECK BENEATH DANBOORU. THE GREP FOR DANBOORU WILL MATCH.
		source="http://safebooru.donmai.us"
		name="danbooru"
		site="safebooru"
	elif [ -n "$(echo $1 | grep 'testbooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "testbooru" ]; then
		# Testbooru (which has four images and is a good test.) NEVER MOVE THIS CHECK BENEATH DANBOORU. THE GREP FOR DANBOORU WILL MATCH.
		source="http://testbooru.donmai.us"
		name="danbooru"
		site="testbooru"
	elif [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "danbooru" ]; then
		# Danbooru
		source="http://danbooru.donmai.us"
		name="danbooru"
		site="danbooru"
	else
		# Unknown, but apparently a booru.
		source="$1"
		echo "[Booru] This type of Booru isn't specified as supported."
		echo "[Booru] It may work, depending on whether the autodetect picks up a valid scheme,"
		echo "[Booru] or it may completely bomb out. If it works, please submit a bug report"
		echo "[Booru] so I can add it to the supported list."
		name="unknown"
		site="$source"
	fi

	booru_${name}_init

	shift

	tagarray=($(echo "$1" | tr ',' ' '))
	sizearray=($(echo "$2" | tr ',' ' '))

	booru_tag_crunch
	booru_size_crunch

	echo "[Booru] Info:"
	echo "[Booru]   Fetching URL $(echo ${final[0]}${final[1]}${final[2]}${final[3]} | tr -d ' ')"
	echo -n "[Booru]   Will download "
	if [ $mode = 1 ]; then
		if [ "${pagerange[0]}" = "${pagerange[1]}" ]; then
			echo "page ${pagerange[0]}."
		else
			TO=${pagerange[1]}
			if [ $TO = $MAX_INT ]; then
				TO="Max"
			fi
			echo "pages ${pagerange[0]} to $TO."
		fi
	else
		# XXX - Why in god's name would a user do this? Dunno, but handle their idiocy anyways.
		if [ "${imagerange[0]}" = "${imagerange[1]}" ]; then
			echo -n "image ${imagerange[0]}"
		else
			echo -n "images ${imagerange[0]} to ${imagerange[1]}"
		fi
		echo ", starting from page ${pagerange[1]}."
	fi

	if [ $TAGCOUNT = 0 ] && [ ${pagerange[1]} == $MAX_INT ]; then
		# Holy fucking shit. The user is nuts. Downloading a whole booru? WTF?
		# Ask them to make damn well sure that they meant that.

		echo "[Booru] Holy fucking shit. You're telling me to download a whole booru? Are you insane?"
		echo "[Booru] Not to mention, I didn't even DOCUMENT this syntax on purpose."
		echo "[Booru] Type 'Yes, I am sane.' after the colon if you really meant to do this."
		echo -n "[Booru] You sane? : "
		read sanequery
		if [ "$sanequery" = "Yes, I am sane." ]; then
			echo "[Booru] Okay, nutjob, if you say so. Don't come complaining later."
		else
			echo "[Booru] Yeah, thought so. Whew~"
			exit 1
		fi
	fi

	if [ ! "${final[2]}" = "" ]; then
		echo "[Booru]   Requesting $(echo ${final[2]} | sed 's|+limit%3A||') images per page."
	fi
	if (( $TAGCOUNT >= 2 )); then
		echo "[Booru]   Warning, more than two normal tags. Some boorus don't like this."
	fi

	if [ ! "$3" = "" ]; then
		tagdir="$3"
	else
		tagdir="$site - $1"
	fi

	echo "[Booru]   Output to folder '$tagdir'."

	mkdir -p "$tagdir"
	cd "$tagdir"

	mkdir -p meta content

	# The first file is expected to be in the root tagdir.

	# Download page data and make a list of IDs.
	# Also, ideally, extract metadata.
	if [ $RESUME = 0 ]; then
		echo -n '' > meta.txt
		echo -n "[Booru] Fetching pages... "
		for (( range=${pagerange[0]} ; range <= ${pagerange[1]} ; range++ )); do
			spinner "${range} -> ${pagerange[0]}/${pagerange[1]}"
			booru_${name}_page "$range"
			R=$?

			lines=$(wc -l meta.txt | sed 's| .*||g')
			if [ $mode = 2 ] && (( $lines > ${imagerange[1]} )); then
				break
			fi

			if [ $R = 1 ]; then
				break
			fi
		done
	fi

	spinner_done

	lines=$(wc -l meta.txt | sed 's| .*||g')

	DONE=0
	LINES=$(wc -l meta.txt | sed 's| .*||g')

	# All metadata files should be placed in tagdir/meta.

	cd meta

	echo -n "[Booru] Checking and refetching metadata... "

	# Ideally metadata should have been taken care of above.
	# Some sites don't provide all info in a page API query though.
	while read meta_id; do
		SKIP=0
		if [ $mode = 2 ]; then
			if (( $DONE < ${imagerange[0]} )); then
				SKIP=1
			fi

			if (( $DONE > ${imagerange[1]} )); then
				break
			fi
			TOTAL="$RANGE"
		elif [ $RESUME = 1 ]; then
			DONE=$((DONE - 1))
			if [ $DONE = 0 ]; then
				RESUME=0
				DONE=$PRE
			fi
		else
			TOTAL="$LINES"
			spinner "${DONE} -> ${LINES}"
			booru_${name}_meta "$meta_id"
			DONE=$((DONE + 1))
		fi
	done < ../meta.txt

	spinner_done

	echo -n "[Booru] Downloading content... "

	META_DIR="$(pwd)"

	# All images should be in tagdir/content
	cd ../content

	DONE=0

	# Download images.
	while read meta_id; do
		SKIP=0
		if [ $mode = 2 ]; then
			if (( $DONE < ${imagerange[0]} )); then
				SKIP=1
			fi

			if (( $DONE > ${imagerange[1]} )); then
				break
			fi
			TOTAL="$RANGE"
		elif [ $RESUME = 1 ]; then
			DONE=$((DONE - 1))
			if [ $DONE = 0 ]; then
				RESUME=0
				DONE=$PRE
			fi
		else
			TOTAL="$LINES"
			spinner "${DONE} -> ${LINES}"
			booru_${name}_content "$meta_id"
			DONE=$((DONE + 1))
		fi
	done < ../meta.txt

	rm ../meta.txt

	spinner_done

	echo "[Booru] Done with download. If you don't care about the metadata, it is no longer needed."
}

scrape_booru() {
	exit 1
}
