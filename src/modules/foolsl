#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Unlike all of the other plugins, this is a generic downloader for foolslide based sites.
# Therefore, instead of keeping it a generic and needing to specifically ask for this plugin,
# I use a compatibility list.

# Sites using foolslide:
#   vortex-scans.com
#   foolrulez.org

foolsl_longname="FoolSlide"
foolsl_url="Generic"
foolsl_state=1
foolsl_filt=0

auto_foolsl() {
	if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# FoolRulez
		return 1
	elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# VortexScans
		return 1
	elif [ -n "`echo $1 | grep 'dokusha.info/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# ray=out
		return 1
	elif [ -n "`echo $1 | grep 'kobato.hologfx.com/reader/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Doki
		return 1
	fi

	return 0
}

dl_foolsl() {
	# Attempt a lazy download first. Only image scrape if rejected.

	message "Attempting Lazy Download..."

	LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`

	if [ -n "`echo $1 | grep 'kobato.hologfx.com/reader/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Fixup for LE-based cert. A lot of system trust stores don't include LE yet.
		NO_CHECK_CERT=1
	fi

	FAILED=0
	fetch "$LAZYURL" || export FAILED=1
	if [ $FAILED = 1 ]; then
		message "Nope."

		# Get the main page.
		DATA="$(fetch "$1" "-")"

		# Get page count.
		PAGES="$(echo "$DATA" | grep '<div class=\"tbtitle dropdown_parent dropdown_right\">' | sed -e 's| â¤µ</div>||g' -e 's|.*>||g')"

		# Get title.
		TITLE="$(echo "$DATA" | grep "<title>.*</title>" | sed -e 's|.*<title>||g' -e 's|</title>.*||g' | entity_to_char | remove_illegal)"

		BASE="$(echo "$1" | sed 's|/$||g')/page/"

		mkdir -p "$TITLE"
		cd "$TITLE"

		message "Downloading '$TITLE'... "

		for (( on=1 ; on <= $PAGES ; on++ )); do
			# Download pages and extract images.
			spinner "$on / $PAGES"

			IMG_URL=""
			while [ "$IMG_URL" = "" ]; do
				IMG_URL="$(fetch "${BASE}${on}" "-" | grep '<img class="open" src=' | sed -e 's|.* src="||g' -e 's|".*||g')"
			done
			IMG_NAM=""
			while [ "$IMG_URL" = "" ]; do
				IMG_NAM="$(basename "${IMG_URL}")"
			done

			FETCH_RESULT=1
			while [ ! $FETCH_RESULT = 0 ]; do
				fetch "$IMG_URL" "$IMG_NAM"
			done
		done

		spinner_done

		cd ..

		cbz_make "$TITLE"
	else
		message "OK"
	fi

	unset NO_CHECK_CERT
}

scrape_foolsl() {
	if [ -n "`echo $1 | grep 'kobato.hologfx.com/reader/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Fixup for LE-based cert. A lot of system trust stores don't include LE yet.
		NO_CHECK_CERT=1
	fi

	message "Scraping Chapters..."

	fetch "$1" "-" | \
		grep '<div class="title"><a href=' | \
		sed -e 's|<div class="title"><a href="||g' \
			-e 's|" title=.*||g' \
			-e "s/^[[:space:]]*//" \
			-e "s/[[:space:]]*$//" | \
	reverse_lines >> batch.txt

	message "Scraped chapters to batch.txt."

	unset NO_CHECK_CERT
}
