#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

eh_longname="E-H / ExH"
eh_url="g.e-hentai.org / exhentai.org"
eh_state=1
eh_filt=0
eh_note="Logging in will result in HQ images and less H@H peer-related issues =_=;"

eh_uselogin=1

login_eh() {
	use_cookies=1

	# Lots of extra shat they use to prevent bots. If it gets updated; let me know so I can fix it.
	
	s_login "UserName" "PassWord" "$1" "$2" "http://forums.e-hentai.org/index.php?act=Login&CODE=01" \
		"CookieDate=1&b=&bt=&referer=http://forums.e-hentai.org/?act=idx"
	user="$(fetch "http://forums.e-hentai.org/?act=idx" "-" | grep 'Logged in as' | sed -e 's|.*showuser=||' -e 's|<.*||' -e 's|.*>||g')"
	if [ ! "$user" = "" ]; then
		echo -e "\n[E-H] Logged in as: $user"
		exit 0
	fi

	exit 1
}

limitcheck_eh() {
	use_cookies=1

	line="$(fetch "http://g.e-hentai.org/home.php" "-" | grep "<p>You are currently at")"

	LIMIT_AT="$(echo $line | sed -e 's|.*<p>You are currently at <strong>||g' -e 's|</strong>.*||g')"
	LIMIT_OF="$(echo $line | sed -e 's|.*</strong> towards a limit of <strong>||g' -e 's|</strong>.*||g')"
	LIMIT_REGEN="$(echo $line | sed -e 's|.*</strong>. This regenerates at a rate of <strong>||g' -e 's|</strong>.*||g')"
}

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# E-H
		return 1
	elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Ex
		return 1
	fi

	return 0
}

check_eh() {
	if [ "$check_eh_done" = "1" ]; then
		return 0
	fi

	LOGGEDIN=0
	use_cookies=0
	if [ -e "cookiejar" ]; then
		if [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
			echo -ne "[E-H] Checking for Ex cookies..."
			grep 'exhentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
			RES=$?
			if [ $RES = 0 ]; then
				echo "yes"
				use_cookies=1
				echo -ne "[E-H] Checking for access..."

				# This is a good way of checking if it succeeded.
				fetch "exhentai.org" "exchk"
				cat exchk | grep "The X Makes It Sound Cool" >/dev/null 2>&1
				RES=$?
				if [ ! $RES = 0 ]; then
					echo -n "no, "
					if [ "$(sha256sum exchk | sed 's| .*||g')" = "a279e4ccd74cffbf20baa41459a17916333c5dd55d23a518e7f10ae1c288644f" ]; then
						echo "panda."
					else
						echo "not cool."
					fi
					exit 1
				else
					LOGGEDIN=1
					echo "yes"
				fi
			else
				echo "no"
				echo -ne "[E-H] Checking for E-H cookies..."
				grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
				RES=$?
				if [ $RES = 0 ]; then
					echo "yes, copying them"
					grep 'e-hentai.org' $COOKIEJAR >> cookiejar.edit
					sed 's|e-hentai.org|exhentai.org|g' cookiejar.edit >> cookiejar
					rm cookiejar.edit

					echo -ne "[E-H] Fixed cookies. Checking for access..."
					use_cookies=1
					fetch "exhentai.org" "-" | grep "The X Makes It Sound Cool" >/dev/null 2>&1
					RES=$?
					if [ ! $RES = 0 ]; then
						echo -n "no, "
						if [ "$(sha256sum exchk | sed 's| .*||g')" = "a279e4ccd74cffbf20baa41459a17916333c5dd55d23a518e7f10ae1c288644f" ]; then
							echo "panda."
						else
							echo "not cool."
						fi
						exit 1
					else
						LOGGEDIN=1
						echo "yes"
					fi
				else
					echo "no."
					echo "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
					exit 1
				fi
			fi
		else
			grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
			RES=$?
			if [ $RES = 0 ]; then
				echo "[E-H] We seem to have cookies...using them."
				LOGGEDIN=1
				use_cookies=1
			fi
		fi
	elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		echo "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
		exit 1
	fi

	check_eh_done=1

	return 0
}

dl_eh() {
	sitepage=$1

	check_eh

	echo "[E-H] Fetching index page..."

	# This is so that we don't get the 'offensive' warning.
	# XXX - Is this needed for Ex?
	fetch "${sitepage}/?nw=always" tmp.1

	while [ ! $FETCH_RESULT = 0 ]; do
		fetch "${sitepage}/?nw=always" tmp.1
	done

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
	# the last sed deals with names like fate/stay night which are invalid
	folder="$(cat tmp.1 | grep 'title>' | sed -e 's/<title>//g'  -e 's/<\/title>//g' -e 's/ - E-Hentai Galleries//g' -e 's/ - ExHentai.org//g' -e 's|/|_|g' | entity_to_char | remove_illegal)"
	mkdir -p "$folder"
	cd "$folder"

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[E-H] Already downloaded. Skipping."
		return 0
	fi

	echo "[E-H] Downloading '$folder'... "
	
	page=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/' | sed 's/"><img alt.*//g' | grep 'url:' | sed 's/url://g')

	MAXPAGES=$(cat ../tmp.1 | grep 'Length:' | sed -e 's|.*Length:</td><td class="gdt2">||g' -e 's| pages.*||g')

	if [ $LOGGEDIN = 1 ]; then
		limitcheck_eh
		LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))
		THIS_COUNT=$MAXPAGES
		if [ $LOGGEDIN = 1 ]; then
			THIS_COUNT=$(( MAXPAGES * 5 ))
		fi

		echo "[E-H] Limits: ${LIMIT_AT} / ${LIMIT_OF}, +${LIMIT_REGEN}/min"

		WARNED_YOU=0

		if (( $LIMIT_LEFT < $THIS_COUNT )); then
			echo "[E-H] This will probably exceed your download quota. Hit enter in the next five seconds, or I'll abort:"
			read -t 5
			R=$?
			if [ ! $R = 0 ]; then
				echo "[E-H] Aborting."
				exit 1
			fi

			WARNED_YOU=1
		fi
	fi

	rm ../tmp.1

	doneyet=0

	CDNFAIL=0

	CUR=1
	while [ $doneyet = 0 ]; do
		if [ $WARNED_YOU = 1 ]; then
			# Check to see (again) if CUR > MAX.
			# If it is, sit around and sleep until we have enough credits.
			limitcheck_eh
			LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))

			if (( $LIMIT_LEFT < 5 )); then
				message=":#"
						
				zzz=$(( LIMIT_REGEN * 60 ))

				while [ ! $zzz = 0 ]; do
					spinner "Over, wait ${zzz}s"
					sleep 1s
					zzz=$(( zzz - 1 ))
				done

				limitcheck_eh
				LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))
			fi
		fi

		fetch "$page" "$CUR.htm"
		while [ ! $FETCH_RESULT = 0 ]; do
			# For the most part, we'll not hit this unless there's network issues.
			# However, it can cause a failed fetch to go uncaught, causing next to be ''
			# And if the next loop "succeeds" with a zero length page, it will propogate
			# to next, and the 'done' check will trigger incorrectly.
			message=":O"
			spinner "RETR $CUR / $MAXPAGES"
			fetch "$page" "$CUR.htm"
		done

		lq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'keystamp' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
		hq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
		next_cnt=$((CUR + 1))
		next=$(cat $CUR.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

		extra=""

		spinner "IMG $CUR / $MAXPAGES"
		if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
			fetch "$lq" "$(basename $lq)"
		else
			# HQ version; this is the 'Download source resolution' button.
			fetch "$hq"
		fi

		if [ ! $FETCH_RESULT = 0 ]; then
			message=" Alt"

			while [ ! $FETCH_RESULT = 0 ]; do
				# Clobber existing pages.
				CLOBBER=1

				fetch "$page" "${CUR}.htm"
				notload=$(cat ${CUR}.htm | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")

				spinner "FAL $CUR / $MAXPAGES"
				fetch "$page?nl=$notload" "${CUR}nl.htm"

				# Turn clobber off now that we're done.
				CLOBBER=0
			done

			lq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'image.php' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
			hq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
			next=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

			spinner "IMG $CUR / $MAXPAGES"
			if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
				fetch "$lq"
			else
				fetch "$hq"
			fi

			rm "${CUR}nl.htm"

			if [ $FETCH_RESULT = 0 ]; then
				CUR=$(( CUR + 1 ))
				if [ ! "$next" = "" ]; then
					page="$next"
				fi
			fi
		else
			rm $CUR.htm
			CUR=$(( CUR + 1 ))
			if [ ! "$next" = "" ]; then
				page="$next"
			fi
		fi

		# This is a much saner condition than checking if next is null.
		if (( $CUR > $MAXPAGES )); then
			break
		fi

		spinner "$CUR / $MAXPAGES"
	done
	
	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	# TODO - Support grabbing page list of one tag.
	# There are some things to account for though; namely, layouts. Thumb layout vs list.
	# Number of things per page, etc.
	# A pain basically.
	echo -e "[E-H] Not yet implemented."
}
