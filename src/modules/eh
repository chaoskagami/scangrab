#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

eh_longname="E-Hentai"
eh_url="http://e-hentai.org/"
eh_state=1
eh_filt=0
eh_note="H@H is an unreliable CDN. Also, they count images by logged in IP."

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# E-H
		return 1
	fi

	return 0
}

dl_eh() {
	sitepage=$1

	# This is so that we don't get the 'offensive' warning.
	fetch "${sitepage}/?nw=always" tmp.1

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
	# the last sed deals with names like fate/stay night which are invalid
	folder="$(cat tmp.1 | grep 'title>' - | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/ - E-Hentai Galleries//g' | sed 's|/|_|g' | entity_to_char)"
	mkdir -p "$folder"
	cd "$folder"
	
	DATA=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/g' | sed 's/"><img alt.*//g' | grep 'url:')
	rm ../tmp.1

	eval "urls=(`echo $DATA | tr '\n' ' ' | sed 's/url://g'`)"

	page="${urls[0]}"

	echo -n "[e-h] Downloading '$folder' "

	doneyet=0

	CDNFAIL=0

	CUR=0
	while [ $doneyet == 0 ]; do
		get=($(fetch "$page" "-" | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'keystamp'))

		extra=""

		if [ "${get[0]}" == "$page" ]; then
			doneyet=1
		fi

		if [ $CDNFAIL -ge 3 ]; then
			FETCH_RESULT=1
		else
			fetch "${get[1]}" "$extra"
		fi

		if [ ! $FETCH_RESULT = 0 ]; then

			message=" :("

			CDNFAIL=$((CDNFAIL + 1))
			if [ $CDNFAIL -ge 3 ]; then
				message=" >:("
			fi

			notload=$(fetch "$page" "-" | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")
			get=($(fetch "$page?nl=${notload}" "-" | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'image.php' | entity_to_char ))
			extra="$(echo ${get[1]} | sed 's/.*n=//')"

			fetch "${get[1]}" "$extra"
			if [ ! $FETCH_RESULT = 0 ]; then
				message=" >:["
			else
				CUR=$(( CUR + 1 ))
			fi
		else
			CUR=$(( CUR + 1 ))
		fi

		if [ ! "${get[0]}" = "" ]; then
			page="${get[0]}"
		fi

		spinner "$CUR"
	done
	
	done_spin

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	echo -e "[e-h] This isn't supported, considering there's really zero categorization here."
}
