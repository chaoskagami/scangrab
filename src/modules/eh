#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

eh_longname="E-Hentai"
eh_url="http://e-hentai.org/"
eh_state=1
eh_filt=0
eh_note="Logging in will result in HQ images and less H@H peer-related issues =_=;"

eh_uselogin=1

login_eh() {
	use_cookies=1

	# Lots of extra shat they use to prevent bots. If it gets updated; let me know.
	
	s_login "UserName" "PassWord" "$1" "$2" "http://forums.e-hentai.org/index.php?act=Login&CODE=01" \
		"CookieDate=1&b=&bt=&referer=http://forums.e-hentai.org/?act=idx"
	user="$(fetch "http://forums.e-hentai.org/?act=idx" "-" | grep 'Logged in as' | sed -e 's|.*showuser=||' -e 's|<.*||' -e 's|.*>||g')"
	if [ ! "$user" = "" ]; then
		echo "[E-H] Logged in as: $user"
		exit 0
	fi

	exit 1
}

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# E-H
		return 1
	fi

	return 0
}

dl_eh() {
	LOGGEDIN=0
	use_cookies=0
	if [ -e "cookiejar" ]; then
		grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
		RES=$?
		if [ $RES = 0 ]; then
			echo "[E-H] We seem to have cookies...using them."
			LOGGEDIN=1
			use_cookies=1
		fi
	fi

	sitepage=$1

	echo "[E-H] Fetching index page..."

	# This is so that we don't get the 'offensive' warning.
	fetch "${sitepage}/?nw=always" tmp.1

	while [ ! $FETCH_RESULT = 0 ]; do
		echo "[E-H] Whoa. Failed to download index. Going again..."
		fetch "${sitepage}/?nw=always" tmp.1
	done

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
	# the last sed deals with names like fate/stay night which are invalid
	folder="$(cat tmp.1 | grep 'title>' | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/ - E-Hentai Galleries//g' | sed 's|/|_|g' | entity_to_char)"
	mkdir -p "$folder"
	cd "$folder"
	
	page=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/' | sed 's/"><img alt.*//g' | grep 'url:' | sed 's/url://g')

	rm ../tmp.1

	echo -n "[E-H] Downloading '$folder' "
	if [ $LOGGEDIN = 1 ]; then
		echo -n "[HQ] "
	else
		echo -n "[LQ] "		
	fi

	doneyet=0

	CDNFAIL=0

	CUR=1
	while [ $doneyet == 0 ]; do
		fetch "$page" "$CUR.htm"
		lq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'keystamp' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
		echo $lq
		hq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
		next_cnt=$((CUR + 1))
		next=$(cat $CUR.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

		extra=""

		if [ "$next" == "" ]; then
			doneyet=1
		fi

		if [ $CDNFAIL -ge 3 ]; then
			FETCH_RESULT=1
		else
			if [ $LOGGEDIN = 0 ]; then
				fetch "$lq" "$(basename $lq)"
			else
				# HQ version; this is the 'Download source resolution' button.
				fetch "$hq"
			fi
		fi

		if [ ! $FETCH_RESULT = 0 ]; then

			message=" :("

			CDNFAIL=$((CDNFAIL + 1))
			if [ $CDNFAIL -ge 3 ]; then
				message=" >:("
			fi

			notload=$(cat $CUR.htm | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")
			
			fetch "$page?nl=$notload" "${CUR}nl.htm"
			lq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'image.php' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
			hq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
			next=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

			if [ $LOGGEDIN = 0 ]; then
				fetch "$lq"
			else
				fetch "$hq"
			fi

			rm "${CUR}nl.htm"

			if [ ! $FETCH_RESULT = 0 ]; then
				message=" >:["
			else
				rm $CUR.htm
				CUR=$(( CUR + 1 ))
				if [ ! "$next" = "" ]; then
					page="$next"
				fi
			fi
		else
			rm $CUR.htm
			CUR=$(( CUR + 1 ))
			if [ ! "$next" = "" ]; then
				page="$next"
			fi
		fi

		spinner "$CUR"
	done
	
	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	echo -e "[E-H] This isn't supported, considering there's really zero categorization here."
}
