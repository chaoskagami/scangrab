#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

eh_longname="E-Hentai"
eh_url="http://e-hentai.org/"
eh_state=1
eh_filt=0

#notice_eh() {
#	if [ "$noticed_eh" = "" ]; then
#		echo -e "[e-h] Warning, this is experimental and subject to random failures."
#		echo -e "[e-h] This is because H@H is a unreliable download service.
#		echo -e "[e-h] And we are not a proper node, if you know what I mean."
#		echo -e "[e-h] Also - this will count towards your image limit of 5k."
#		noticed_eh=1
#	fi
#}

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# E-H
		return 1
	fi

	return 0
}

dl_eh() {
	#notice_eh

	sitepage=$1

	fetch "${sitepage}/?nw=always" tmp.1

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
	# the last sed deals with names like fate/stay night which are invalid
	folder="$(cat tmp.1 | grep 'title>' - | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/ - E-Hentai Galleries//g' | sed 's|/|_|g' | entity_to_char)"
	mkdir -p "$folder"
	cd "$folder"
	
	DATA=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/g' | sed 's/"><img alt.*//g' | grep 'url:')
	rm ../tmp.1

	eval "urls=(`echo $DATA | tr '\n' ' ' | sed 's/url://g'`)"

	page="${urls[0]}"

	echo -n "[e-h] Downloading '$folder' "

	doneyet=0

	CUR=0
	while [ $doneyet == 0 ]; do
		get=($(fetch "$page" "-" | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'keystamp' -))

		extra=""

		if [ "${get[1]}" == "" ]; then
			get=($(fetch "$page" "-" | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 '.php?f=' -))
			extra="-O $(echo ${get[1]} | sed 's/.*n=//')"
		fi

		if [ "${get[0]}" == "$page" ]; then
			doneyet=1
		fi

		fetch "${get[1]}" "$extra"

		if [ ! "${get[0]}" == "" ]; then
			page="${get[0]}"
		fi

		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done
	
	done_spin

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	echo -e "[e-h] This isn't supported, considering there's really zero categorization here."
}
