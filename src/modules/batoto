#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

batoto_longname="Batoto"
batoto_url="bato.to"
batoto_state=0
batoto_filt=1
batoto_note="Broken until further notice."

auto_batoto() {
	if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Batoto
		return 1
	fi

	return 0
}

dl_batoto() {
	# Batoto requires a different strategy.
	# The URLs are not preloaded like the former, so the fetch one page done thing won't work.
	# Unfortunately, short of grabbing pages until an image 404's, there's no way of knowing when we're done.
	fetch_jscommand "$1" "PAGE"

	folder="$(cat PAGE | grep -C0 "<title>" | sed -e "s/^[[:space:]]*<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e "s/[[:space:]]*$/\n/" | entity_to_char | remove_illegal)"

	rm PAGE

	echo "$folder"

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[Batoto] Already downloaded. Skipping."
		return 0
	fi

	mkdir -p "$folder"
	cd "$folder"

	CUR=1
	PAGES=0
	RET=0

	echo -n "[Batoto] Downloading '$folder' "

	BASE="$1"

	while [ "$RET" = "0" ]; do
		fetch_jscommand "${BASE}_${CUR}" "page"

		img="$(grep -C0 'z-index: 1003' page | sed -e 's/^[[:space:]]*<img src="//g' -e 's/".*$//g')"

		# If this 404's, fetch will return non-zero. Thus, loop breaks.
		fetch "$img"
		RET=$?

		rm page

		spinner "$CUR"

		CUR=$((CUR + 1))
	done

	PAGES=$(( CUR - 1 ))

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_batoto() {
	notice_batoto

	echo -n "[Batoto] Scraping Chapters..."

	fetch "$1" scrape.htm

	# Batoto sometimes gives out gunzips. We need to account for that... =_=
	if [ "$(mimetype scrape.htm)" = "application/x-gzip" ]; then
		mv $CUR.htm $CUR.htm.gz
		gunzip $CUR.htm.gz
		message=" :/"
	fi

	grep -A 2 'Sort:' scrape.htm >> batch.txtr

	# Delete the useless lines.
	sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr

	# Remove Language lines.
	sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
	sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr

	# Edit up URL.
	sed -i "s|<a href=\"||g" batch.txtr
	sed -i "s|\" title=.*||g" batch.txtr

	# Delete blank lines/space lines
	sed -i '/^[[:space:]]*$/d' batch.txtr

	# Strip.
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	sed -i 's|^--$||g' batch.txtr

	# Delete Blank lines.
	sed -i '/^$/d' batch.txtr

	cat batch.txtr | reverse_lines > batch.txtf

	if [ "$2" = "" ]; then
		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf

		cat batch.txtf >> batch.txt
	else
		echo -ne "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Applying Language Filter '$2'..."

		grep -A 1 "$2" batch.txtf > batch.txtf2

		# Delete '--' lines
		sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2

		# Delete Blank lines.
		sed -i '/^$/d' batch.txtf2

		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf2

		cat batch.txtf2 >> batch.txt
	fi

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr batch.txtf*

	echo -en "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"

	for ((n=0;n < ${#2}; n++)); do
		echo -en '\b'
	done

	echo -e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
