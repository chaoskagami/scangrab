batoto_longname="Batoto"
batoto_url="http://bato.to/"
batoto_state=1
batoto_filt=1

notice_batoto() {
	if [ "$noticed_batoto" = "" ]; then
		echo "[Batoto] If this so happens to fail, they changed their page generator again."
		noticed_batoto=1
	fi
}

auto_batoto() {
	if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Batoto
		return 1
	fi

	return 0
}

dl_batoto() {
	notice_batoto

	# Batoto requires a different strategy.
	# The URLs are not preloaded like the former, so the fetch one page done thing won't work.
	# Unfortunately, short of grabbing pages until an image 404's, there's no way of knowing when we're done.
	
	folder="$(fetch "$1" "-" | grep -C0 "<title>" | sed -e "s/<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e s/[[:space:]]*$// | entity_to_char)"

	mkdir -p "$folder"
	cd "$folder"

	CUR=0
	PAGES=0
	RET=0
	
	echo -n "[Batoto] Downloading '$folder' "

	while [ "$RET" = "0" ]; do
		# Increment CUR.
		CUR=$(( CUR + 1 ))
	
		fetch "$1/$CUR" "$CUR.htm"
		
		# Batoto sometimes gives out gunzips. We need to account for that... =_=

		echo `file $CUR.htm` | grep 'gzip' - > /dev/null 2>&1

		RET=$?

		# Quirk: GZip. The server ignores our request for non-gz sometimes.
		if [ $RET = 0 ]; then # Yeah, got a gz.
			echo -ne "  :/\b\b\b\b"

			mv $CUR.htm $CUR.htm.gz
			gunzip $CUR.htm.gz
		fi

		grep -A 1 'z-index: 1002' $CUR.htm | tail -n1 > $CUR.tmp
	
		# Edit magic.
		sed -i "s/[[:space:]]*<img //g" $CUR.tmp
		sed -i "s/ style=.*//g" $CUR.tmp

		# Load 'src'
		. $CUR.tmp

		# If this 404's, fetch will return non-zero. Thus, loop breaks.
		fetch "$src"
		RET=$?
	
		spinner "$CUR"
	done

	PAGES=$(( CUR - 1 ))

	done_spin

	# Clean.

	rm *.htm
	rm *.tmp

	cd ..

	cbz_make "$folder"
}

scrape_batoto() {
	notice_batoto

	echo -n "[Batoto] Scraping Chapters..."

	fetch "$1" scrape.htm
		
	# Batoto sometimes gives out gunzips. We need to account for that... =_=

	echo `file scrape.htm` | grep 'gzip' - > /dev/null 2>&1

	RET=$?

	# Quirk: GZip. The server ignores our request for non-gz sometimes.
	if [ $RET = 0 ]; then # Yeah, got a gz.
		echo -ne "  :/\b\b\b\b"

		mv scrape.htm scrape.htm.gz
		gunzip scrape.htm.gz
	fi

	grep -A 2 'Sort:' scrape.htm >> batch.txtr

	# Delete the useless lines.
	sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr

	# Remove Language lines.
	sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
	sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr

	# Edit up URL.
	sed -i "s|<a href=\"||g" batch.txtr
	sed -i "s|\" title=.*||g" batch.txtr

	# Delete blank lines/space lines
	sed -i '/^[[:space:]]*$/d' batch.txtr

	# Strip.
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	sed -i 's|^--$||g' batch.txtr

	# Delete Blank lines.
	sed -i '/^$/d' batch.txtr

	# Lines are reverse order. tac.
	# If whatever we're using has no tac, you're stuck with reverse order.
	tac batch.txtr > batch.txtf || tail -r batch.txtr > batch.txtf

	if [ "$2" = "" ]; then
		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf
		
		cat batch.txtf >> batch.txt
	else
		echo -ne "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Applying Language Filter '$2'..."

		grep -A 1 "$2" batch.txtf > batch.txtf2
		
		# Delete '--' lines
		sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2

		# Delete Blank lines.
		sed -i '/^$/d' batch.txtf2

		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf2

		cat batch.txtf2 >> batch.txt
	fi

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr batch.txtf*
	
	echo -en "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"

	for ((n=0;n < ${#2}; n++)); do
		echo -en '\b'
	done

	echo -e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
