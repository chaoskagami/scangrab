#!/bin/bash
#############################################
#####@ORIGINAL-FILE 'rev'
rev=75720c24d921fc8e09ee6398078264d160289403
branch=master
#############################################
#####@ORIGINAL-FILE 'xul_dat'
XUL_ZIP_DATA="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"
#############################################
#####@ORIGINAL-FILE 'functions/s_login'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# This creates a cookie jar.
# $1 - Username field
# $2 - Password field
# $3 - Username
# $4 - Password
# $5 - URL
# $6 - extra shit
s_login() {
	if [ $_FETCHTOOL = 1 ]; then
		# WGET
		_CMD="$_FETCH_CMD --post-data='$1=$3&$2=$4&$6' \"$5\"  --load-cookies='$COOKIEJAR' --save-cookies='$COOKIEJAR' --keep-session-cookies -O/dev/null"

	elif [ $_FETCHTOOL = 2 ]; then
		# CURL
		_CMD="$_FETCH_CMD -d '$1=$3&$2=$4&$6' $5 -b '$COOKIEJAR' -c '$COOKIEJAR' >/dev/null"

	elif [ $_FETCHTOOL = 3 ]; then
		#ARIA2C
		# How the fuck do I post? Maybe I can't with araia2c...meh.
		echo "[Warn] aria2c can't post at the moment; this will fail to get required cookies."
		_CMD="$_FETCH_CMD $5 --load-cookies=$COOKIEJAR --save-cookies='$COOKIEJAR' -o/dev/null"

	fi
	
	if [ $VERBOSE = 1 ]; then
		echo -e "\n$_CMD"
		return
	fi

	eval " $_CMD" 2>/dev/null
	FETCH_RESULT=$?
}
#############################################
#####@ORIGINAL-FILE 'functions/spinner_clear'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

spinner_clear() {
	# Clear previous output
	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne "\b"
	done

	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne " "
	done

	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne "\b"
	done
}
#############################################
#####@ORIGINAL-FILE 'functions/fetch'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# AVOID CHANGING THIS FUNCTION IF AT ALL POSSIBLE.
# THINGS WILL BREAK IN EVERYTHING IF THIS ONE BREAKS.
fetch() {
	if [ $_FETCHTOOL = 1 ]; then
		if [ $SAFETY_HACKS = 1 ] || [ "$1" = "-" ]; then
			_CMD="$_FETCH_CMD \"$1\""
		else
			_CMD="$_FETCH_CMD --content-disposition \"$1\""
		fi

		if [ ! $CLOBBER = 1 ]; then
			_CMD="$_CMD -c"
		fi

		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD --load-cookies='$COOKIEJAR'"
		fi

		# Wget uses content-disposition to hopefully get a good name in abscence of one.
		# Of course, that assumes that SAFETY_HACKS is off.

		STDOUT=0
		if [ "$2" = "-" ]; then
			_CMD="$_CMD -O -"
			STDOUT=1
		elif [ ! "$2" = "" ]; then
			_CMD="$_CMD -O \"$2\""
		fi

	elif [ $_FETCHTOOL = 2 ]; then

		_CMD="$_FETCH_CMD $1"
		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD -b '$COOKIEJAR' -c '$COOKIEJAR'"
		fi

		if [ "$2" = "" ]; then
			_CMD="$_CMD > $(basename "$1")"
		elif [ "$2" = "-" ]; then
			_CMD="$_CMD"
		else
			_CMD="$_CMD > \"$2\""
		fi

	elif [ $_FETCHTOOL = 3 ]; then

		_CMD="$_FETCH_CMD $1"
		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD --load-cookies='$COOKIEJAR'"
		fi

		if [ "$2" = "" ]; then
			_CMD="$_CMD -o $(basename "$1")"
		elif [ "$2" = "-" ]; then
			# FIXME - This doesn't work as expected.
			_CMD="$_CMD -o -"
		else
			_CMD="$_CMD -o \"$2\""
		fi
	fi

	if [ $VERBOSE = 1 ]; then
		echo -e "\n$_CMD"
	fi

	# Wget doesn't have an option to clobber forcibly
	# with content disposition on. I'm so bemused by this
	# that I'm not even sure what to say.

	# We use a rather idiotic workaround. We download to a temp folder,
	# and move it out afterwards.

	if [ $CLOBBER = 1 ] && [ $_FETCHTOOL = 1 ] && [ ! $STDOUT = 1 ]; then
		WGET_TMP="$(temp d)"
		cd "$WGET_TMP"
	fi

	eval $_CMD 2>/dev/null
	FETCH_RESULT=$?

	if [ $CLOBBER = 1 ] && [ $_FETCHTOOL = 1 ] && [ ! $STDOUT = 1 ]; then
		mv ./* ../ 2>/dev/null
		cd ..
		rm -rf "$WGET_TMP"
	fi

	# If this is an image, check validity.
	MIME="$(mimetype "$_FILE")"
	if [ "$MIME" = "image/jpeg" ] || [ "$MIME" = "image/png" ] || [ "$MIME" = "image/gif" ] || [ "$MIME" = "image/bmp" ]; then
		verify "$_FILE"
		VALID=$?
		if [ ! $VALID = 0 ]; then
			echo "[WARN] File '$_FILE' is corrupted."
		fi
	fi

	return $FETCH_RESULT
}
#############################################
#####@ORIGINAL-FILE 'functions/type'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

type() {
	if [ $_COLOR = 1 ]; then
		echo -ne "\x1b[$1m"
	elif [ $_COLOR = 2 ]; then
		if [ $1 = 0 ]; then
			tput sgr0
		elif [ $1 = 1 ]; then
			tput bold
		elif [ $1 = 2 ]; then
			tput dim
		fi
	fi
}

#############################################
#####@ORIGINAL-FILE 'functions/spinner'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
spinner() {
	if [ $VERBOSE = 1 ]; then
		echo "Status: $1 - $message"
		return
	fi
	if [ "$_SPINNER_CHAR" = "|" ]; then
		_SPINNER_CHAR="/"
	elif [ "$_SPINNER_CHAR" = "/" ]; then
		_SPINNER_CHAR="-"
	elif [ "$_SPINNER_CHAR" = "-" ]; then
		_SPINNER_CHAR="\\"
	elif [ "$_SPINNER_CHAR" = "\\" ]; then
		_SPINNER_CHAR="|"
	fi

	# Clear previous output
	spinner_clear

	STR="[$1 $_SPINNER_CHAR] $message"
	_NUM="${#STR}"

	echo -ne "$STR"
}
#############################################
#####@ORIGINAL-FILE 'functions/mimetype'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
mimetype() {
	echo "$(file --mime-type "$1" | sed 's/.* //g')"
}
#############################################
#####@ORIGINAL-FILE 'functions/verify'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# Checks if an image is uncorrupted. If you don't have the tools, this never happens.
	# Returns 0 on OK / no tool
	# Returns error code on corrupt
verify() {
	if [ -f $_identify ]; then
		$_identify -verbose -regard-warnings "$1" 2>&1 >/dev/null
		_IDRES=$?
		return $_IDRES
	fi

	return 0
}
#############################################
#####@ORIGINAL-FILE 'functions/fetch_no_ops'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# This version adds no extra parameters, and requires an output.
fetch_no_ops() {
	if [ $_FETCHTOOL = 1 ]; then
		_CMD="wget -q \"$1\" -O \"$2\""
	elif [ $_FETCHTOOL = 2 ]; then
		_CMD="curl \"$1\" > \"$2\""
	elif [ $_FETCHTOOL = 3 ]; then
		_CMD="aria2c \"$1\" -o \"$2\""
	fi
	
	if [ $VERBOSE = 1 ]; then
		echo -e "\n$_CMD"
	fi

	eval $_CMD 2>/dev/null
	FETCH_RESULT=$?

	return $FETCH_RESULT
}
#############################################
#####@ORIGINAL-FILE 'functions/bash_get'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# This provides an alternative stupid-ass way to get files from the internet
# with some rather severe limitations. This can be used in abscence of wget
# curl and aria2c but requires a bash with support for /dev/tcp.

# It is also incapable of POSTs, https, and every form of header (like
# content-disposition) and may produce general weirdness.

bash_get() {
    if [ "$1" = "" ]; then
        echo "Usage: bget url [output_file]"
        return 1
    fi

    BASE="$(echo -ne $1 | sed -e 's|http://||g' -e 's|https://||g' -e 's|/.*||g')"
    GET="$(echo -ne $1 | sed -e 's|http://||g' -e 's|https://||g' -e "s|$BASE||g")"
    STDOUT=0

    if [ "$2" = "-" ]; then
        STDOUT=1
        OUT="stdout"
    elif [ "$2" = "" ]; then
        OUT="$(basename $GET | sed 's|?.*||g')"
        GET_L=$((${#GET}-1))
        if [ "${GET:$GET_L:1}" = "/" ]; then
            OUT="index.htm"
        fi
    else
        OUT="$2"
    fi

    # Open connection.
    exec 3<>/dev/tcp/${BASE}/80

    printf "GET ${GET} HTTP/1.1\r\nhost: ${BASE}\r\nConnection: close\r\n\r\n" >&3

    ( cat <&3 | cat > $OUT ) 2>&1 >/dev/null

    # Close.
    exec 3>&-

    STAT_CODE=0
    LINE=0

    # Edit out / interpret headers.
    while read line; do
        if [ "$(echo -n $line | tr -d "\r")" = "" ]; then
            break
        fi
        if [ $LINE = 0 ]; then
            # HTTP status code always first.
            STAT_CODE="$(echo $line | sed -e 's|HTTP/1.1 ||g' -e 's| .*||g')"
        else
            echo $line | grep "Set-Cookie" 2>&1 >/dev/null
            R=$?
            if [ $R = 0 ]; then
                # Set cookie.
                cookie=$(echo $line | sed 's|Set-Cookie: ||g')
            fi
        fi
        LINE=$((LINE + 1))
    done < $OUT

    LINE=$((LINE + 1))

    sed -i "1,${LINE}d" $OUT

    if [ "$STDOUT" = "1" ]; then
        cat $OUT
        rm $OUT
    fi

    if [ $STAT_CODE = 200 ]; then
        return 0
    else
        return 1
    fi
}
#############################################
#####@ORIGINAL-FILE 'functions/spinner_reset'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

spinner_reset() {
	if [ $VERBOSE = 1 ]; then
		echo "Status: $1 - $message"
		return
	fi

	spinner_clear

	_SPINNER_CHAR="|"
	_NUM=0
}
#############################################
#####@ORIGINAL-FILE 'functions/remove_illegal'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# These are characters that are either issue causing or
# not FAT-legal.

# Forward slash is not legal on linux.
# Pipe character isn't legal on FAT and can cause issues with redirection.
# ? is not FAT legal, so it is replaced with the unicode wide version
# Angle brackets are not FAT legal - they're replaced with parenthesis.
# \t is a no-no in filenames. I've seen at least one instance of this nastiness.
# \n - see \t.
# The last two rules are more a pet peeve. Spaces are stripped from the front and back.
# The last head command cuts the string to the max filename length of the system.
# If getconf isn't a valid command, we use 255 instead.

if [ "$FSTYPE" = "ext4" ] || [ "$FSTYPE" = "ext3" ] || [ "$FSTYPE" = "ext2" ] || [ "$FSTYPE" = "xfs" ] || [ "$FSTYPE" = "jfs" ] || [ "$FSTYPE" = "btrfs" ]; then
	# Filesystem has support for fanciness.
	remove_illegal() {
		sed \
			-e "s|/|-|g" |
			-e "s|^[[:space:]]*||g" \
			-e "s|[[:space:]]*$||g" |
		head -c $(getconf NAME_MAX . || echo 255)
	}
else
	# We don't know what it is, so apply FAT restrictions.
	remove_illegal() {
		sed \
			-e "s/|/-/g" \
			-e "s|/|-|g" \
			-e "s|?|？|g" \
			-e "s|<|(|g" \
			-e "s|>|)|g" \
			-e "s|\t| |g" \
			-e "s|\"|'|g" \
			-e "s|\n| |g" \
			-e "s|:|-|g" \
			-e "s|^[[:space:]]*||g" \
			-e "s|[[:space:]]*$||g" |
			head -c $(getconf NAME_MAX . || echo 255)
	}
fi
#############################################
#####@ORIGINAL-FILE 'functions/color'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
color() { 
	if [ $_COLOR = 1 ]; then
		echo -ne "\x1b[3$1m"
	elif [ $_COLOR = 2 ]; then
		tput setaf $1
	fi
}
#############################################
#####@ORIGINAL-FILE 'functions/entity_to_char'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

entity_to_char() {
	# This probably doesn't handle every case. It should be enough.
	# It also handles the case of illegal characters on windows/FAT/NTFS.
	# And the case of a slash in the name which is the only illegal one on linux.

	# Always put 0* before any number, since some pages have &#039, for example.

	sed                     \
		-e "s/&#0*32;/ /g"    \
		-e "s/&nbsp;/ /g"   \
		-e "s/&#0*33;/\!/g"   \
		-e "s/&#0*34;/\"/g"   \
		-e "s/&quot;/\"/g"   \
		-e "s/&#0*35;/\#/g"   \
		                    \
		-e "s/&#0*36;/\$/g"   \
		-e "s/&#0*37;/\%/g"   \
		-e "s/&amp;/\&/g"   \
		-e "s/&#0*38;/\&/g"   \
		-e "s/&#0*39;/'/g"    \
		                    \
		-e "s/&#0*40;/\(/g"   \
		-e "s/&#0*41;/\)/g"   \
		-e "s/&#0*42;/\*/g"   \
		-e "s/&#0*43;/\+/g"   \
		-e "s/&#0*44;/\,/g"   \
		                    \
		-e "s/&#0*45;/\-/g"   \
		-e "s/&#0*46;/\./g"   \
		-e "s/&#0*58;/\:/g"   \
		-e "s/&#0*59;/\;/g"   \
		-e "s/&lt;/\</g"    \
		                    \
		-e "s/&#0*60;/\</g"   \
		-e "s/&gt/\>/g"     \
		-e "s/&#0*61;/\>/g"   \
		-e "s/&#0*63;/\?/g"   \
		-e "s/&#0*64;/\@/g"   \
		                    \
		-e "s/&#0*91;/\[/g"   \
		-e "s/&#0*92;/\\\\/g" \
		-e "s/&#0*93;/\]/g"   \
		-e "s/&#0*94;/\^/g"   \
		-e "s/&#0*95;/\_/g"   \
		                    \
		-e "s/&#0*123;/\{/g"  \
		-e "s/&#0*124;/\|/g"  \
		-e "s/&#0*125;/\}/g"  \
		-e "s/&#0*126;/\~/g"  \
		-e "s/&yen;/¥/g"    \
		                    \
		-e "s/&#0*165;/¥/g"   \
		-e "s/&sup2;/²/g"   \
		-e "s/&#0*178;/²/g"   \
		-e "s/&sup3;/³/g"   \
		-e "s/&#0*179;/³/g"   \
		                    \
		-e "s/&frac14;/¼/g" \
		-e "s/&#0*188;/¼/g"   \
		-e "s/&frac12;/½/g" \
		-e "s/&#0*189;/½/g"   \
		-e "s/&frac34;/¾/g" \
		                    \
		-e "s/&#0*190;/¾/g"   \
		-e "s/&spades;/♠/g" \
		-e "s/&#0*9824;/♠/g"  \
		-e "s/&clubs;/♣/g"  \
		-e "s/&#0*9827;/♣/g"  \
		                    \
		-e "s/&hearts;/♥/g" \
		-e "s/&#0*9829;/♥/g"  \
		-e "s/&diams;/♦/g"  \
		-e "s/&#0*9830;/♦/g"

}
#############################################
#####@ORIGINAL-FILE 'functions/reverse_lines'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

reverse_lines() {
    readarray -t LINES
    for (( I = ${#LINES[@]}; I; )); do
        echo "${LINES[--I]}"
    done
}
#############################################
#####@ORIGINAL-FILE 'functions/spinner_done'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

spinner_done() {
	spinner_reset

	_MESG="OK"
	if [ ! "$1" = "" ]; then
		_MESG="$1"
	fi
	echo -e "[$_MESG]"
}
#############################################
#####@ORIGINAL-FILE 'functions/is_done'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# Checks if file exists.

is_done() {
	if [ -e "${1}.cbz" ]; then
		return 1
	fi
	return 0
}
#############################################
#####@ORIGINAL-FILE 'functions/cbz_make'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

cbz_make() {
	# Check and make sure the folder has something, and is actually a folder.
	if [ ! -d "$1" ]; then
		echo -e "\n[Error] Not a folder. Something went wrong."
		exit 1
	fi

	# If nothing is in it, something went wrong.
	if [ "$(ls "$1")" = "" ]; then
		echo "[Error] No files? Download failed."
		exit 1
	fi


	if [ "$FORMAT" = "cbz" ] || [ "$FORMAT" = "zip" ]; then
		# Zip format
		echo -e "[Post] Zipping up..."
		zip -r "$1.zip" "$1" > /dev/null 2>&1
		mv "$1.zip" "$1.cbz" > /dev/null 2>&1

		echo -e "[Post] Cleanup..."
		rm -rf "$1"
	elif [ "$FORMAT" = "cb7" ] || [ "$FORMAT" = "7z" ]; then
		# 7zip format
		echo -e "[Post] 7z'ing..."
		7z a "$1.7z" "$1" > /dev/null 2>&1
		mv "$1.7z" "$1.cb7" > /dev/null 2>&1

		echo -e "[Post] Cleanup..."
		rm -rf "$1"
	elif [ "$FORMAT" = "cbr" ] || [ "$FORMAT" = "rar" ]; then
		# RAR format (unsupported)
		echo -e "[Post] RAR is not supported, and never will be. Sorry."
	elif [ "$FORMAT" = "cbt" ] || [ "$FORMAT" = "tar" ]; then
		# Tar archive
		echo -e "[Post] Tarring..."
		tar -cf "$1.tar" "$1" > /dev/null 2>&1
		mv "$1.tar" "$1.cbt" > /dev/null 2>&1

		echo -e "[Post] Cleanup..."
		rm -rf "$1"
	elif [ "$FORMAT" = "raw" ]; then
		# Leave it as a folder
		echo -e "[Post] Raw was specified. Leaving as a folder."
	else
		echo -e "[Post] Don't know how to handle format '$FORMAT'. Zipping instead."

		# Zip format
		echo -e "[Post] Zipping up..."
		zip -r "$1.zip" "$1" > /dev/null 2>&1
		mv "$1.zip" "$1.cbz" > /dev/null 2>&1

		echo -e "[Post] Cleanup..."
		rm -rf "$1"
	fi

	echo -e "[Post] Finished!"

}
#############################################
#####@ORIGINAL-FILE 'functions/fetch_jscommand'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

# This version uses an available proper web parser to get output which
# would otherwise be incomplete due to javascript/ajax/etc

# Currently supports the following backends:
# * XulRunner / Firefox / Pale Moon

fetch_jscommand() {
	if [ $_HAVE_JSCOMMAND = 1 ]; then
		# Firefox / XUL.

		XUL_TEMP="$(temp d)"

		mkdir "$XUL_TEMP" && cd "$XUL_TEMP"

		# Not extracted. Do so.
		echo $XUL_ZIP_DATA | base64 -d > xul.zip && unzip xul.zip >/dev/null 2>&1 && rm xul.zip >/dev/null 2>&1

		_CMD="echo \"PAGE_URL='$1';\" > ${XUL_TEMP}/xul/url.js && $_JSCOMMAND --app ${XUL_TEMP}/xul/application.ini --console > \"$2\""


		cd ..
	else
		echo "NOT SUPPORTED."
		return 1
	fi

	if [ $VERBOSE = 1 ]; then
		echo -e "\n$_CMD"
	fi

	if [ -f "$_Xvfb" ]; then
		Xvfb :1 &
		XPID=$!

		SAVE_D="$DISPLAY"
		export DISPLAY=":1"
	fi

	eval $_CMD 2>/dev/null
	rm -rf ~/.scangrab_xul
	FETCH_RESULT=$?

	if [ -f "$_Xvfb" ]; then
		kill $XPID
		export DISPLAY="$SAVE_D"
	fi

	rm -rf ${XUL_TEMP}

	return $FETCH_RESULT
}
#############################################
#####@ORIGINAL-FILE 'operations/batch'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

batch() {
	# $2 is a file. Read it in line by line as $1 and $2.
	while read chunk; do
		eval auto $chunk
	done < $1
}
#############################################
#####@ORIGINAL-FILE 'operations/scrape'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
scrape() {
	for module in ${MODS[@]}; do
		auto_${module} "$@"
		RETCHECK=$?
		if [ $RETCHECK = 1 ]; then
			scrape_${module} "$@"
		fi
	done
}
#############################################
#####@ORIGINAL-FILE 'operations/help'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

help() {
	if [ ! "$MODULE" = "a" ]; then
		${MODULE}_help
		exit 1
	fi

	type 1
	echo -e "Usage:"
	type 0
	echo -ne "     $0     "
	color 3
	echo -ne "[FLAGS]     "
	color 5
	echo -e "URL/Parameters"
	type 0
	type 1
	echo -e "Operations:"
	type 0
	echo -e "     By default, this tool will download a manga from a URL."
	echo -e "     If it can detect what module should be used, it will choose that."
	echo ""
	color 3
	echo -e "     -b     Download as batch list"
	type 0
	echo -e "          Takes a file with URLs which will be run as downloads with auto."
	echo ""
	color 3
	echo -e "     -s     Scrape list of manga from page"
	type 0
	echo -e "          Will take a manga's page and scrape chapters to"
	echo -e "          a file named batch.txt. Some modules may handle this"
	echo -e "          differently - consult module help for more info."
	echo ""
	color 3
	echo -e "     -l     Login to site"
	type 0
	echo -e "          REQUIRES a module (-m) argument. It will ask to login."
	echo -e "          This will generate a cookie jar as ./cookiejar so you can download"
	echo -e "          things that are normally restricted (like Niconico Manga, or ExH)"
	echo ""
	color 3
	echo -e "     -U     Upgrade scangrab"
	type 0
	echo -e "          Checks github for a newer copy of scangrab and"
	echo -e "          updates itself. Make sure you have write permission."
	echo -e "          There is no short form of this action."
	echo ""
	color 3
	echo -e "     -mMODULE    Use specified module 'MODULE'"
	type 0
	echo -e "          Use MODULE to perform any operations."
	echo -e "          This bypasses the autodetection code."
	color 3
	echo ""
	echo -e "     -fFORMAT    Output as 'FORMAT'"
	type 0
	echo -e "          Outputs the resulting download to a specified format."
	echo -e "          Valid options are:"
	echo -e "              zip cbz 7z cb7 tar cbt raw"
	echo -e "          The following are recognized but ignored:"
	echo -e "              rar cbr"
	echo ""
	color 3
	echo -e "     -cCOOKIES   Load alternate file for cookies."
	type 0
	echo -e "          Loads from a cookies.txt compatible format."
	color 3
	echo ""
	echo -e "     -h   Display help"
	type 0
	echo -e "          Print all of this stuff."
	echo ""
	color 3
	echo -e "     -u   Usage"
	type 0
	echo -e "          Print a short version of help."
	echo ""
	type 0
	color 3
	echo -e "     -e   Returns immediately."
	type 0
	echo -e "          Used in development for testing internal functions."
	echo -e "          Please don't use unless you know what you're doing."
	echo ""
	type 1
	echo -e "Download Modules:"
	type 0
	for mod in "${MODS[@]}"; do
		longname=$(temp=\$${mod}_longname && eval echo $temp)
		url=$(temp=\$${mod}_url && eval echo $temp)
		broke=$(temp=\$${mod}_state && eval echo $temp)
		filter=$(temp=\$${mod}_filt && eval echo $temp)
		note=$(temp=\$${mod}_note && eval echo $temp)

		echo -ne "     Module Name:                "
		color 3
		echo -e "$mod"

		type 0
		echo -ne "          Long Name:             "
		color 4
		echo -e "$longname"

		type 0
		echo -ne "          Site(s) Used with:     "
		color 5
		echo -e "$url"
		type 0

		type 0
		echo -ne "          Site(s) Current state: "
		if [ "$broke" = "1" ]; then
			color 2
			echo -e "Works"
		elif [ "$broke" = "2" ]; then
			color 3
			echo -e "InDev"
		else
			color 1
			echo -e "Broken"
		fi

		if [ ! "$note" = "" ]; then
			type 0
			echo -e "          Site Note:             $note"
		fi

		type 0
		if [ "$filter" = "1" ]; then
			echo -e "          Supports filters for scrape"
		fi

		echo ""
	done
	type 1
	echo -e "System Tools"
	type 0
	if [ ! "$_wget" = "" ]; then
		echo -e "     wget:                $_wget"
	fi
	if [ ! "$_curl" = "" ]; then
		echo -e "     curl:                $_curl"
	fi
	if [ ! "$_aria" = "" ]; then
		echo -e "     aria2c:              $_aria"
	fi
	echo -ne "     Will use:            "
	if [ $_FETCHTOOL = 1 ]; then
		echo -ne "wget"
		if [ $_BUSYBOX = 1 ]; then
			echo -ne ", busybox"
		else
			echo -ne ""
		fi
	elif [ $_FETCHTOOL = 2 ]; then
		echo -ne "curl"
	elif [ $_FETCHTOOL = 3 ]; then
		echo -ne "aria2c"
	else
		echo -ne "no tool. Install one of: "
	fi
	echo " (wget, curl, aria2c)"
	echo -e "     JS fetching tool:    $_JSCOMMAND"
	echo -ne "     Check broken images: "
	if [ $_CHECK_VALID = 1 ]; then
		if [ -f $_identify ]; then
			echo -ne "imagemagick ($_identify)"
		fi
		echo ""
	else
		echo "no"
	fi
	echo -ne "     Color:               "
	if [ $_COLOR = 1 ]; then
		color 1
		echo -ne "y"
		color 2
		echo -ne "e"
		color 3
		echo -ne "s"
		color 4
		echo -ne " "
		echo -ne "("
		color 5
		echo -ne "d"
		color 6
		echo -ne "u"
		color 1
		echo -ne "m"
		color 2
		echo -ne "b"
		color 3
		echo ")"
		type 0
	elif [ $_COLOR = 2 ]; then
		color 1
		echo -ne "y"
		color 2
		echo -ne "e"
		color 3
		echo -ne "s"
		color 4
		echo -ne " "
		echo -ne "("
		color 5
		echo -ne "t"
		color 6
		echo -ne "p"
		color 1
		echo -ne "u"
		color 2
		echo -ne "t"
		color 3
		echo ")"
		type 0
	else
		echo "no (TERM='$TERM')"
	fi
	type 1
	echo -ne "License / Info"
	type 0
	echo ""
	echo -e "     Copyright (C) 2015     Jon Feldman/@chaoskagami"
	echo ""
	echo -e "     This program is free software: you can redistribute it and/or modify"
	echo -e "     it under the terms of the GNU General Public License as published by"
	echo -e "     the Free Software Foundation, either version 3 of the License, or"
	echo -e "     (at your option) any later version."
	echo ""
	echo -e "     This program is distributed in the hope that it will be useful,"
	echo -e "     but WITHOUT ANY WARRANTY; without even the implied warranty of"
	echo -e "     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the"
	echo -e "     GNU General Public License for more details."
	echo ""
	echo -e "     You should have received a copy of the GNU General Public License"
	echo -e "     along with this program.  If not, see <http://www.gnu.org/licenses/>"
	echo ""
	echo -e "     The latest version of scangrab can always be found at the github"
	echo -e "     page here: https://github.com/chaoskagami/scangrab"
	echo ""
	echo -e "     Build: $type, $branch, $rev"
}
#############################################
#####@ORIGINAL-FILE 'operations/usage'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

usage() {
	echo "Usage:    $0 [-hu] [-U] [-bsl] [-mmodule] args ..."
	echo "Modules"
	echo "  ${MODS[@]}"
}
#############################################
#####@ORIGINAL-FILE 'operations/auto'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

auto() {
	for module in ${MODS[@]}; do
		auto_${module} "$@"
		RETCHECK=$?
		if [ $RETCHECK = 1 ]; then
			dl_${module} "$@"
			return 0
		fi
	done
	return 1
}
#############################################
#####@ORIGINAL-FILE 'operations/configure_env'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

COOKIEJAR="$(pwd)/cookiejar"
HAVE_IMAGICK=0
FETCH_RESULT=0

if [ "$VERBOSE" = "" ]; then
	VERBOSE=0
fi

# These values are used by support/core but are controlled by modules.
message=""
use_cookies=0
SAFETY_HACKS=0
CLOBBER=0

# Anything which starts with an underscore is internal and shouldn't be touched.
_COLOR=0
_FETCH_CMD=""
_CHECK_VALID=0
_FETCHTOOL=0
_BUSYBOX=0
_SPINNER_CHAR="|"
_NUM=0

# Tools. Use these variables instead of actual command names.
_tput="$(which --skip-alias tput 2>/dev/null)"
_identify="$(which --skip-alias identify 2>/dev/null)"
_convert="$(which --skip-alias convert 2>/dev/null)"
_wget="$(which --skip-alias wget 2>/dev/null)"
_curl="$(which --skip-alias curl 2>/dev/null)"
_aria="$(which --skip-alias aria2c 2>/dev/null)"

# Used for things which need to be parsed.
_firefox="$(which --skip-alias firefox 2>/dev/null)"
_palemoon="$(which --skip-alias palemoon 2>/dev/null)"
_xulrunner="$(which --skip-alias xulrunner 2>/dev/null)"
_HAVE_JSCOMMAND=0 # 1 is a xul browser.
_JSCOMMAND="unsupported"

if [ -f "$_firefox" ]; then
	_JSCOMMAND="$_firefox"
	_HAVE_JSCOMMAND=1
elif [ -f "$_palemoon" ]; then
	_JSCOMMAND="$_palemoon"
	_HAVE_JSCOMMAND=1
elif [ -f "$_xulrunner" ]; then
	_JSCOMMAND="$_xulrunner"
	_HAVE_JSCOMMAND=1
fi

#########################
# This function is required earlier than usual, so it lives here.

function temp() {
	type="$1"
	if [ "$type" = "d" ]; then
		NAME="$(mktemp -d -p "$(pwd)")"
	elif [ "$type" = "f" ]; then
		NAME="$(mktemp -p "$(pwd)")"
	fi
	echo "$(basename "$NAME")"
}

#########################
# Figure out our mountpoint so we know the limits of the FS.

MINFO="$(temp f)"
mount -l > "$MINFO"

CURRENT_DIR="$(readlink -f "$(pwd)")"

grep "$CURRENT_DIR" "$MINFO" >/dev/null 2>&1
M_FOUND=$?
while [ ! $M_FOUND = 0 ]; do
	PLEN=$(echo $CURRENT_DIR | tr '/' '\n' | wc -l)
	PLEN=$((PLEN - 1))

	CURRENT_DIR="$(echo -n $CURRENT_DIR | tr '/' '\n' | head -n$PLEN | tr '\n' '/' | sed 's|/$||g')"

	grep "$CURRENT_DIR" "$MINFO" >/dev/null 2>&1
	M_FOUND=$?
done

LINE="$(grep "$CURRENT_DIR" "$MINFO")"

declare -a splitinfo
splitinfo=($(echo "$LINE" | sed -e "s| on |\n|" -e "s| type |\n|g"))

MOUNTPOINT=${splitinfo[2]}
FSTYPE=${splitinfo[3]}

rm "$MINFO"
unset splitinfo

######################

# Used for headless horsemen.
_Xvfb="$(which --skip-alias Xvfb 2>/dev/null)"

for ((i = 1, n = 2;; n = 1 << ++i)); do
	if [[ ${n:0:1} == '-' ]]; then
		MAX_INT=$(((1 << i) - 1))
		break
	fi
done

type="raw"

if [ "$TERM" = "xterm" ] || [ "$TERM" = "linux" ] || [ "$TERM" = "screen" ]; then
	_COLOR=1
	if [ -f $_tput ]; then
		# Better method.
		_COLOR=2
	fi
fi

if [ -f "$_identify" ]; then
	_CHECK_VALID=1
fi

if [ -f "$_identify" ]; then
	HAVE_IMAGICK=1
fi

if [ ! "$_wget" = "" ]; then
	common_opts=" --quiet --no-cache --user-agent=\"Mozilla/5.0\" -t 1 -T 10 --random-wait "

	if [ ! "$($_wget --help 2>&1 | grep busybox)" = "" ]; then
		echo "[Warning] Your system wget is busybox, which can't actually do some things like reject cache and retry."
		common_opts=" -q -U \"Mozilla/5.0\""
		_BUSYBOX=1
	fi

	_FETCH_CMD="$_wget $common_opts"
	_FETCHTOOL=1
else
	if [ ! "$_curl" = "" ]; then
			_FETCH_CMD=$_curl
			_FETCHTOOL=2
	else
		if [ ! "$_aria" = "" ]; then
				_FETCH_CMD=$_aria
				_FETCHTOOL=3
		fi
	fi
fi
#############################################
#####@ORIGINAL-FILE 'operations/upgrade_self'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

upgrade_self() {
	if [ "$type" = "repo" ]; then
		echo "[Upgrade] You're in a git repo. Use 'git pull' instead."
		exit 0
	fi
	URL="https://raw.githubusercontent.com/chaoskagami/scangrab/$branch/dist/scangrab.$type"
	LOG="https://raw.githubusercontent.com/chaoskagami/scangrab/$branch/CHANGES-SNAP"

	echo "[Upgrade] Checking this scangrab's sha256sum..."

	fetch_no_ops "${URL}.sha256sum" "${0}.new.sha256sum"
	if [ ! $FETCH_RESULT = 0 ]; then
		echo "[Upgrade] Error fetching sha256. Either you don't have +w permission,"
		echo "[Upgrade] or your internet is down. Either way, aborting."
		exit 1
	fi

	this_sha256="$( cat "${0}" | sha256sum )"
	gith_sha256="$( cat "${0}.new.sha256sum" )"

	rm "${0}.new.sha256sum"

	if [ "$this_sha256" = "$gith_sha256" ]; then
		echo "[Upgrade] Not required. Same sha256 as upstream."
		exit 0
	else
		echo "[Upgrade] Doesn't match upstream. Fetching..."
		fetch_no_ops "$URL" "${0}.new"
		if [ ! $FETCH_RESULT = 0 ]; then
			echo "[Upgrade] Fetch failed. Error code: $R. Do you have write permission?"
			rm "${0}.new" "${0}.new.sha256sum" 2>/dev/null
			exit $R
		fi
		echo "[Upgrade] Downloaded replacement. Checking replacement's sha256sum..."
		new_sha256="$( cat "${0}.new" | sha256sum )"
		if [ "$new_sha256" = "$gith_sha256" ]; then
			echo "[Upgrade] Matched. Replacing self..."
			cp -f "${0}.new" "${0}"
			R=$?
			if [ ! $R = 0 ]; then
				echo "[Upgrade] Couldn't replace. cp error: $R. Abort."
				rm "${0}.new" "${0}.new.sha256sum" 2>/dev/null
				exit $R
			fi
			echo "[Upgrade] Sync to avoid race conditions..."
			sync
			echo "[Upgrade] Checking to make sure it has been replaced..."
			rm "${0}.new" "${0}.new.sha256" 2>/dev/null
			new_sha256="$(cat "${0}" | sha256sum )"
			if [ ! "$new_sha256" = "$gith_sha256" ]; then
				echo "[Upgrade] Sanity check failed. This is not normal."
				exit 1
			fi
			echo "[Upgrade] Succeeded. Displaying changelog, then exiting..."
			echo "--- Changelog ----------------------"
			fetch_no_ops "$LOG" "/tmp/scangrab-changelog-$(echo ${new_sha256} | sed 's/ .*//g')"
			if [ ! $FETCH_RESULT = 0 ]; then
				echo "[Upgrade] Failed to fetch changelog."
				exit 0
			fi
			cat "/tmp/scangrab-changelog-$(echo ${new_sha256} | sed 's/ .*//g')" | sed -n '/== START ==/,/== END ==/p' | head -n-1 | tail -n+2
			rm "/tmp/scangrab-changelog-$(echo ${new_sha256} | sed 's/ .*//g')" 2>/dev/null
			exit 0
		else
			echo "[Upgrade] Failed. sha256 does not match."
		fi
	fi
	exit 1
}
#############################################
#####@ORIGINAL-FILE 'operations/mod_login'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#

mod_login() {
	for module in ${MODS[@]}; do
		if [ "$1" = "$module" ]; then
			if [ "$( eval echo \$${module}_uselogin )" = "1" ]; then
				echo -ne "[$module] Username: "
				read username

				echo -ne "[$module] Password for $username (will not echo): "
				read -s password

				login_${module} "$username" "$password"
				exit 0
			else
				echo "$module does not need login."
				exit 0
			fi
		fi
	done
	echo "No such module."
	exit 1
}
#############################################
#####@AUTOGENERATED 'MODS'
MODS=(mangabox batoto niconicoseiga eh fakku dynsc foolsl mpark imgur booru)
#############################################
#############################################
#####@ORIGINAL-FILE 'modules/mangabox'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# I need to add an option later for page splitting with imagemagick.
# The whole two-in-one thing is annoying.

mangabox_longname="Mangabox"
mangabox_url="www.mangabox.me"
mangabox_state=1
mangabox_filt=0
mangabox_note="Automatically splits pages with IM. The PC webpage is broken badly. Don't report it."

auto_mangabox() {
	if [ -n "`echo $1 | grep 'mangabox.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# MangaBox
		return 1
	fi

	return 0
}

dl_mangabox() {

	echo "[MangaBox] Getting list of pages..."

	TEMP="$(temp f)"

	fetch "$1" "$TEMP"

	fullname="$(cat "$TEMP" | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char | remove_illegal)"

	is_done "$fullname"
	R=$?
	if [ $R = 1 ]; then
		echo "[MangaBox] Already downloaded. Skipping."
		return 0
	fi

	declare -a list
	list=($(cat "$TEMP" | grep 'class="jsNext"' | sed -e 's|.*<li><img src="||g' -e 's|?.*||g'))

	mkdir -p "$fullname"
	cd "$fullname"

	echo -n "[MangaBox] Downloading '$fullname'... "

	NUM=0
	for page in ${list[@]}; do
		NUM=$((NUM + 1))
		VALID=1
		while [ ! $VALID = 0 ]; do
			fetch $page
			verify $(basename $page)
			VALID=$?
		done

		if [ $HAVE_IMAGICK = 1 ]; then
			spinner "Download:$NUM"

			$_convert $(basename $page) -crop 2x1@ +repage +adjoin "${NUM}_%d.png"

			spinner "Reformat:$NUM"

			rm $(basename $page)

			spinner "Collat:$NUM"

			mv ${NUM}_0.png ${NUM}_b.png
			mv ${NUM}_1.png ${NUM}_a.png
		else
			spinner "$NUM"
		fi
	done

	cd ..

	spinner_done

	rm tmp.htm

	cbz_make "$fullname"
}

scrape_mangabox() {

	echo "[MangaBox] Not yet supported, sorry."
	exit 1
}
#############################################
#####@ORIGINAL-FILE 'modules/booru.mods/danbooru'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# This should set up any variables needed by this module.

booru_danbooru_init() {
	final[0]="$source/posts.json?tags="
}


# This should build a URL which can download a page; call dl_booru_page,
# And then extract the pages. It should not itself do the download.

booru_danbooru_page() {
	final[4]=$1

	if [ $RESUME = 1 ]; then
		# Assume this is done.
		return 1
	fi

	if [ $is_pool = 0 ]; then
		dl_booru_page "$(echo ${final[@]} | tr -d ' ')" "page${range}.json"

		BEFORE=$(wc -l meta.txt | sed 's| .*||g')
		cat "page${range}.json" | tr ',' "\n" | grep '"id"' | sed 's|.*:||g' >> meta.txt
		AFTER=$(wc -l meta.txt | sed 's| .*||g')

		if [ "$BEFORE" = '' ]; then
			BEFORE=0
		fi
		PAGE_SIZE=$((AFTER - BEFORE))

		# We preprocess metadata.
		cat "page${range}.json" | sed "s|},{|}]\n[{|g" | grep '"id"' > tmp
		mv tmp "page${range}.json"

		LEN=0
		while read line; do
			FIRST=$((PAGE_SIZE - LEN))
			id="$(cat meta.txt | tail -n $FIRST | head -n 1)"
			echo "$line" > meta/meta_${id}.json
			LEN=$((LEN + 1))
		done < "page${range}.json"

		rm "page${range}.json"

		# No more pages.
		if (( BEFORE == AFTER )); then
			return 1
		fi

		# Still more.
		return 0
	else
		final[0]="${source}/pools.json?commit=Search&search[order]=updated_at&search[name_matches]="

		dl_booru_page "$(echo ${final[@]} | tr -d ' ')" "${final[1]}.json"

		cat "${final[1]}.json" | tr ',' "\n" | grep '"post_ids"' | sed -e 's|.*:||g' -e 's|"||g' | tr ' ' "\n" >> meta.txt
	fi
}

# This should download a page's meta info. If there isn't any, it should copy the ID to a file named as the ID.

booru_danbooru_meta() {
	declare -a base
	base[0]="$source/posts/"
	base[1]="${1}.json"

	if [ ! -e "meta_${base[1]}" ]; then
		message="fetch ${1}"
		spinner "${DONE} -> ${LINES}"
		message=""

		dl_booru_page "$(echo ${base[@]} | tr -d ' ')" "meta_${base[1]}"
	fi
}

booru_danbooru_content() {
	declare -a base
	base[0]="$source/posts/"
	base[1]="${1}.json"

	url_img="$(cat "${META_DIR}/meta_${base[1]}" | tr ',' "\n" | grep '"file_url"' | sed -e 's|.*:||g' -e "s|\"||g")"
	file_ext="$(cat "${META_DIR}/meta_${base[1]}" | tr ',' "\n" | grep '"file_ext"' | sed 's|.*:||g')"

	if [ "$file_ext" = "" ] || [ "$url_img" = "" ]; then
		spinner_done
		echo "[Booru] ID:${1} appears to be deleted/hidden. Skipping."

		# ID is deleted. Skip.
		return
	fi

	if [ ! -e "image_${1}.${file_ext}" ]; then
		dl_booru_page "${source}${url_img}" "image_${1}.${file_ext}"
	fi
}
#############################################
#####@ORIGINAL-FILE 'modules/batoto'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

batoto_longname="Batoto"
batoto_url="bato.to"
batoto_filt=0

if [ ! $_HAVE_JSCOMMAND = 0 ]; then
	batoto_state=2
	batoto_note="Partially broken - no scraping functionality."
else
	batoto_state=0
	batoto_note="Broken. Get one of these: 'firefox', 'palemoon', 'xulrunner'."
fi

auto_batoto() {
	if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Batoto
		return 1
	fi

	return 0
}

dl_batoto() {
	# Batoto breaks their stuff. A LOT. Recently, they broke static serving - JS now required.

	TEMP="$(temp f)"

	fetch_jscommand "$1" "$TEMP"

	folder="$(cat "$TEMP" | grep -C0 "<title>" | sed -e "s/^[[:space:]]*<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e "s/[[:space:]]*$/\n/" | entity_to_char | remove_illegal)"

	PAGES="$(cat "$TEMP" | grep -C0 'page 1</option>' | head -n1 | sed -e 's|</option>||g' -e 's|.*>page ||g')"

	rm PAGE

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[Batoto] Already downloaded. Skipping."
		return 0
	fi

	mkdir -p "$folder"
	cd "$folder"

	CUR=1
	RET=0

	echo -n "[Batoto] Downloading '$folder' "

	BASE="$1"

	while (( $CUR <= $PAGES )); do
		TEMP="$(temp f)"

		fetch_jscommand "${BASE}_${CUR}" "$TEMP"

		img="$(grep -C0 'z-index: 1003' "$TEMP" | sed -e 's/^[[:space:]]*<img src="//g' -e 's/".*$//g')"

		# If this 404's, fetch will return non-zero. Thus, loop breaks.
		fetch "$img"
		RET=$?

		rm "$TEMP"

		spinner "$CUR / $PAGES"

		if [ $RET = 0 ]; then
			CUR=$((CUR + 1))
		fi
	done

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_batoto() {
	echo -n "[Batoto] Scraping Chapters..."

	fetch "$1" scrape.htm

	# Batoto sometimes gives out gunzips. We need to account for that... =_=
	if [ "$(mimetype scrape.htm)" = "application/x-gzip" ]; then
		mv $CUR.htm $CUR.htm.gz
		gunzip $CUR.htm.gz
		message=" :/"
	fi

	grep -A 2 'Sort:' scrape.htm >> batch.txtr

	# Delete the useless lines.
	sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr

	# Remove Language lines.
	sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
	sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr

	# Edit up URL.
	sed -i "s|<a href=\"||g" batch.txtr
	sed -i "s|\" title=.*||g" batch.txtr

	# Delete blank lines/space lines
	sed -i '/^[[:space:]]*$/d' batch.txtr

	# Strip.
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	sed -i 's|^--$||g' batch.txtr

	# Delete Blank lines.
	sed -i '/^$/d' batch.txtr

	cat batch.txtr | reverse_lines > batch.txtf

	if [ "$2" = "" ]; then
		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf

		cat batch.txtf >> batch.txt
	else
		echo -ne "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Applying Language Filter '$2'..."

		grep -A 1 "$2" batch.txtf > batch.txtf2

		# Delete '--' lines
		sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2

		# Delete Blank lines.
		sed -i '/^$/d' batch.txtf2

		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf2

		cat batch.txtf2 >> batch.txt
	fi

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr batch.txtf*

	echo -en "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"

	for ((n=0;n < ${#2}; n++)); do
		echo -en '\b'
	done

	echo -e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/niconicoseiga'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# NiconicoSeiga is odd in that an account is required. Therefore, I had to implement cookiejars.

niconicoseiga_longname="Niconico Seiga"
niconicoseiga_url="seiga.nicovideo.jp"
niconicoseiga_state=1
niconicoseiga_filt=0
niconicoseiga_note="Please login first."

# Need to login.
niconicoseiga_uselogin=1

auto_niconicoseiga() {
	if [ -n "`echo $1 | grep 'seiga.nicovideo.jp' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Niconico Seiga
		return 1
	fi

	return 0
}

login_niconicoseiga() {
	use_cookies=1
	s_login "mail_tel" "password" "$1" "$2" "https://secure.nicovideo.jp/secure/login"
}

dl_niconicoseiga() {
	use_cookies=1

	TEMP="$(temp f)"

	fetch $1 "$TEMP"

	fullname="$(cat "$TEMP" | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char | remove_illegal)"

	if [ ! -f "$TEMP" ]; then
		echo "[Niconico] No page found, aborting."
		return 1
	fi

	echo "[Niconico] Extracting image list..."

	TEMP2="$(temp f)"

	cat "$TEMP" | grep 'data-image-id' | sed -e 's|^[[:space:]]*data-image-id="||g' -e 's|"[[:space:]]*$||g' > "$TEMP2"

	if [ "$(cat "$TEMP2")" = "" ]; then
		echo "[Niconico] No images in page. Did you login?"
		return 1
	fi

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[Niconico] Already downloaded. Skipping."
		return 0
	fi

	mkdir -p "$fullname"
	cd "$fullname"

	echo -n "[Niconico] Downloading '$fullname' (from $name) "

	COUNT=0
	while read ID; do
		COUNT=$((COUNT + 1))
		fetch "http://lohas.nicoseiga.jp/thumb/${ID}p" "${COUNT}.jpg"
		spinner $COUNT
	done < "../$TEMP2"

	spinner_done

	cd ..
	rm "$TEMP" "$TEMP2"

	cbz_make "$fullname"
}

scrape_niconicoseiga() {
	COOKIES=1

	echo "[Niconico] Scraping chapters..."

	fetch "$1" "-" | grep 'class="episode"' | sed -e 's|.*<a href="||g' -e 's|?.*||g' -e 's|/watch|http://seiga.nicovideo.jp/watch|g' >> batch.txt

	echo "[Niconico] Done."
}
#############################################
#####@ORIGINAL-FILE 'modules/eh'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

eh_longname="E-H / ExH"
eh_url="g.e-hentai.org / exhentai.org"
eh_state=1
eh_filt=0
eh_note="Logging in will result in HQ images, cap detection, and if you can, Ex use."

eh_uselogin=1

login_eh() {
	use_cookies=1

	# Lots of extra shat they use to prevent bots. If it gets updated; let me know so I can fix it.

	s_login "UserName" "PassWord" "$1" "$2" "https://forums.e-hentai.org/index.php?act=Login&CODE=01" \
		"CookieDate=1&Privacy=1&b=&bt=&referer=https://forums.e-hentai.org/?act=Login&temporary_https="
	user="$(fetch "https://forums.e-hentai.org/?act=idx" "-" | grep 'Logged in as' | sed -e 's|.*showuser=||' -e 's|<.*||' -e 's|.*>||g')"
	if [ ! "$user" = "" ]; then
		echo -e "\n[E-H] Logged in as: $user"
		exit 0
	else
		echo -e "\n[E-H] Login seems to have failed."
		exit 1
	fi
}

limitcheck_eh() {
	use_cookies=1

	line="$(fetch "http://g.e-hentai.org/home.php" "-" | grep "<p>You are currently at")"

	LIMIT_AT="$(echo $line | sed -e 's|.*<p>You are currently at <strong>||g' -e 's|</strong>.*||g')"
	LIMIT_OF="$(echo $line | sed -e 's|.*</strong> towards a limit of <strong>||g' -e 's|</strong>.*||g')"
	LIMIT_REGEN="$(echo $line | sed -e 's|.*</strong>. This regenerates at a rate of <strong>||g' -e 's|</strong>.*||g')"
}

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# E-H
		return 1
	elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Ex
		return 1
	fi

	return 0
}

# This returns a code corresponding to whether it can fetch from there, waiting out bans if needed.
check_low_eh() {
	if [ "$check_eh_done" = "1" ]; then
		return 0
	fi

	LOGGEDIN=0
	use_cookies=0
	if [ -f "$COOKIEJAR" ]; then
		if [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
			echo -ne "[E-H] Checking for Ex cookies..."
			grep 'exhentai.org' "$COOKIEJAR" | grep 'ipb_member_id' >/dev/null
			RES=$?
			if [ $RES = 0 ]; then
				echo "yes"
				use_cookies=1
				echo -ne "[E-H] Checking for access..."

				TEMP="$(temp f)"

				# This is a good way of checking if it succeeded.
				fetch "exhentai.org" "$TEMP"
				cat "$TEMP" | grep "The X Makes It Sound Cool" >/dev/null 2>&1
				RES=$?
				if [ ! $RES = 0 ]; then
					echo -n "no, "
					if [ "$(sha256sum "$TEMP" | sed 's| .*||g')" = "a279e4ccd74cffbf20baa41459a17916333c5dd55d23a518e7f10ae1c288644f" ]; then
						echo "panda. <3"
						return 1
					elif [ ! "$(grep 'Your IP address has been temporarily banned' "$TEMP")" = "" ]; then
						echo "tempban. Hilarious."
						ban_eh_wait
						LOGGEDIN=1
						check_low_eh "$1"
						R=$?
						return $R
					else
						echo "format update, world is ending?"
						return 1
					fi
				else
					LOGGEDIN=1
					echo "yes"
				fi

				rm "$TEMP"

				grep "exhentai\.org.*uconfig" "$COOKIEJAR" >/dev/null 2>&1
				RES=$?
				if [ $RES = 0 ]; then
					echo "[E-H] Warning - uconfig is set in cookie jar. This will cause problems."
					if [ "$COOKIEJAR" = "$(pwd)/cookiejar" ]; then
						echo -n "[E-H] Attempting to automatically fix the issue..."
						sed -i "/exhentai\.org.*uconfig/d" "$COOKIEJAR"
						grep "exhentai\.org.*uconfig" "$COOKIEJAR" >/dev/null 2>&1
						RES=$?
						if [ ! $RES = 0 ]; then
							echo "done"
						else
							echo "failed"
						fi
					else
						echo "[E-H] User specified custom cookie jar. Will not automatically fix."
						echo "[E-H] Subsequent download may fail, or get stuck in retry loop."
					fi
				fi
			else
				echo "no"
				echo -ne "[E-H] Checking for E-H cookies..."
				grep 'e-hentai.org' "$COOKIEJAR" | grep 'ipb_member_id' >/dev/null
				RES=$?
				if [ $RES = 0 ]; then
					echo "yes, copying them"
					grep 'e-hentai.org' "$COOKIEJAR" >> cookiejar.edit
					sed 's|e-hentai.org|exhentai.org|g' cookiejar.edit >> cookiejar
					rm cookiejar.edit

					echo -ne "[E-H] Fixed cookies. Checking for access..."
					use_cookies=1

					TEMP="$(temp f)"
					fetch "exhentai.org" "$TEMP"

					grep "The X Makes It Sound Cool" "$TEMP" >/dev/null 2>&1
					RES=$?
					if [ ! $RES = 0 ]; then
						echo -n "no, "
						if [ "$(sha256sum "$TEMP" | sed 's| .*||g')" = "a279e4ccd74cffbf20baa41459a17916333c5dd55d23a518e7f10ae1c288644f" ]; then
							echo "panda. <3"
							return 1
						elif [ ! "$(grep 'Your IP address has been temporarily banned' "$TEMP")" = "" ]; then
							echo "tempban. Hilarious."
							ban_eh_wait
							LOGGEDIN=1
							check_low_eh "$1"
							R=$?
							return $R
						else
							echo "format update, world is ending?"
							return 1
						fi
					else
						LOGGEDIN=1
						echo "yes"
					fi

					rm "$TEMP"

					grep "exhentai\.org.*uconfig" "$COOKIEJAR" >/dev/null 2>&1
					RES=$?
					if [ $RES = 0 ]; then
						echo "[E-H] Warning - uconfig is set in cookie jar. This will cause problems."
						if [ "$COOKIEJAR" = "$(pwd)/cookiejar" ]; then
							echo -n "[E-H] Attempting to automatically fix the issue..."
							sed -i "/exhentai\.org.*uconfig/d" "$COOKIEJAR"
							grep "exhentai\.org.*uconfig" "$COOKIEJAR" >/dev/null 2>&1
							RES=$?
							if [ ! $RES = 0 ]; then
								echo "done"
							else
								echo "failed"
							fi
						else
							echo "[E-H] User specified custom cookie jar. Will not automatically fix."
							echo "[E-H] Subsequent download may fail, or get stuck in retry loop."
						fi
					fi
				else
					echo "no."
					echo "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
					return 1
				fi
			fi
		else
			grep 'e-hentai.org' "$COOKIEJAR" | grep 'ipb_member_id' >/dev/null
			RES=$?
			if [ $RES = 0 ]; then
				echo "[E-H] We seem to have cookies...using them."
				LOGGEDIN=1
				use_cookies=1
			fi
		fi
	elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		echo "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
		exit 1
	fi

	if [ $LOGGEDIN = 0 ]; then
		echo "[E-H] Not logged in. Expect random failures at anoymous quota cap."
	fi

	check_eh_done=1

	return 0
}

check_eh() {
	# This runs check_low_eh, and will abort on error.
	check_low_eh "$1"
	R=$?
	if [ $R = 1 ]; then
		exit 1
	fi
}

ban_eh_wait() {
	ipbanned="$(fetch "http://g.e-hentai.org/home.php" "-" | grep "^Your IP address has been temporarily banned for")"
	if [ ! "$ipbanned" = "" ]; then
		# Hoo boy, you like downloading stuff, yeah?
		# We're IP banned now until their clock is up. Figure out how long.
		BANSTR="$(echo "$ipbanned" | grep "^Your IP address has been temporarily banned for" | sed -e 's|.*The ban expires in ||' -e 's| and||g' )"

		echo "$BANSTR" | grep "days"
		R=?
		if [ $R = 0 ]; then
			echo "Banned for at least a day, apparently. Just giving up."
			exit 1
		fi
		echo "$BANSTR" | grep "months"
		R=?
		if [ $R = 0 ]; then
			echo "Banned for months? Are you trying to circumvent a ban? It won't work."
			exit 1
		fi
		echo "$BANSTR" | grep "years"
		R=?
		if [ $R = 0 ]; then
			echo "Now would be a good time to find another source to use, Mr. Banned-for-life."
			exit 1
		fi

		declare -a ban_time
		ban_time=($BANSTR)

		ban_str=""

		seconds=0

		for (( i=0 ; i < ${ban_time#} ; i+=2 )); do
			time="${ban_time[$i]}"
			unit="${ban_time[$((i+1))]}"

			ban_str="${ban_str}${time}${unit:0:1}"

			if [ "$unit" = "seconds" ]; then
				seconds=$((seconds + $time))
			elif [ "$unit" = "minutes" ]; then
				time=$((time * 60))
				seconds=$((seconds + $time))
			elif [ "$unit" = "hours" ]; then
				time=$((time * 60 * 60))
				seconds=$((seconds + $time))
			fi
		done

		echo "[E-H] Banned for $ban_str seconds. :<"
		echo -n "[E-H] Sleeping..."

		while (( $seconds > 0 )); do
			seconds=$((seconds - 1))
			sleep 1s
			spinner "${seconds}"
		done

		echo -e "\n[E-H] Resuming..."
	fi
}

dl_eh_wait() {
	ban_eh_wait

	# Check to see (again) if CUR > MAX.
	# If it is, sit around and sleep until we have enough credits.
	limitcheck_eh
	LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))

	# It is totally possible for images to take an absurd chunk of quota.
	# I still have no clue how it is calculated, but I've never seen one
	# image cost more than 100 quota.
	if (( $LIMIT_LEFT < 100 )); then
		message=":#"

		zzz=$(( LIMIT_REGEN * 60 ))

		while [ ! $zzz = 0 ]; do
			spinner "Over, wait ${zzz}s"
			sleep 1s
			zzz=$(( zzz - 1 ))
		done

		limitcheck_eh
		LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))
	fi
}

dl_eh() {
	sitepage=$1

	check_eh "$1"

	if [ $LOGGEDIN = 1 ]; then
		limitcheck_eh
		LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))
		THIS_COUNT=$MAXPAGES
		if [ $LOGGEDIN = 1 ]; then
			THIS_COUNT=$(( MAXPAGES * 100 ))
		fi

		echo "[E-H] Limit check: ${LIMIT_AT} / ${LIMIT_OF}, +${LIMIT_REGEN}/min"
	fi

	echo "[E-H] Fetching index page..."

	# This is so that we don't get the 'offensive' warning.
	# XXX - Is this needed for Ex?

	TEMP="$(temp f)"

	# Don't wait here.
	fetch "${sitepage}/?nw=always" "$TEMP"

	while [ ! $FETCH_RESULT = 0 ]; do
		fetch "${sitepage}/?nw=always" "$TEMP"
	done

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
	# the last sed deals with names like fate/stay night which are invalid
	folder="$(cat "$TEMP" | grep 'title>' | sed -e 's/<title>//g'  -e 's/<\/title>//g' -e 's/ - E-Hentai Galleries//g' -e 's/ - ExHentai.org//g' -e 's|/|_|g' | entity_to_char | remove_illegal)"

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[E-H] Already downloaded."
		rm "./$TEMP"
		return 0
	fi

	mkdir -p "$folder"
	cd "$folder"

	page=$(cat "../$TEMP" | sed 's/0 no-repeat\"><a href=\"/\nurl:/' | sed 's/"><img alt.*//g' | grep 'url:' | sed 's/url://g')

	MAXPAGES=$(cat "../$TEMP" | grep 'Length:' | sed -e 's|.*Length:</td><td class="gdt2">||g' -e 's| pages.*||g')

	rm "../$TEMP"

	echo "[E-H] Downloading '$folder'... "

	doneyet=0

	CDNFAIL=0

	if [ "$STARTAT" = "" ]; then
		STARTAT=1
	fi

	extra=""

	CUR=1
	while [ $doneyet = 0 ]; do
		spinner "$CUR / $MAXPAGES"

		dl_eh_wait

		TMPHT="$(temp f)"
		fetch "${page}${extra}" "$TMPHT"

		while [ ! $FETCH_RESULT = 0 ]; do
			# For the most part, we'll not hit this unless there's network issues.
			# However, it can cause a failed fetch to go uncaught, causing next to be ''
			# And if the next loop "succeeds" with a zero length page, it will propogate
			# to next, and the 'done' check will trigger incorrectly.
			message="???"
			spinner "RETR $CUR / $MAXPAGES"

			dl_eh_wait
			fetch "${page}${extra}" "$TMPHT"
		done

		lq="$(cat "$TMPHT" | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'keystamp' -e 'image.php' -e '/im/' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)"
		hq="$(cat "$TMPHT" | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)"
		next="$(cat "$TMPHT" | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)"

		if [ "$VERBOSE" = "1" ]; then
			echo "This page: '$page'"
			echo "LQ Image:  '$lq'"
			echo "HQ Image:  '$hq'"
			echo "Next page: '$next'"
		fi

		if (( $CUR < $STARTAT )); then
			rm "$TMPHT"
			page="$next"
			CUR=$((CUR + 1))
			continue
		fi

		spinner "IMG $CUR / $MAXPAGES"

		if [ ! "$LOGGEDIN" = 1 ] || [ "$hq" = "" ]; then
			dl_eh_wait
			fetch "$lq" "$(basename "$lq")"
		else
			# HQ version; this is the 'Download source resolution' button.
			dl_eh_wait
			fetch "$hq"
		fi

		if [ $FETCH_RESULT = 0 ]; then
			extra=""
			if [ ! "$next" = "" ]; then
				page="$next"
				CUR=$((CUR + 1))
			fi
		else
			# Didn't fetch properly.
			notload=$(cat "$TMPHT" | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")
			extra="?nl=${notload}"
		fi

		rm "$TMPHT"

		# This is a much saner condition than checking if next is null.
		if (( $CUR > $MAXPAGES )); then
			break
		fi
	done

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	# TODO - Implement search functionality.
	FETCH=""

	# Two syntaxes; A URL, or a query followed by a mode string.
	echo "$1" | grep "e.hentai\.org/\?" 2>&1 >/dev/null
	S=$?
	if [ $S = 0 ]; then
		# This is a URL.
		FETCH="${1}&page="
	else
		# This is all broken and needs to be fixed.
		KEYW=""

		# Mode strings: dtagwnicpx
		DEFAULT=1
		M_D=$DEFAULT # Doujinshi
		M_M=$DEFAULT # Manga.
		M_A=$DEFAULT # Artist CG.
		M_G=$DEFAULT # Game CG
		M_W=$DEFAULT # Western
		M_N=$DEFAULT # Non-H
		M_I=$DEFAULT # Imageset
		M_C=$DEFAULT # Cosplay
		M_P=$DEFAULT # That category which nobody uses E-h for (maybe)
		M_X=$DEFAULT # Misc

		check_low_eh "http://exhentai.org/"
		EX_WORKS=$?

		# We *should* use exhentai if the user has access.
		if [ $EX_WORKS = 0 ]; then
			BASE_URL="http://exhentai.org/"
		else
			BASE_URL="http://g.e-hentai.org/"
		fi

		for((i=0; i<${#SUBTYPES}; i++)); do
			INDEX=${SUBTYPES:i:1}
			if [ $i = 0 ] && [ "$INDEX" = "^" ]; then
				# Invert all set values.
				[ $M_D = 0 ] && M_D=1 || M_D=0 # This is equivalent to !M_D in a 1 -> 0 -> 1 sense.
				[ $M_M = 0 ] && M_M=1 || M_M=0
				[ $M_A = 0 ] && M_A=1 || M_A=0
				[ $M_G = 0 ] && M_G=1 || M_G=0
				[ $M_W = 0 ] && M_W=1 || M_W=0
				[ $M_N = 0 ] && M_N=1 || M_N=0
				[ $M_I = 0 ] && M_I=1 || M_I=0
				[ $M_C = 0 ] && M_C=1 || M_C=0
				[ $M_P = 0 ] && M_P=1 || M_P=0
				[ $M_X = 0 ] && M_X=1 || M_X=0
			else
				case "$INDEX" in
					"d")
						[ $M_D = 0 ] && M_D=1 || M_D=0
					;;
					"m")
						[ $M_M = 0 ] && M_M=1 || M_M=0
					;;
					"a")
						[ $M_A = 0 ] && M_A=1 || M_A=0
					;;
					"g")
						[ $M_G = 0 ] && M_G=1 || M_G=0
					;;
					"w")
						[ $M_W = 0 ] && M_W=1 || M_W=0
					;;
					"n")
						[ $M_N = 0 ] && M_N=1 || M_N=0
					;;
					"i")
						[ $M_I = 0 ] && M_I=1 || M_I=0
					;;
					"c")
						[ $M_C = 0 ] && M_C=1 || M_C=0
					;;
					"p")
						[ $M_P = 0 ] && M_P=1 || M_P=0
					;;
					"x")
						[ $M_X = 0 ] && M_X=1 || M_X=0
					;;
				esac
			fi
		done

		MODE_PART="f_doujinshi=$M_D&f_manga=$M_M&f_artistcg=$M_A&f_gamecg=$M_G&f_western=$M_W&f_non-h=$M_N&f_imageset=$M_I&f_cosplay=$M_C&f_asianporn=$M_P&f_misc=$M_X"

		# Read search keywords.
		while [ ! -z "$1" ]; do
			KEYW="$KEYW+\"$(echo $1 | sed -e 's| |+|g' -e 's|:|%3A|g')\""
			shift
		done

		KEYW="$(echo $KEYW | sed 's|^\+||g')"

		FETCH="${BASE_URL}?${MODE_PART}&f_search=${KEYW}&f_apply=Apply+Filter&page="
	fi

	# Notes; page=1 is page 2. Pages are base 0 indexed.
	check_eh "${FETCH}0"

	# How many pages of shit?
	PAGE_COUNT="$(fetch "${FETCH}0" "-" | grep "Jump to page" | sed 's|.*Jump to page: (1-||g' | sed 's|).*||g' | head -n1)"

	if [ "$PAGE_COUNT" = "" ]; then
		PAGE_COUNT=1
	fi

	echo "[E-h] Pages: $PAGE_COUNT"

	TEMP="$(temp f)"

	for (( i=0 ; i < $PAGE_COUNT ; i++ )); do
		FETCH_RESULT=1
		while [ ! $FETCH_RESULT = 0 ]; do
			fetch "${FETCH}${i}" "-" >> "$TEMP"

			echo "[E-H] Grabbing page $((i + 1))..."
		done
	done

	cat "$TEMP" | sed "s|<|\n|g" | grep "a href=\"http://\(g.\)\?e.hentai.org/g/" | sed -e 's|a href="||g' -e 's|".*||g' | sort | uniq >> batch.txt
	rm "$TEMP"
}
#############################################
#####@ORIGINAL-FILE 'modules/fakku'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Preface - code does not discriminate. Plus, if you read manga, let's be honest;
# you've made the mistake of ending up here accidentally once.

fakku_longname="FAKKU"
fakku_url="fakku.net/"
fakku_state=1
fakku_filt=0
fakku_note="They've been shuffling things around recently."

auto_fakku() {
	if [ -n "$(echo $1 | grep 'fakku.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
		# Fakku
		return 1
	fi

	return 0
}

dl_fakku() {
	# They don't play well with content disposition. Actually; they reply 404.
	SAFETY_HACKS=1

	PAGE="$(fetch "$1/read" "-")"

	# First. Escape fixups. Nuke escaped forward slashes
	# Next. Reformat decl.
	# Next. Nuke '.thumb'
	# Next. Nuke commas. Still, they can occur in names so we'll carefully filter.
	# Last transform. '/thumbs/' to '/images/'
	PAGEDATA="$(echo "$PAGE" | grep "window.params.thumbs =" | sed -e 's/\\\//\//g' -e 's/\];/)/g' -e 's/window.params.thumbs = \[/pages=(/g' -e 's/\.thumb//g' -e 's/","/" "/g' -e 's/\/thumbs\//\/images\//g')"

	# Because some pages have unicode escapes (javascript thing ugh) we have to echo the contents of the file
	# to itself to escape them. UGHHHHH

	DATA="$(printf "$PAGEDATA")"

	folder="$(echo -n "$PAGE" | grep '<title>' | sed -e 's/[[:space:]]*<title>Read //g' -e 's|</title>||g' | entity_to_char | remove_illegal)"

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[Fakku] Already downloaded. Skipping."
		return 0
	fi

	mkdir -p "$folder"
	cd "$folder"

	# Load in the array.
	eval "$DATA"

	# Download loop-de-loop.

	echo -n "[Fakku] Downloading '$folder' "

	CUR=0
	for image in "${pages[@]}"; do
		fetch "https:$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done

	cd ..

	cbz_make "$folder"

	SAFETY_HACKS=0
}

scrape_fakku() {
	echo -n "[Fakku] Scraping Chapters..."

	TEMP="$(temp f)"
	TEMPR="$(temp f)"

	fetch "$1" "$TEMP"

	grep 'class="content-title"' "$TEMP" > "$TEMPR"

	sed -i 's|^.*href="||g' "$TEMPR"
	sed -i 's|" title=.*||g' "$TEMPR"
	sed -i "s/^[[:space:]]*//" "$TEMPR"
	sed -i "s/[[:space:]]*$//" "$TEMPR"

	# Links are local.
	sed -i "s|^|https://fakku.net|g" "$TEMPR"

	# Don't bother doing any re-ordering here, since orders are chaotic.
	cat "$TEMPR" >> batch.txt

	# Clean up.
	rm "$TEMPR" "$TEMP"

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Fakku] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/dynsc'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

dynsc_longname="Dynasty Scans"
dynsc_url="dynasty-scans.com/"
dynsc_state=1
# No filter
dynsc_filt=0

auto_dynsc() {
	if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Dynasty Scans.
		return 1
	fi

	return 0
}

dl_dynsc() {

	# Now loop-de-loop. First, make a decent name. Dynasty always has
	# a short-title at the end of the URL.

	PAGEDATA="$(fetch "$1" "-")"

	folder="$(echo "$PAGEDATA" | grep "<title>" | sed -e 's/<title>Dynasty Reader &raquo; //g' -e 's|</title>||g' | entity_to_char | remove_illegal)"

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[DynastyScans] Already downloaded. Skipping."
		return 0
	fi

	mkdir -p "$folder"
	cd "$folder"

	PAGELIST="$(echo "$PAGEDATA" | grep "var pages")"

	# This set of seds cuts up the pagelist in a manner
	# that makes it identical to a bash array.
	# So we're essentially modifying the webpage into a dl-script.
	# Cool, eh?

	PAGETMP="$(echo "$PAGELIST" | sed -e "s/\"image\"\://g" -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" -e "s/\}\]/\)/g" -e "s/{//g" -e "s/}//g" -e "s/;//g" -e "s/ //g" -e "s/varpages=\[/pages=\(/g" -e "s/,/ /g")"

	# One possible nasty. Spaces.
	# sed -i "s/\%20/ /g" tmp.1

	# Load in the array.
	eval "$PAGETMP"

	echo -n "[DynastyScans] Downloading '$folder' "

	CUR=0

	for image in "${pages[@]}"; do
		fetch "http://dynasty-scans.com$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_dynsc() {
	echo -n "[DynastyScans] Scraping Chapters..."

	# URLS are local.
	fetch "$1" "-" | 			\
	grep 'class="name"' | 		\
	sed -e 's|^.*href="||g' 	\
		-e 's|" class=.*||g' 	\
		-e "s/^[[:space:]]*//" 	\
		-e "s/[[:space:]]*$//" 	\
		-e "s|^|http://dynasty-scans.com|g" >> batch.txt

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/foolsl'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Unlike all of the other plugins, this is a generic downloader for foolslide based sites.
# Therefore, instead of keeping it a generic and needing to specifically ask for this plugin,
# I use a compatibility list.

# Sites using foolslide:
#   vortex-scans.com
#   foolrulez.org

foolsl_longname="FoolSlide"
foolsl_url="Generic"
foolsl_state=1
foolsl_filt=0

auto_foolsl() {
	if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# FoolRulez
		return 1
	elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# VortexScans
		return 1
	fi

	return 0
}

dl_foolsl() {
	# Attempt a lazy download first. Only image scrape if rejected.

	LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`

	echo -n "[FoolSlide] Attempting Lazy Download..."

	FAILED=0

	fetch "$LAZYURL" || export FAILED=1

	if [ $FAILED = 1 ]; then
		echo "Requesting zip failed."
	else
		echo "[OK]"
	fi
}

scrape_foolsl() {

	echo -n "[Foolslide] Scraping Chapters..."

	fetch "$1" "-" |							\
	grep '<div class="title"><a href='			\
	sed -e 's|<div class="title"><a href="||g'	\
	-e 's|" title=.*||g'						\
	-e "s/^[[:space:]]*//"						\
	-e "s/[[:space:]]*$//"					  | \
	reverse_lines >> batch.txt
	
	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Foolslide] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/mpark'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

mpark_longname="MangaPark"
mpark_url="mangapark.me"
# Broken
mpark_state=1
# No filter
mpark_filt=0

auto_mpark() {
	if [ -n "`echo $1 | grep 'mangapark.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Mangapark
		return 1
	fi

	return 0
}

dl_mpark() {

	sitepage="$1"

	# We need a specific type of URL - the 'all on one page' type. Remove specifiers.
	sitepage=`echo $sitepage | sed "s|/1$||g" | sed "s|/3-1$||g" | sed "s|/6-1$||g" | sed "s|/10-1$||g"`

	FETCH="$(fetch "$sitepage" "-")"

	folder="$(echo "$FETCH" | grep '<title>' | sed -e 's/<title>//g' -e 's/ Online For Free.*$//g' -e 's/.* - Read //g' | entity_to_char | remove_illegal)"

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[MangaPark] Already downloaded. Skipping."
		return 0
	fi

	mkdir -p "$folder"
	cd "$folder"

	declare -a DATA
	DATA=$(echo "$FETCH" | grep 'target="_blank"' - | sed -e '1d' -e 's|^[[:space:]]*<a.*target="_blank" href=||g' -e "s/ title=.*$//" -e "s/\"//g"| tr '\n' ' ')

	echo -n "[Mangapark] Downloading '$folder' "

	CUR=0
	for image in ${DATA[@]}; do
		fetch "$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_mpark() {
	echo -e "[Mangapark] Scraping Chapters..."

	TEMP="$(temp f)"
	RTEMP="$(temp f)"

	fetch "$1" "$TEMP"

	grep 'class="ch sts"' "$TEMP" > "$RTEMP"

	sed -i 's|^.*href="||g' "$RTEMP"
	sed -i 's|">.*||g' "$RTEMP"
	sed -i "s/^[[:space:]]*//" "$RTEMP"
	sed -i "s/[[:space:]]*$//" "$RTEMP"

	# URLS are local.
	sed -i "s|^|http://mangapark.com|g" "$RTEMP"

	# We need a specific type of URL - the 'all on one page' type. Remove specifiers.
	sed -i "s|/1$||g" "$RTEMP"
	sed -i "s|/3-1$||g" "$RTEMP"
	sed -i "s|/6-1$||g" "$RTEMP"
	sed -i "s|/10-1$||g" "$RTEMP"

	# Lines are reverse order.
	cat "$RTEMP" | reverse_lines >> batch.txt

	# We've scraped a batch file from the URL list. Clean up.
	rm "$RTEMP" "$TEMP"

	echo -e "[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/imgur'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

imgur_longname="Imgur Gallery"
imgur_url="imgur.com/"
imgur_state=1
# No filter
imgur_filt=0

auto_imgur() {
	if [ -n "`echo $1 | grep 'imgur.com/a/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Imgur.
		return 1
	fi

	return 0
}

dl_imgur() {

	# Now loop-de-loop. First, make a decent name. Dynasty always has
	# a short-title at the end of the URL.

	PAGEDATA="$(fetch "$1" "-")"

	folder="$(echo "$PAGEDATA" | grep "</title>" | sed -e 's|<title>||g' -e 's/^[[:space:]]*//g' -e 's|</title>||g' | sed 's| - Album on Imgur||g' | entity_to_char | remove_illegal)"

	is_done "$folder"
	R=$?
	if [ $R = 1 ]; then
		echo "[Imgur] Already downloaded. Skipping."
		return 0
	fi

	mkdir -p "$folder"
	cd "$folder"

	declare -a PAGELIST
	PAGELIST=($(echo "$PAGEDATA" | grep '<meta property="og:image"' | sed -e 's|^.*content="||g' -e 's|".*$||g' -e '/?fb/d'))

	echo -n "[Imgur] Downloading '$folder' "

	CUR=0

	for image in "${PAGELIST[@]}"; do
		EXT=$(echo "$image" | sed 's|.*\.||g')
		fetch "$image" "${CUR}.${EXT}"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_imgur() {
	echo -n ""
}



#############################################
#####@ORIGINAL-FILE 'modules/booru'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# This is another generic; also, there's a bit of extra here. You can provide either a URL
# or a known Booru's name, and use the filter for a tag.

# e.g. scangrab booru danbooru touhou
# will fetch everything from the touhou tag. (which is expensive. How many damned pages are there?)

# Past that, you can also specify how many pages, like so:
#   scangrab booru danbooru touhou:50
#   (downloads 50 pages)
# Or how many images you want:
#   scangrab booru danbooru touhou::100
#   (downloads 100 images)
# You can also specify ranges if you want to get specific; like so:
#   scangrab booru danbooru touhou:20-30:10-150
#   (starts at page twenty, downloads till page 30. Skips 10 images, and downloads 140.
# Because of this, it's recommended to avoid passing URLs. They get cut up for parameters, anyways.

# Danbooru also functions differently; it is the only grabber which DOES NOT zip up things.
# By default, things are stored in folders by the tag you grab by, and named according to the image's
# MD5 (as danbooru does.) A file is stored with it that has meta-info from danbooru.

booru_longname="*Booru"
booru_url="Generic"
booru_state=2
booru_filt=1

auto_booru() {
	if [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
		# Danbooru / Safebooru
		return 1
	elif [ "$1" = "danbooru" ]; then
		# Danbooru direct.
		return 1
	elif [ "$1" = "testbooru" ]; then
		# Danbooru direct.
		return 1
	elif [ "$1" = "safebooru" ]; then
		# Safebooru direct.
		return 1
	fi

	return 0
}

# Because the syntax is hell.
booru_help() {
	echo "Usage: NAME TAGLIST SPEC [FOLDERNAME]"
	echo ""
	echo "TAGLIST:"
	echo "    tagname                   - Specify a tag"
	echo "    tagname1,tagname2,...     - Specify multiple tags"
	echo "    pool:poolname             - Download from a pool"
	echo ""
	echo "  Specifiers:"
	echo "      art:tagname               - Search by artist"
	echo "      char:tagname              - Search by character"
	echo "      copy:tagname              - Search by copyright"
	echo "      user:username             - Anything uploaded by username"
	echo "      fav:username              - Anything favorited by username"
	echo "      ordfav:username           - Anything favorited by username (chrono)"
	echo "      md5:hash                  - Posts with a specific MD5 hash."
	echo "      rating:rate               - Safety rating, e.g. s, q, e"
	echo "      See http://safebooru.donmai.us/wiki_pages/43049 for a more"
	echo "      exhaustive reference."
	echo ""
	echo "  Modifers:"
	echo "      tag1,-tag2        - Anything matching tag1, but not tag2"
	echo "      ~tag1,~tag2       - Anything marked tag1 or tag2"
	echo "      tag1,tag2         - Anything marked with both tag1 AND tag2"
	echo "      *tag*             - All tags containing 'tag'"
	echo ""
	echo "SPEC:"
	echo "    p10                - Download page 10."
	echo "    l50,p10            - 50 images per page, download page 10. (This is a"
	echo "                         lowercase L if your font doesn't distinguish)"
	echo "    i100               - Download 100 images"
	echo "    i5-100             - Download images 5-100"
	echo "    p10-50             - Download from page 10 to 50 inclusive"
	echo "    l50,p10,i100       - Pages with 50 images, save the first 100 images,"
	echo "                         starting at page 10"
	echo "    l50,p5-10,i100     - Pages with 50 images, save the first 100 images,"
	echo "                         starting at page 5"
	echo "                         (This syntax is legal, but -10 is ignored.)"
	echo "    pages:50           - Download 50 pages, Long syntax"
	echo "    images:100         - Download 100 images, Long syntax"
	echo "    limit:50           - 50 images on a page, Long syntax"
	echo "    resume             - Resume a previous Ctrl+C'd download,"
	echo "    r                  - Short for resume"
	echo ""
	echo "  Minor note is that *technically* limit:N/lN is a tag."
	echo ""
	echo "FOLDERNAME (optional):"
	echo "    Output folder. If not specified, it uses the format:"
	echo "    'NAME - TAGLIST'"
	echo ""
	echo "Also, please note that the following sites are either alternate"
	echo "source bases or NOT boorus, but will eventually be supported"
	echo "here due to some structural similiarities:"
	echo ""
	echo "    Moebooru-based (https://github.com/moebooru/moebooru)"
	echo "        yande.re (yandere)"
	echo "        konachan.com (konachan_g) / konachan.net (konachan)"
	echo ""
	echo "    Shimmie-based (https://github.com/shish/shimmie2)"
	echo "        shimmie.katawa-shoujo.com (mishimme)"
	echo ""
	echo "    Custom / Needs research"
	echo "        zerochan.net (zerochan)"
	echo ""
	echo "    All the same, dunno what they run"
	echo "        *.booru.net (Many different things)"
	echo "        safebooru.net (Not the same as safebooru)"
	echo "        gelbooru.net"
	exit 1
}

# Range to download.
declare -a pagerange
declare -a imagerange
declare -a tagarray
declare -a sizearray
declare -a final
TAGCOUNT=0
page_index=0

# base URL
# Tags
# Images per page. Blank if no.
# page=
# Page Number
final=("$source/posts?tags=" "" "" "&page=" 1)

RESUME=0

pagerange=(1 1)

# Paged mode by default.
# 1=paged, 2=images
mode=1
source=""
name=""

is_pool=0

# Computes final tags.
booru_tag_crunch() {
	# Tag array can be pasted together.
	for (( i=0 ; i < ${#tagarray[@]} ; i++ )); do
		if [ ! "$i" = "0" ] && [ ! "$((i + 1))" = "${#tagarray}" ]; then
			final[1]="${final[1]}+"
		fi

		# This is a pool. Ignore the other things.
		if [[ "${tagarray[i]}" == pool:* ]]; then
			echo "[Booru] This is apparently a pool."
			is_pool=1
			TAGCOUNT=1
			final[1]="$(echo ${tagarray[i]} | sed -e 's|pool:||g')"
			break
		fi

		tag="$(echo ${tagarray[i]} | sed -e 's|:|%3A|g')"

		final[1]="${final[1]}${tag}"
		if [[ ! "$tag" == *:* ]]; then
			TAGCOUNT=$((TAGCOUNT + 1))
		fi
	done
}

booru_size_crunch() {
	# No settings for a pool.
	if [ $is_pool = 1 ]; then
		return
	fi

	for (( i=0 ; i < ${#sizearray[@]} ; i++ )); do
		if [[ "${sizearray[i]}" == pages:* ]] || [[ "${sizearray[i]}" == p* ]]; then
			pagerange=($(echo "${sizearray[i]}" | tr -d 'p' | tr '-' ' '))
			if [ "${pagerange[1]}" = "" ]; then
				pagerange[1]=${pagerange[0]}
			fi
			final[4]=${pagerange[0]}
		elif [[ "${sizearray[i]}" == limit:* ]] || [[ "${sizearray[i]}" == l* ]]; then
			if [[ ! "${sizearray[i]}" == "limit:*" ]]; then
				final[2]="+$(echo ${sizearray[i]} | sed 's|l|limit:|')"
			else
				final[2]="+${sizearray[i]}"
			fi
			final[2]="$(echo ${final[2]} | sed -e 's|:|%3A|g')"
		elif [[ "${sizearray[i]}" == images:* ]] || [[ "${sizearray[i]}" == i* ]]; then
			imagerange=($(echo "${sizearray[i]}" | tr -d 'i' | tr '-' ' '))
			if [ "${imagerange[1]}" = "" ]; then
				imagerange[1]=${imagerange[0]}
				imagerange[0]=1
			fi
			mode=2
		elif [[ "${sizearray[i]}" == "all" ]]; then
			# 1st page.
			pagerange[0]=1
			# Arbitrarily high number that will never be reached. Maybe. Dunno.
			# Point is, we stop when no images can be extracted from the page.
			pagerange[1]=$MAX_INT

			final[4]=${pagerange[0]}
		elif [[ "${sizearray[i]}" == "resume" ]] || [[ "${sizearray[i]}" == "r" ]]; then
			# Resume.
			RESUME=1
		fi
	done
}

dl_booru_page() {
	url="$1"
	name_out="$2"

	FETCH_RESULT=1
	while [ ! $FETCH_RESULT = 0 ]; do
		fetch "$url" "$name_out"

		if [ ! $FETCH_RESULT = 0 ]; then
			FAILS=$((FAILS + 1))
			message=" :<"
			if [ ! $FAILS = 0 ]; then
				spinner_done "!!!"

				echo -en "[Booru] Server refused us. Waiting a bit... "
				SLEEP=$((60 * FAILS))
				while [ ! $SLEEP = 0 ]; do
					sleep 1s
					spinner "${SLEEP}s"
					SLEEP=$((SLEEP - 1))
				done

				spinner_done "DONE"

				echo -en "[Booru] Continuing... "
			fi
		fi

		# This is for safety.
		# A lot of boorus get mad at you for hammering the database repeatedly and punish you
		# by denying connections for a while. This ensures we aren't hammering *too* hard.
		sleep 5s
	done
	FAILS=0
}

dl_booru() {
	if [ "$2" = "" ] && [ "$3" = "" ] || [ "$1" = "" ]; then
		booru_syntax_help
	fi

	if [ -n "$(echo $1 | grep 'safebooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "safebooru" ]; then
		# Safebooru directly. NEVER MOVE THIS CHECK BENEATH DANBOORU. THE GREP FOR DANBOORU WILL MATCH.
		source="http://safebooru.donmai.us"
		name="danbooru"
		site="safebooru"
	elif [ -n "$(echo $1 | grep 'testbooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "testbooru" ]; then
		# Testbooru (which has four images and is a good test.) NEVER MOVE THIS CHECK BENEATH DANBOORU. THE GREP FOR DANBOORU WILL MATCH.
		source="http://testbooru.donmai.us"
		name="danbooru"
		site="testbooru"
	elif [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "danbooru" ]; then
		# Danbooru
		source="http://danbooru.donmai.us"
		name="danbooru"
		site="danbooru"
	else
		# Unknown, but apparently a booru.
		source="$1"
		echo "[Booru] This type of Booru isn't specified as supported."
		echo "[Booru] It may work, depending on whether the autodetect picks up a valid scheme,"
		echo "[Booru] or it may completely bomb out. If it works, please submit a bug report"
		echo "[Booru] so I can add it to the supported list."
		name="unknown"
		site="$source"
	fi

	booru_${name}_init

	shift

	tagarray=($(echo "$1" | tr ',' ' '))
	sizearray=($(echo "$2" | tr ',' ' '))

	booru_tag_crunch
	booru_size_crunch

	echo "[Booru] Info:"
	echo "[Booru]   Fetching URL $(echo ${final[0]}${final[1]}${final[2]}${final[3]} | tr -d ' ')"
	echo -n "[Booru]   Will download "
	if [ $mode = 1 ]; then
		if [ "${pagerange[0]}" = "${pagerange[1]}" ]; then
			echo "page ${pagerange[0]}."
		else
			TO=${pagerange[1]}
			if [ $TO = $MAX_INT ]; then
				TO="Max"
			fi
			echo "pages ${pagerange[0]} to $TO."
		fi
	else
		# XXX - Why in god's name would a user do this? Dunno, but handle their idiocy anyways.
		if [ "${imagerange[0]}" = "${imagerange[1]}" ]; then
			echo -n "image ${imagerange[0]}"
		else
			echo -n "images ${imagerange[0]} to ${imagerange[1]}"
		fi
		echo ", starting from page ${pagerange[1]}."
	fi

	if [ $TAGCOUNT = 0 ] && [ ${pagerange[1]} == $MAX_INT ]; then
		# Holy fucking shit. The user is nuts. Downloading a whole booru? WTF?
		# Ask them to make damn well sure that they meant that.

		echo "[Booru] Holy fucking shit. You're telling me to download a whole booru? Are you insane?"
		echo "[Booru] Not to mention, I didn't even DOCUMENT this syntax on purpose."
		echo "[Booru] Type 'Yes, I am sane.' after the colon if you really meant to do this."
		echo -n "[Booru] You sane? : "
		read sanequery
		if [ "$sanequery" = "Yes, I am sane." ]; then
			echo "[Booru] Okay, nutjob, if you say so. Don't come complaining later."
		else
			echo "[Booru] Yeah, thought so. Whew~"
			exit 1
		fi
	fi

	if [ ! "${final[2]}" = "" ]; then
		echo "[Booru]   Requesting $(echo ${final[2]} | sed 's|+limit%3A||') images per page."
	fi
	if (( $TAGCOUNT >= 2 )); then
		echo "[Booru]   Warning, more than two normal tags. Some boorus don't like this."
	fi

	if [ ! "$3" = "" ]; then
		tagdir="$3"
	else
		tagdir="$site - $1"
	fi

	echo "[Booru]   Output to folder '$tagdir'."

	mkdir -p "$tagdir"
	cd "$tagdir"

	mkdir -p meta content

	# The first file is expected to be in the root tagdir.

	# Download page data and make a list of IDs.
	# Also, ideally, extract metadata.
	if [ $RESUME = 0 ]; then
		echo -n '' > meta.txt
		echo -n "[Booru] Fetching pages... "
		for (( range=${pagerange[0]} ; range <= ${pagerange[1]} ; range++ )); do
			spinner "${range} -> ${pagerange[0]}/${pagerange[1]}"
			booru_${name}_page "$range"
			R=$?

			lines=$(wc -l meta.txt | sed 's| .*||g')
			if [ $mode = 2 ] && (( $lines > ${imagerange[1]} )); then
				break
			fi

			if [ $R = 1 ]; then
				break
			fi
		done
	fi

	spinner_done

	lines=$(wc -l meta.txt | sed 's| .*||g')

	DONE=0
	LINES=$(wc -l meta.txt | sed 's| .*||g')

	# All metadata files should be placed in tagdir/meta.

	cd meta

	echo -n "[Booru] Checking and refetching metadata... "

	# Ideally metadata should have been taken care of above.
	# Some sites don't provide all info in a page API query though.
	while read meta_id; do
		SKIP=0
		if [ $mode = 2 ]; then
			if (( $DONE < ${imagerange[0]} )); then
				SKIP=1
			fi

			if (( $DONE > ${imagerange[1]} )); then
				break
			fi
			TOTAL="$RANGE"
		elif [ $RESUME = 1 ]; then
			DONE=$((DONE - 1))
			if [ $DONE = 0 ]; then
				RESUME=0
				DONE=$PRE
			fi
		else
			TOTAL="$LINES"
			spinner "${DONE} -> ${LINES}"
			booru_${name}_meta "$meta_id"
			DONE=$((DONE + 1))
		fi
	done < ../meta.txt

	spinner_done

	echo -n "[Booru] Downloading content... "

	META_DIR="$(pwd)"

	# All images should be in tagdir/content
	cd ../content

	DONE=0

	# Download images.
	while read meta_id; do
		SKIP=0
		if [ $mode = 2 ]; then
			if (( $DONE < ${imagerange[0]} )); then
				SKIP=1
			fi

			if (( $DONE > ${imagerange[1]} )); then
				break
			fi
			TOTAL="$RANGE"
		elif [ $RESUME = 1 ]; then
			DONE=$((DONE - 1))
			if [ $DONE = 0 ]; then
				RESUME=0
				DONE=$PRE
			fi
		else
			TOTAL="$LINES"
			spinner "${DONE} -> ${LINES}"
			booru_${name}_content "$meta_id"
			DONE=$((DONE + 1))
		fi
	done < ../meta.txt

	rm ../meta.txt

	spinner_done

	echo "[Booru] Done with download. If you don't care about the metadata, it is no longer needed."
}

scrape_booru() {
	exit 1
}
#############################################
#####@ORIGINAL-FILE 'main'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Turn off history expansion. It only causes issues with exclamation points.
# TODO - this broke some shit. Fix it.
set +H

# Determine operation.

# New syntax; default is auto.
#  -b Batch download
#  -s Scrape list
#  -l Login
#  -U upgrade scangrab
#  -mModule Use module instead of detection.
#  -fformat Specify output type

CMD_STR="$1"
OPERATION=a
MODULE=a
FORMAT=cbz
TYPE=0
while [ "${CMD_STR:0:1}" = "-" ]; do
	if [ ${#CMD_STR} = 2 ]; then
		case ${CMD_STR:1:1} in
			b)
				OPERATION=b ;;
			s)
				OPERATION=s ;;
			l)
				OPERATION=l ;;
			U)
				upgrade_self
				exit 0
				;;
			u)
				usage
				exit 0
				;;
			h)
				help
				exit 0
				;;
			v)
				VERBOSE=1
				;;
			e)
				# This is used to import functions to a shell.
				# Not for general use.
				return 0 # cannot use exit - that would kill the shell
				;;
		esac
	elif [ "${CMD_STR:0:2}" = "-m" ]; then
		MODULE="${CMD_STR:2}"
		TYPE=1
	elif [ "${CMD_STR:0:2}" = "-t" ]; then
		SUBTYPES="${CMD_STR:2}" # Used as a filter in some modules.
	elif [ "${CMD_STR:0:2}" = "-f" ]; then
		FORMAT="${CMD_STR:2}"
	elif [ "${CMD_STR:0:2}" = "-c" ]; then
		COOKIEJAR="${CMD_STR:2}"
		if [ "${COOKIEJAR:0:1}" = "~" ]; then
			COOKIEJAR="${HOME}${COOKIEJAR:1}"
		fi
	fi
	shift
	CMD_STR="$1"
done

# Any operation except login is invalid without a URL
if [ "$1" = "" ] && [ ! "$OPERATION" = "l" ]; then
	echo "Invalid syntax. See usage (-u) or help (-h) for more info."
	exit 1
fi

case $OPERATION in
	a)
		if [ $TYPE = 0 ]; then
			auto "$@"
		else
			dl_$MODULE "$@"
		fi
		exit 0
		;;
	b)
		batch "$@"
		exit 0
		;;
	s)
		if [ $TYPE = 0 ]; then
			scrape "$@"
		else
			scrape_$MODULE "$@"
		fi
		exit 0
		;;
	l)
		if [ $TYPE = 0 ]; then
			echo "[scangrab] Please specify a module with the '-m' option."
			exit 1
		else
			mod_login "$MODULE"
		fi
		exit 0
		;;
esac
#############################################
