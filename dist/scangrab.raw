#!/bin/bash
#############################################
#####@ORIGINAL-FILE 'support'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# These values can be read but should not be written by a module.
COOKIEJAR="$(pwd)/cookiejar"
HAVE_IMAGICK=0
FETCH_RESULT=0

if [ "$VERBOSE" = "" ]; then
	VERBOSE=0
fi

# These values are used by support/core but are controlled by modules.
message=""
use_cookies=0
SAFETY_HACKS=0
CLOBBER=0

# Anything which starts with an underscore is internal and shouldn't be touched.
_COLOR=0
_FETCH_CMD=""
_CHECK_VALID=0
_FETCHTOOL=0
_BUSYBOX=0
_SPINNER_CHAR="|"
_NUM=0

# Tools. Use these variables instead of actual command names.
_tput="$(which tput 2>/dev/null)"
_identify="$(which identify 2>/dev/null)"
_convert="$(which convert 2>/dev/null)"
_wget="$(which wget 2>/dev/null)"
_curl="$(which curl 2>/dev/null)"
_aria="$(which aria2c 2>/dev/null)"

for ((i = 1, n = 2;; n = 1 << ++i)); do
	if [[ ${n:0:1} == '-' ]]; then
		MAX_INT=$(((1 << i) - 1))
		break
	fi
done

type="raw"

if [ "$TERM" = "xterm" ] || [ "$TERM" = "linux" ] || [ "$TERM" = "screen" ]; then
	_COLOR=1
	if [ -f $_tput ]; then
		# Better method.
		_COLOR=2
	fi
fi

if [ -f "$_identify" ]; then
	_CHECK_VALID=1
fi

if [ -f "$_identify" ]; then
	HAVE_IMAGICK=1
fi

if [ ! "$_wget" = "" ]; then
	common_opts=" --quiet --no-cache --user-agent=\"Mozilla/5.0\" -t 1 -T 10 --random-wait "

	if [ ! "$($_wget --help 2>&1 | grep busybox)" = "" ]; then
		echo "[Warning] Your system wget is busybox, which can't actually do some things like reject cache and retry."
		common_opts=" -q -U \"Mozilla/5.0\""
		_BUSYBOX=1
	fi

	_FETCH_CMD="$_wget $common_opts"
	_FETCHTOOL=1
else
	if [ ! "$_curl" = "" ]; then
			_FETCH_CMD=$_curl
			_FETCHTOOL=2
	else
		if [ ! "$_aria" = "" ]; then
				_FETCH_CMD=$_aria
				_FETCHTOOL=3
		fi
	fi
fi	

type() {
	if [ $_COLOR = 1 ]; then
		echo -ne "\x1b[$1m"
	elif [ $_COLOR = 2 ]; then
		if [ $1 = 0 ]; then
			tput sgr0
		elif [ $1 = 1 ]; then
			tput bold
		elif [ $1 = 2 ]; then
			tput dim
		fi
	fi
}

color() { 
	if [ $_COLOR = 1 ]; then
		echo -ne "\x1b[3$1m"
	elif [ $_COLOR = 2 ]; then
		tput setaf $1
	fi
}

cbz_make() {
	echo -e "[Post] Making CBZ..."

	# Check and make sure the folder has something, and is actually a folder.
	if [ ! -d "$1" ]; then
		echo -e "\n[Error] Not a folder. Something went wrong."
		exit 1
	fi
	
	if [ "$(ls "$1")" = "" ]; then
		echo "[Error] No files? Download failed."
		exit 1
	fi

	zip -r "$1.zip" "$1" > /dev/null 2>&1
	mv "$1.zip" "$1.cbz" > /dev/null 2>&1
	
	echo -e "[Post] Cleanup..."
	rm -rf "$1"
	echo -e "[Post] Finished!"
}

# Checks if an image is uncorrupted. If you don't have the tools, this never happens.
	# Returns 0 on OK / no tool
	# Returns error code on corrupt
verify() {
	if [ -f $_identify ]; then
		$_identify -verbose -regard-warnings "$1" 2>&1 >/dev/null
		_IDRES=$?
		return $_IDRES
	fi

	return 0
}

spinner_clear() {
	# Clear previous output
	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne "\b"
	done

	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne " "
	done

	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne "\b"
	done
}

spinner() {
	if [ $VERBOSE = 1 ]; then
		echo "Status: $1 - $message"
		return
	fi
	if [ "$_SPINNER_CHAR" = "|" ]; then
		_SPINNER_CHAR="/"
	elif [ "$_SPINNER_CHAR" = "/" ]; then
		_SPINNER_CHAR="-"
	elif [ "$_SPINNER_CHAR" = "-" ]; then
		_SPINNER_CHAR="\\"
	elif [ "$_SPINNER_CHAR" = "\\" ]; then
		_SPINNER_CHAR="|"
	fi

	# Clear previous output
	spinner_clear

	STR="[$1 $_SPINNER_CHAR] $message"
	_NUM="${#STR}"

	echo -ne "$STR"
}

spinner_reset() {
	if [ $VERBOSE = 1 ]; then
		echo "Status: $1 - $message"
		return
	fi

	spinner_clear

	_SPINNER_CHAR="|"
	_NUM=0
}

spinner_done() {
	spinner_reset

	_MESG="OK"
	if [ ! "$1" = "" ]; then
		_MESG="$1"
	fi
	echo -e "[$_MESG]"
}

mimetype() {
	echo "$(file --mime-type "$1" | sed 's/.* //g')"
}


# This creates a cookie jar.
# $1 - Username field
# $2 - Password field
# $3 - Username
# $4 - Password
# $5 - URL
# $6 - extra shit
s_login() {
	if [ $_FETCHTOOL = 1 ]; then
		# WGET
		_CMD="$_FETCH_CMD --post-data='$1=$3&$2=$4&$6' \"$5\"  --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies -O/dev/null"

	elif [ $_FETCHTOOL = 2 ]; then
		# CURL
		_CMD="$_FETCH_CMD -d '$1=$3&$2=$4&$6' $5 -b $COOKIEJAR -c $COOKIEJAR >/dev/null"

	elif [ $_FETCHTOOL = 3 ]; then
		#ARIA2C
		# How the fuck do I post? Maybe I can't with araia2c...meh.
		echo "[Warn] aria2c can't post at the moment; this will fail to get required cookies."
		_CMD="$_FETCH_CMD $5 --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR -o/dev/null"

	fi
	
	if [ $VERBOSE = 1 ]; then
		echo -e "\n$_CMD"
		return
	fi

	eval " $_CMD" 2>/dev/null
	FETCH_RESULT=$?
}

# AVOID CHANGING THIS FUNCTION IF AT ALL POSSIBLE.
# THINGS WILL BREAK IN EVERYTHING IF THIS ONE BREAKS.
fetch() {
	if [ $_FETCHTOOL = 1 ]; then
		if [ $SAFETY_HACKS = 1 ]; then
			_CMD="$_FETCH_CMD \"$1\""
		else
			_CMD="$_FETCH_CMD --content-disposition \"$1\""
		fi

		if [ ! $CLOBBER = 1 ]; then
			_CMD="$_FETCH_CMD -c \"$1\""
		fi

		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD --load-cookies=$COOKIEJAR"
		fi

		# Wget uses content-disposition to hopefully get a good name in abscence of one.
		# Of course, that assumes that SAFETY_HACKS is off.

		STDOUT=0
		if [ "$2" = "-" ]; then
			_CMD="$_CMD -O -"
			STDOUT=1
		elif [ ! "$2" = "" ]; then
			_CMD="$_CMD -O \"$2\""
		fi

	elif [ $_FETCHTOOL = 2 ]; then

		_CMD="$_FETCH_CMD $1"
		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD -b $COOKIEJAR -c $COOKIEJAR"
		fi

		if [ "$2" = "" ]; then
			_CMD="$_CMD > $(basename "$1")"
		elif [ "$2" = "-" ]; then
			_CMD="$_CMD"
		else
			_CMD="$_CMD > \"$2\""
		fi

	elif [ $_FETCHTOOL = 3 ]; then

		_CMD="$_FETCH_CMD $1"
		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD --load-cookies=$COOKIEJAR"
		fi

		if [ "$2" = "" ]; then
			_CMD="$_CMD -o $(basename "$1")"
		elif [ "$2" = "-" ]; then
			_CMD="$_CMD -o -"
		else
			_CMD="$_CMD -o \"$2\""
		fi
	fi
	
	if [ $VERBOSE = 1 ]; then
		echo -e "\n$_CMD"
	fi

	# Wget doesn't have an option to clobber forcibly
	# with content disposition on. I'm so bemused by this
	# that I'm not even sure what to say.
	if [ $CLOBBER = 1 ] && [ $_FETCHTOOL = 1 ] && [ ! $STDOUT = 1 ]; then
		mkdir -p "wget.tmp"
		cd wget.tmp
	fi

	eval " $_CMD" 2>/dev/null
	FETCH_RESULT=$?
	
	if [ $CLOBBER = 1 ] && [ $_FETCHTOOL = 1 ] && [ ! $STDOUT = 1 ]; then
		mv ./* ../ 2>/dev/null
		cd ..
		rm -rf wget.tmp
	fi

	# If this is an image, check validity.
	MIME="$(mimetype "$_FILE")"
	if [ "$MIME" = "image/jpeg" ] || [ "$MIME" = "image/png" ] || [ "$MIME" = "image/gif" ] || [ "$MIME" = "image/bmp" ]; then
		verify "$_FILE"
		VALID=$?
		if [ ! $VALID = 0 ]; then
			echo "[WARN] File '$_FILE' is corrupted."
		fi
	fi

	return $FETCH_RESULT
}

entity_to_char() {
	# This probably doesn't handle every case. It should be enough.
	# It also handles the case of illegal characters on windows/FAT/NTFS.
	# And the case of a slash in the name which is the only illegal one on linux.

	sed \
		-e "s/&#32;/ /g" \
		-e "s/&nbsp;/ /g" \
		-e "s/&#33;/\!/g" \
		-e "s/&#34;/\"/g" \
		-e "s/&#35;/\#/g" \
		\
		-e "s/&#36;/\$/g" \
		-e "s/&#37;/\%/g" \
		-e "s/&amp;/\&/g" \
		-e "s/&#38;/\&/g" \
		-e "s/&#39;/'/g" \
		\
		-e "s/&#40;/\(/g" \
		-e "s/&#41;/\)/g" \
		-e "s/&#42;/\*/g" \
		-e "s/&#43;/\+/g" \
		-e "s/&#44;/\,/g" \
		\
		-e "s/&#45;/\-/g" \
		-e "s/&#46;/\./g" \
		-e "s/&#58;/\:/g" \
		-e "s/&#59;/\;/g" \
		-e "s/&lt;/\</g" \
		\
		-e "s/&#60;/\</g" \
		-e "s/&gt/\>/g" \
		-e "s/&#61;/\>/g" \
		-e "s/&#63;/\?/g" \
		-e "s/&#64;/\@/g" \
		\
		-e "s/&#91;/\[/g" \
		-e "s/&#92;/\\\\/g" \
		-e "s/&#93;/\]/g" \
		-e "s/&#94;/\^/g" \
		-e "s/&#95;/\_/g" \
		\
		-e "s/&#123;/\{/g" \
		-e "s/&#124;/\|/g" \
		-e "s/&#125;/\}/g" \
		-e "s/&#126;/\~/g" \
		-e "s/&yen;/¥/g" \
		\
		-e "s/&#165;/¥/g" \
		-e "s/&sup2;/²/g" \
		-e "s/&#178;/²/g" \
		-e "s/&sup3;/³/g" \
		-e "s/&#179;/³/g" \
		\
		-e "s/&frac14;/¼/g" \
		-e "s/&#188;/¼/g" \
		-e "s/&frac12;/½/g" \
		-e "s/&#189;/½/g" \
		-e "s/&frac34;/¾/g" \
		\
		-e "s/&#190;/¾/g" \
		-e "s/&spades;/♠/g" \
		-e "s/&#9824;/♠/g" \
		-e "s/&clubs;/♣/g" \
		-e "s/&#9827;/♣/g" \
		\
		-e "s/&hearts;/♥/g" \
		-e "s/&#9829;/♥/g" \
		-e "s/&diams;/♦/g" \
		-e "s/&#9830;/♦/g"

}

# Known fixed snafus.
# Remove pipe, slash.
# Replace question mark with fullwidth question mark.
remove_illegal() {

	sed \
		-e "s/|/-/g" \
		-e "s|/|-|g" \
		-e "s|?|？|g"

}

reverse_lines() {
    readarray -t LINES
    for (( I = ${#LINES[@]}; I; )); do
        echo "${LINES[--I]}"
    done
}

auto() {
	for module in ${MODS[@]}; do
		auto_${module} "$@"
		RETCHECK=$?
		if [ $RETCHECK = 1 ]; then
			dl_${module} "$@"
			return 0
		fi
	done
	return 1
}

batch() {
	# $2 is a file. Read it in line by line as $1 and $2.
	while read chunk; do
		$0 $chunk
	done < $1
}

autobatch() {
	# $2 is a file. Read it in line by line as $1 and $2.
	while read chunk; do
		$0 auto $chunk
	done < $1
}

scrape() {
	for module in ${MODS[@]}; do
		auto_${module} "$@"
		RETCHECK=$?
		if [ $RETCHECK = 1 ]; then
			scrape_${module} "$@"
		fi
	done
}

mod_login() {
	for module in ${MODS[@]}; do
		if [ "$1" = "$module" ]; then
			if [ "$( eval echo \$${module}_uselogin )" = "1" ]; then
				if [ ! "$3" = "" ]; then
					echo "[Warn] Caveat; you probably need to wipe your shell history now. Try not to do it like this."
					username="$2"
					password="$3"
				elif [ ! "$2" = "" ]; then
					username="$2"
					echo -ne "[$module] Password for $username (will not echo): "
					read -s password
				else
					echo -ne "[$module] Username: "
					read username
					echo -ne "[$module] Password for $username (will not echo): "
					read -s password
				fi
				login_${module} "$username" "$password"
				exit 0
			else
				echo "$module does not need login."
				exit 0
			fi
		fi
	done
	echo "No such module."
	exit 1
}

upgrade_self() {
	if [ "$type" = "repo" ]; then
		echo "[Upgrade] You're in a git repo. Use 'git pull' instead."
		exit 0
	fi
	URL="https://raw.githubusercontent.com/chaoskagami/scangrab/$branch/dist/scangrab.$type"
	echo "[Upgrade] Checking this scangrab's sha256sum..."
	this_sha256="$(cat "$0" | sha256sum - | sed "s| .*||g")"
	gith_sha256="$(fetch "$URL.sha256sum" "-" | sed "s| .*||g")"
	if [ "$this_sha256" = "$gith_sha256" ]; then
		echo "[Upgrade] Not required. Same sha256 as upstream."
		exit 0
	else
		echo "[Upgrade] Local:  $this_sha256"
		echo "[Upgrade] Github: $gith_sha256"
		echo "[Upgrade] This doesn't match upstream. Upgrading..."
		fetch "$URL" "$0"
		echo "[Upgrade] We appear to have replaced ourselves. Checking sha256..."
		this_sha256="$(cat "$0" | sha256sum - | sed "s| .*||g")"
		if [ "$this_sha256" = "$gith_sha256" ]; then
			echo "[Upgrade] Succeded."
		else
			echo "[Upgrade] Failed. sha256 does not match github."
			echo "[Upgrade] Make sure you have write permission."
		fi
	fi
}

help() {
	type 1
	echo -e "Usage:"
	type 0
	echo -ne "     $0     "
	color 3
	echo -ne "OPERATION     "
	color 5
	echo -e "[PARAMS]"
	type 0
	type 1
	echo -e "Operations:"
	type 0
	color 3
	echo -e "     auto (a)"
	type 0
	echo -e "          Chooses module based on URL"
	color 3
	echo -e "     batch (l)"
	type 0
	echo -e "          Takes a file with a list of types and URLs"
	color 3
	echo -e "     autobatch (b)"
	type 0
	echo -e "          Takes a file with URLs which will be run with auto."
	color 3
	echo -e "     scrape (s)"
	type 0
	echo -e "          Will take a manga's page and scrape chapters to"
	echo -e "          a file named batch.txt"
	color 3
	echo -e "     login (u)"
	type 0
	echo -e "          Pass the module name, your username and password."
	echo -e "          This will generate a cookie jar as ./cookiejar"
	color 3
	echo -e "     upgrade"
	type 0
	echo -e "          Checks github for a newer copy of scangrab and"
	echo -e "          updates itself. Make sure you have write permission."
	echo -e "          There is no short form of this action."
	echo ""
	echo -e "     You can also specify a module name followed by"
	echo -e "     the URL instead of using the auto-detect."
	echo ""
	echo -e "     If you don't specify an operation and pass only a URL"
	echo -e "     then we assume you want auto (a)."
	type 1
	echo -e "Download Modules:"
	type 0
	for mod in "${MODS[@]}"; do
		longname=$(temp=\$${mod}_longname && eval echo $temp)
		url=$(temp=\$${mod}_url && eval echo $temp)
		broke=$(temp=\$${mod}_state && eval echo $temp)
		filter=$(temp=\$${mod}_filt && eval echo $temp)
		note=$(temp=\$${mod}_note && eval echo $temp)

		echo -ne "     Module Name:                "
		color 3
		echo -e "$mod"

		type 0
		echo -ne "          Long Name:             "
		color 4
		echo -e "$longname"

		type 0
		echo -ne "          Site(s) Used with:     "
		color 5
		echo -e "$url"
		type 0

		type 0
		echo -ne "          Site(s) Current state: "
		if [ "$broke" = "1" ]; then
			color 2
			echo -e "Works"
		elif [ "$broke" = "2" ]; then
			color 3
			echo -e "InDev"
		else
			color 1
			echo -e "Broken"
		fi

		if [ ! "$note" = "" ]; then
			type 0
			echo -e "          Site Note:             $note"
		fi

		type 0
		if [ "$filter" = "1" ]; then
			echo -e "          Supports filters for scrape"
		fi

		echo ""
	done
	type 1
	echo -e "Misc Info"
	type 0
	echo -e "     If you see an emote in the output, it means we had to deal"
	echo -e "     with a retrieval quirk."
	echo -e "\n     [ :/ ]       Given GZip'd data even though we said it wasn't"
	echo -e "                  supported in the GET."
	type 2
	echo -e "                  This happens frequently with batoto when doing"
	echo -e "                  multiple fetches. :/"
	type 0
	echo -e "\n     [ :( ]       Site didn't respond and so the DL failed"
	echo -e "                  We had to revert to a fallback method."

	echo -e "\n     [ >:( ]      Too many normal requests failed; we reverted"
	echo -e "                  to using entirely the fallback method."
	echo ""
	echo -e "     Some modules accept an extra parameter. Usually, this"
	echo -e "     is a filter. Try values like 'English' or 'French'."
	type 1
	echo -e "System Tools"
	type 0
	if [ ! "$_wget" = "" ]; then
		echo -e "     wget:                $_wget"
	fi
	if [ ! "$_curl" = "" ]; then
		echo -e "     curl:                $_curl"
	fi
	if [ ! "$_aria" = "" ]; then
		echo -e "     aria2c:              $_aria"
	fi
	echo -ne "     Will use:            "
	if [ $_FETCHTOOL = 1 ]; then
		echo -ne "wget"
		if [ $_BUSYBOX = 1 ]; then
			echo -ne ", busybox"
		else
			echo -ne ""
		fi
	elif [ $_FETCHTOOL = 2 ]; then
		echo -ne "curl"
	elif [ $_FETCHTOOL = 3 ]; then
		echo -ne "aria2c"
	else
		echo -ne "no tool. Install one of: "
	fi
	echo " (wget, curl, aria2c)"
	echo -ne "     Check broken images: "
	if [ $_CHECK_VALID = 1 ]; then
		if [ -f $_identify ]; then
			echo -ne "imagemagick ($_identify)"
		fi
		echo ""
	else
		echo "no"
	fi
	echo -ne "     Color:               "
	if [ $_COLOR = 1 ]; then
		color 1
		echo -ne "y"
		color 2
		echo -ne "e"
		color 3
		echo -ne "s"
		color 4
		echo -ne " "
		echo -ne "("
		color 5
		echo -ne "d"
		color 6
		echo -ne "u"
		color 1
		echo -ne "m"
		color 2
		echo -ne "b"
		color 3
		echo ")"
		type 0
	elif [ $_COLOR = 2 ]; then
		color 1
		echo -ne "y"
		color 2
		echo -ne "e"
		color 3
		echo -ne "s"
		color 4
		echo -ne " "
		echo -ne "("
		color 5
		echo -ne "t"
		color 6
		echo -ne "p"
		color 1
		echo -ne "u"
		color 2
		echo -ne "t"
		color 3
		echo ")"
		type 0
	else
		echo "no (TERM='$TERM')"
	fi
	type 1
	echo -ne "License / Info"
	type 0
	echo ""
	echo -e "     Copyright (C) 2015     Jon Feldman/@chaoskagami"
	echo ""
	echo -e "     This program is free software: you can redistribute it and/or modify"
	echo -e "     it under the terms of the GNU General Public License as published by"
	echo -e "     the Free Software Foundation, either version 3 of the License, or"
	echo -e "     (at your option) any later version."
	echo ""
	echo -e "     This program is distributed in the hope that it will be useful,"
	echo -e "     but WITHOUT ANY WARRANTY; without even the implied warranty of"
	echo -e "     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the"
	echo -e "     GNU General Public License for more details."
	echo ""
	echo -e "     You should have received a copy of the GNU General Public License"
	echo -e "     along with this program.  If not, see <http://www.gnu.org/licenses/>"
	echo ""
	echo -e "     The latest version of scangrab can always be found at the github"
	echo -e "     page here: https://github.com/chaoskagami/scangrab"
	echo ""
	echo -e "     Build: $type, $branch, $rev"
}
#############################################
#####@ORIGINAL-FILE 'rev'
rev=4b21edbec3ca564dc46948dd06705777c7f3634d
branch=master
#############################################
#####@AUTOGENERATED 'MODS'
MODS=(mangabox batoto niconicoseiga eh fakku dynsc foolsl mpark booru)
#############################################
#############################################
#####@ORIGINAL-FILE 'modules/mangabox'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# I need to add an option later for page splitting with imagemagick.
# The whole two-in-one thing is annoying.

mangabox_longname="Mangabox"
mangabox_url="www.mangabox.me"
mangabox_state=1
mangabox_filt=0
mangabox_note="Automatically splits pages with IM. The PC webpage is broken badly. Don't report it."

auto_mangabox() {
	if [ -n "`echo $1 | grep 'mangabox.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# MangaBox
		return 1
	fi

	return 0
}

dl_mangabox() {
	
	echo "[MangaBox] Getting list of pages..."

	fetch "$1" "tmp.htm"

	fullname="$(cat tmp.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char)"

	declare -a list
	list=($(cat tmp.htm | grep 'class="jsNext"' | sed -e 's|.*<li><img src="||g' -e 's|?.*||g'))

	mkdir -p "$fullname"
	cd "$fullname"

	echo -n "[MangaBox] Downloading '$fullname'... "

	NUM=0
	for page in ${list[@]}; do
		NUM=$((NUM + 1))
		VALID=1
		while [ ! $VALID = 0 ]; do
			fetch $page
			verify $(basename $page)
			VALID=$?
		done

		if [ $HAVE_IMAGICK = 1 ]; then
			spinner "Download:$NUM"
			
			$_convert $(basename $page) -crop 2x1@ +repage +adjoin "${NUM}_%d.png"

			spinner "Reformat:$NUM"

			rm $(basename $page)

			spinner "Collat:$NUM"			

			mv ${NUM}_0.png ${NUM}_b.png
			mv ${NUM}_1.png ${NUM}_a.png
		else
			spinner "$NUM"
		fi
	done

	cd ..

	spinner_done

	rm tmp.htm

	cbz_make "$fullname"
}

scrape_mangabox() {

	echo "[MangaBox] Not yet supported, sorry."
	exit 1
}
#############################################
#####@ORIGINAL-FILE 'modules/batoto'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

batoto_longname="Batoto"
batoto_url="bato.to"
batoto_state=1
batoto_filt=1
batoto_note="Their page template is highly unstable."

auto_batoto() {
	if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Batoto
		return 1
	fi

	return 0
}

dl_batoto() {
	# Batoto requires a different strategy.
	# The URLs are not preloaded like the former, so the fetch one page done thing won't work.
	# Unfortunately, short of grabbing pages until an image 404's, there's no way of knowing when we're done.
	
	folder="$(fetch "$1" "-" | grep -C0 "<title>" | sed -e "s/^[[:space:]]*<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e "s/[[:space:]]*$/\n/" | entity_to_char)"

	mkdir -p "$folder"
	cd "$folder"

	CUR=0
	PAGES=0
	RET=0
	
	base="$1"
	if [ ! "${base:${#base}-1}" = "/" ]; then
		base="${base}/"
	fi

	echo -n "[Batoto] Downloading '$folder' "

	while [ "$RET" = "0" ]; do
		# Increment CUR.
		CUR=$(( CUR + 1 ))

		# On batoto, two slashes is a syntax error as of Jun 13, 2015.

		# We also need to fetch to a file here unfortunately, because possible stupidity.
		fetch "${base}${CUR}" "$CUR.htm"

		# Batoto sometimes gives out gunzips. We need to account for that... =_=

		if [ "$(mimetype $CUR.htm)" = "application/x-gzip" ]; then
			mv $CUR.htm $CUR.htm.gz
			gunzip $CUR.htm.gz
			message=" :/"
		fi

		img="$(grep -C0 'z-index: 1003' $CUR.htm | sed -e 's/^[[:space:]]*<img src="//g' -e 's/".*$//g')"

		ext="${img##*.}"

		# If this 404's, fetch will return non-zero. Thus, loop breaks.
		fetch "$img" "${CUR}_${folder}.${ext}"
		RET=$?

		rm $CUR.htm
	
		spinner "$CUR"
	done

	PAGES=$(( CUR - 1 ))

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_batoto() {
	notice_batoto

	echo -n "[Batoto] Scraping Chapters..."

	fetch "$1" scrape.htm
		
	# Batoto sometimes gives out gunzips. We need to account for that... =_=
	if [ "$(mimetype scrape.htm)" = "application/x-gzip" ]; then
		mv $CUR.htm $CUR.htm.gz
		gunzip $CUR.htm.gz
		message=" :/"
	fi

	grep -A 2 'Sort:' scrape.htm >> batch.txtr

	# Delete the useless lines.
	sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr

	# Remove Language lines.
	sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
	sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr

	# Edit up URL.
	sed -i "s|<a href=\"||g" batch.txtr
	sed -i "s|\" title=.*||g" batch.txtr

	# Delete blank lines/space lines
	sed -i '/^[[:space:]]*$/d' batch.txtr

	# Strip.
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	sed -i 's|^--$||g' batch.txtr

	# Delete Blank lines.
	sed -i '/^$/d' batch.txtr

	cat batch.txtr | reverse_lines > batch.txtf

	if [ "$2" = "" ]; then
		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf
		
		cat batch.txtf >> batch.txt
	else
		echo -ne "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Applying Language Filter '$2'..."

		grep -A 1 "$2" batch.txtf > batch.txtf2
		
		# Delete '--' lines
		sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2

		# Delete Blank lines.
		sed -i '/^$/d' batch.txtf2

		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf2

		cat batch.txtf2 >> batch.txt
	fi

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr batch.txtf*
	
	echo -en "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"

	for ((n=0;n < ${#2}; n++)); do
		echo -en '\b'
	done

	echo -e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/niconicoseiga'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# NiconicoSeiga is odd in that an account is required. Therefore, I had to implement cookiejars.

niconicoseiga_longname="Niconico Seiga"
niconicoseiga_url="seiga.nicovideo.jp"
niconicoseiga_state=1
niconicoseiga_filt=0
niconicoseiga_note="Please login first."

# Need to login.
niconicoseiga_uselogin=1

auto_niconicoseiga() {
	if [ -n "`echo $1 | grep 'seiga.nicovideo.jp' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Niconico Seiga
		return 1
	fi

	return 0
}

login_niconicoseiga() {
	use_cookies=1
	s_login "mail_tel" "password" "$1" "$2" "https://secure.nicovideo.jp/secure/login"
}

dl_niconicoseiga() {
	use_cookies=1

	name="$(basename $1)"

	fetch $1 "$name.htm"

	fullname="$(cat $name.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char | remove_illegal)"

	if [ ! -f "$name.htm" ]; then
		echo "[Niconico] No page found, aborting."
		return 1
	fi

	echo "[Niconico] Extracting image list..."

	cat $name.htm | grep 'data-image-id' | sed -e 's|^[[:space:]]*data-image-id="||g' -e 's|"[[:space:]]*$||g' > $name.lst
	
	if [ "$(cat $name.lst)" = "" ]; then
		echo "[Niconico] No images in page. Did you login?"
		return 1
	fi

	mkdir -p "$fullname"
	cd "$fullname"

	echo -n "[Niconico] Downloading '$fullname' (from $name) "

	COUNT=0
	while read ID; do
		COUNT=$((COUNT + 1))
		fetch "http://lohas.nicoseiga.jp/thumb/${ID}p" "${COUNT}.jpg"
		spinner $COUNT
	done < ../$name.lst

	spinner_done

	cd ..
	rm $name.lst $name.htm
	
	cbz_make "$fullname"
}

scrape_niconicoseiga() {
	COOKIES=1

	echo "[Niconico] Scraping chapters..."

	fetch "$1" "-" | grep 'class="episode"' | sed -e 's|.*<a href="||g' -e 's|?.*||g' -e 's|/watch|http://seiga.nicovideo.jp/watch|g' >> batch.txt

	echo "[Niconico] Done."
}
#############################################
#####@ORIGINAL-FILE 'modules/eh'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

eh_longname="E-H / ExH"
eh_url="g.e-hentai.org / exhentai.org"
eh_state=1
eh_filt=0
eh_note="Logging in will result in HQ images and less H@H peer-related issues =_=;"

eh_uselogin=1

login_eh() {
	use_cookies=1

	# Lots of extra shat they use to prevent bots. If it gets updated; let me know so I can fix it.
	
	s_login "UserName" "PassWord" "$1" "$2" "http://forums.e-hentai.org/index.php?act=Login&CODE=01" \
		"CookieDate=1&b=&bt=&referer=http://forums.e-hentai.org/?act=idx"
	user="$(fetch "http://forums.e-hentai.org/?act=idx" "-" | grep 'Logged in as' | sed -e 's|.*showuser=||' -e 's|<.*||' -e 's|.*>||g')"
	if [ ! "$user" = "" ]; then
		echo -e "\n[E-H] Logged in as: $user"
		exit 0
	fi

	exit 1
}

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# E-H
		return 1
	elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Ex
		return 1
	fi

	return 0
}

dl_eh() {
	LOGGEDIN=0
	use_cookies=0
	if [ -e "cookiejar" ]; then
		if [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
			echo -ne "[E-H] Checking for Ex cookies..."
			grep 'exhentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
			RES=$?
			if [ $RES = 0 ]; then
				echo "yes"
				use_cookies=1
				echo -ne "[E-H] Checking for access..."

				# This is a good way of checking if it succeeded.
				fetch "exhentai.org" "exchk"
				cat exchk | grep "The X Makes It Sound Cool" >/dev/null 2>&1
				RES=$?
				if [ ! $RES = 0 ]; then
					echo -n "no, "
					if [ "$(sha256sum exchk | sed 's| .*||g')" = "a279e4ccd74cffbf20baa41459a17916333c5dd55d23a518e7f10ae1c288644f" ]; then
						echo "panda."
					else
						echo "not cool."
					fi
					exit 1
				else
					LOGGEDIN=1
					echo "yes"
				fi
			else
				echo "no"
				echo -ne "[E-H] Checking for E-H cookies..."
				grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
				RES=$?
				if [ $RES = 0 ]; then
					echo "yes, copying them"
					grep 'e-hentai.org' $COOKIEJAR >> cookiejar.edit
					sed 's|e-hentai.org|exhentai.org|g' cookiejar.edit >> cookiejar
					rm cookiejar.edit

					echo -ne "[E-H] Fixed cookies. Checking for access..."
					use_cookies=1
					fetch "exhentai.org" "-" | grep "The X Makes It Sound Cool" >/dev/null 2>&1
					RES=$?
					if [ ! $RES = 0 ]; then
						echo "panda. Dying."
						exit 1
					else
						LOGGEDIN=1
						echo "yes"
					fi
				else
					echo "no."
					echo "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
				fi
			fi
		else
			grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
			RES=$?
			if [ $RES = 0 ]; then
				echo "[E-H] We seem to have cookies...using them."
				LOGGEDIN=1
				use_cookies=1
			fi
		fi
	elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		echo "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
		exit 1
	fi

	sitepage=$1

	echo "[E-H] Fetching index page..."

	# This is so that we don't get the 'offensive' warning.
	# XXX - Is this needed for Ex?
	fetch "${sitepage}/?nw=always" tmp.1

	while [ ! $FETCH_RESULT = 0 ]; do
		fetch "${sitepage}/?nw=always" tmp.1
	done

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
	# the last sed deals with names like fate/stay night which are invalid
	folder="$(cat tmp.1 | grep 'title>' | sed -e 's/<title>//g'  -e 's/<\/title>//g' -e 's/ - E-Hentai Galleries//g' -e 's/ - ExHentai.org//g' -e 's|/|_|g' | entity_to_char | remove_illegal)"
	mkdir -p "$folder"
	cd "$folder"
	
	page=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/' | sed 's/"><img alt.*//g' | grep 'url:' | sed 's/url://g')

	MAXPAGES=$(cat ../tmp.1 | grep 'Length:' | sed -e 's|.*Length:</td><td class="gdt2">||g' -e 's| pages.*||g')

	rm ../tmp.1

	echo -n "[E-H] Downloading '$folder' "
	if [ $LOGGEDIN = 1 ]; then
		echo -n "[HQ] "
	else
		echo -n "[LQ] "		
	fi

	doneyet=0

	CDNFAIL=0

	CUR=1
	while [ $doneyet = 0 ]; do

		fetch "$page" "$CUR.htm"
		while [ ! $FETCH_RESULT = 0 ]; do
			# For the most part, we'll not hit this unless there's network issues.
			# However, it can cause a failed fetch to go uncaught, causing next to be ''
			# And if the next loop "succeeds" with a zero length page, it will propogate
			# to next, and the 'done' check will trigger incorrectly.
			message=":O"
			spinner "RETR $CUR / $MAXPAGES"
			fetch "$page" "$CUR.htm"
		done

		lq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'keystamp' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
		hq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
		next_cnt=$((CUR + 1))
		next=$(cat $CUR.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

		extra=""

		spinner "IMG $CUR / $MAXPAGES"
		if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
			fetch "$lq" "$(basename $lq)"
		else
			# HQ version; this is the 'Download source resolution' button.
			fetch "$hq"
		fi

		if [ ! $FETCH_RESULT = 0 ]; then
			message=" Alt"

			while [ ! $FETCH_RESULT = 0 ]; do
				# Clobber existing pages.
				CLOBBER=1

				fetch "$page" "${CUR}.htm"
				notload=$(cat ${CUR}.htm | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")

				spinner "FAL $CUR / $MAXPAGES"
				fetch "$page?nl=$notload" "${CUR}nl.htm"

				# Turn clobber off now that we're done.
				CLOBBER=0
			done

			lq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'image.php' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
			hq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
			next=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

			spinner "IMG $CUR / $MAXPAGES"
			if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
				fetch "$lq"
			else
				fetch "$hq"
			fi

			rm "${CUR}nl.htm"

			if [ $FETCH_RESULT = 0 ]; then
				CUR=$(( CUR + 1 ))
				if [ ! "$next" = "" ]; then
					page="$next"
				fi
			fi
		else
			rm $CUR.htm
			CUR=$(( CUR + 1 ))
			if [ ! "$next" = "" ]; then
				page="$next"
			fi
		fi

		# This is a much saner condition than checking if next is null.
		if (( $CUR > $MAXPAGES )); then
			break
		fi

		spinner "$CUR / $MAXPAGES"
	done
	
	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	# TODO - Support grabbing page list of one tag.
	# There are some things to account for though; namely, layouts. Thumb layout vs list.
	# Number of things per page, etc.
	# A pain basically.
	echo -e "[E-H] Not yet implemented."
}
#############################################
#####@ORIGINAL-FILE 'modules/fakku'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Preface - code does not discriminate. Plus, if you read manga, let's be honest;
# you've made the mistake of ending up here accidentally once.

fakku_longname="FAKKU"
fakku_url="fakku.net/"
fakku_state=1
fakku_filt=0
fakku_note="They've been shuffling things around recently."

auto_fakku() {
	if [ -n "$(echo $1 | grep 'fakku.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
		# Fakku
		return 1
	fi

	return 0
}

dl_fakku() {
	# They don't play well with content disposition. Actually; they reply 404.
	SAFETY_HACKS=1

	PAGE="$(fetch "$1/read" "-")"

	# First. Escape fixups. Nuke escaped forward slashes
	# Next. Reformat decl.
	# Next. Nuke '.thumb'
	# Next. Nuke commas. Still, they can occur in names so we'll carefully filter.
	# Last transform. '/thumbs/' to '/images/'
	PAGEDATA="$(echo "$PAGE" | grep "window.params.thumbs =" | sed -e 's/\\\//\//g' -e 's/\];/)/g' -e 's/window.params.thumbs = \[/pages=(/g' -e 's/\.thumb//g' -e 's/","/" "/g' -e 's/\/thumbs\//\/images\//g')"

	# Because some pages have unicode escapes (javascript thing ugh) we have to echo the contents of the file
	# to itself to escape them. UGHHHHH

	DATA="$(printf "$PAGEDATA")"

	folder="$(echo "$PAGE" | grep '<title>' | sed -e 's/[[:space:]]*<title>Read //g' -e 's|</title>||g' | entity_to_char)"
	mkdir -p "$folder"
	cd "$folder"

	# Load in the array.
	eval "$DATA"

	# Download loop-de-loop.

	echo -n "[Fakku] Downloading '$folder' "

	CUR=0
	for image in "${pages[@]}"; do
		fetch "https:$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done

	cd ..
	
	cbz_make "$folder"
}

scrape_fakku() {
	echo -n "[Fakku] Scraping Chapters..."
	
	fetch "$1" scrape.htm

	grep 'class="content-title"' scrape.htm > batch.txtf

	sed -i 's|^.*href="||g' batch.txtf
	sed -i 's|" title=.*||g' batch.txtf
	sed -i "s/^[[:space:]]*//" batch.txtf
	sed -i "s/[[:space:]]*$//" batch.txtf

	# Links are local.
	sed -i "s|^|https://fakku.net|g" batch.txtf

	# Don't bother doing any re-ordering here, since orders are chaotic.
	cat batch.txtf >> batch.txt

	# Clean up.
	rm scrape.htm batch.txtf

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Fakku] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/dynsc'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

dynsc_longname="Dynasty Scans"
dynsc_url="dynasty-scans.com/"
dynsc_state=1
# No filter
dynsc_filt=0

auto_dynsc() {
	if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Dynasty Scans.
		return 1
	fi

	return 0
}

dl_dynsc() {
	
	# Now loop-de-loop. First, make a decent name. Dynasty always has
	# a short-title at the end of the URL.

	PAGEDATA="$(fetch "$1" "-")"

	folder="$(echo "$PAGEDATA" | grep "<title>" | sed -e 's/<title>Dynasty Reader &raquo; //g' -e 's|</title>||g')"

	mkdir -p "$folder"
	cd "$folder"

	PAGELIST="$(echo "$PAGEDATA" | grep "var pages")"

	# This set of seds cuts up the pagelist in a manner
	# that makes it identical to a bash array.
	# So we're essentially modifying the webpage into a dl-script.
	# Cool, eh?

	PAGETMP="$(echo $PAGELIST | sed -e "s/\"image\"\://g" -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" -e "s/\}\]/\)/g" -e "s/{//g" -e "s/}//g" -e "s/;//g" -e "s/ //g" -e "s/varpages=\[/pages=\(/g" -e "s/,/ /g")"

	# One possible nasty. Spaces.
	# sed -i "s/\%20/ /g" tmp.1

	# Load in the array.
	eval "$PAGETMP"

	echo -n "[DynastyScans] Downloading '$folder' "

	CUR=0

	for image in "${pages[@]}"; do
		fetch "http://dynasty-scans.com$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done
	
	cd ..

	cbz_make "$folder"
}

scrape_dynsc() {
	echo -n "[DynastyScans] Scraping Chapters..."

	# URLS are local.
	fetch "$1" "-" | 			\
	grep 'class="name"' | 		\
	sed -e 's|^.*href="||g' 	\
		-e 's|" class=.*||g' 	\
		-e "s/^[[:space:]]*//" 	\
		-e "s/[[:space:]]*$//" 	\
		-e "s|^|http://dynasty-scans.com|g" >> batch.txt

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/foolsl'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Unlike all of the other plugins, this is a generic downloader for foolslide based sites.
# Therefore, instead of keeping it a generic and needing to specifically ask for this plugin,
# I use a compatibility list.

# Sites using foolslide:
#   vortex-scans.com
#   foolrulez.org

foolsl_longname="FoolSlide"
foolsl_url="Generic"
foolsl_state=1
foolsl_filt=0

auto_foolsl() {
	if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# FoolRulez
		return 1
	elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# VortexScans
		return 1
	fi

	return 0
}

dl_foolsl() {
	# Attempt a lazy download first. Only image scrape if rejected.

	LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`

	echo -n "[FoolSlide] Attempting Lazy Download..."

	FAILED=0

	fetch "$LAZYURL" || export FAILED=1

	if [ $FAILED = 1 ]; then
		echo "Requesting zip failed."
	else
		echo "[OK]"
	fi
}

scrape_foolsl() {

	echo -n "[Foolslide] Scraping Chapters..."

	fetch "$1" "-" |							\
	grep '<div class="title"><a href='			\
	sed -e 's|<div class="title"><a href="||g'	\
	-e 's|" title=.*||g'						\
	-e "s/^[[:space:]]*//"						\
	-e "s/[[:space:]]*$//"					  | \
	reverse_lines >> batch.txt
	
	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Foolslide] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/mpark'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

mpark_longname="MangaPark"
mpark_url="mangapark.me"
# Broken
mpark_state=1
# No filter
mpark_filt=0

auto_mpark() {
	if [ -n "`echo $1 | grep 'mangapark.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Mangapark
		return 1
	fi

	return 0
}

dl_mpark() {

	sitepage="$1"

	# We need a specific type of URL - the 'all on one page' type. Remove specifiers.
	sitepage=`echo $sitepage | sed "s|/1$||g" | sed "s|/3-1$||g" | sed "s|/6-1$||g" | sed "s|/10-1$||g"`
	
	FETCH="$(fetch "$sitepage" "-")"

	folder="$(echo "$FETCH" | grep '<title>' | sed -e 's/<title>//g' -e 's/ Online For Free.*$//g' -e 's/.* - Read //g')"
	mkdir -p "$folder"
	cd "$folder"

	declare -a DATA
	DATA=$(echo "$FETCH" | grep 'target="_blank"' - | sed -e '1d' -e 's|^[[:space:]]*<a.*target="_blank" href=||g' -e "s/ title=.*$//" -e "s/\"//g"| tr '\n' ' ')

	echo -n "[Mangapark] Downloading '$folder' "

	CUR=0
	for image in ${DATA[@]}; do
		fetch "$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done
	
	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_mpark() {
	echo -e "[Mangapark] Scraping Chapters..."

	fetch "$1" scrape.htm

	grep 'class="ch sts"' scrape.htm > batch.txtr

	sed -i 's|^.*href="||g' batch.txtr
	sed -i 's|">.*||g' batch.txtr
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	# URLS are local.
	sed -i "s|^|http://mangapark.com|g" batch.txtr

	# We need a specific type of URL - the 'all on one page' type. Remove specifiers.
	sed -i "s|/1$||g" batch.txtr
	sed -i "s|/3-1$||g" batch.txtr
	sed -i "s|/6-1$||g" batch.txtr
	sed -i "s|/10-1$||g" batch.txtr

	# Lines are reverse order.
	cat batch.txtr | reverse_lines >> batch.txt

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr

	echo -e "[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@AUTOGENERATED 'booru_MODS'
booru_MODS=(danbooru)
#############################################
#####@ORIGINAL-FILE 'modules/booru.mods/danbooru'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# This should set up any variables needed by this module.

booru_danbooru_init() {
	final[0]="$source/posts.json?tags="
}


# This should build a URL which can download a page; call dl_booru_page,
# And then extract the pages. It should not itself do the download.

booru_danbooru_page() {
	final[4]=$1

	if [ $RESUME = 1 ]; then
		# Assume this is done.
		return 1
	fi

	if [ $is_pool = 0 ]; then
		dl_booru_page "$(echo ${final[@]} | tr -d ' ')" "page${range}.json"

		BEFORE=$(wc -l meta.txt | sed 's| .*||g')
		cat "page${range}.json" | tr ',' "\n" | grep '"id"' | sed 's|.*:||g' >> meta.txt
		AFTER=$(wc -l meta.txt | sed 's| .*||g')

		if [ "$BEFORE" = '' ]; then
			BEFORE=0
		fi
		PAGE_SIZE=$((AFTER - BEFORE))

		# We preprocess metadata.
		sed -i "s|},{|}]\n[{|g" "page${range}.json"
		# There's no final newline, so the read will miss an ID and have to refetch
		# later even though that's dumb.
		echo '' >> "page${range}.json"

		LEN=0
		while read line; do
			FIRST=$((PAGE_SIZE - LEN))
			id="$(cat meta.txt | tail -n $FIRST | head -n 1)"
			echo "$line" > meta/meta_${id}.json
			LEN=$((LEN + 1))
		done < "page${range}.json"
	
		rm "page${range}.json"

		# No more pages.
		if (( BEFORE == AFTER )); then
			return 1
		fi

		# Still more.
		return 0
	else
		final[0]="${source}/pools.json?commit=Search&search[order]=updated_at&search[name_matches]="

		dl_booru_page "$(echo ${final[@]} | tr -d ' ')" "${final[1]}.json"

		cat "${final[1]}.json" | tr ',' "\n" | grep '"post_ids"' | sed -e 's|.*:||g' -e 's|"||g' | tr ' ' "\n" >> meta.txt
	fi
}

# This should download a page's meta info. If there isn't any, it should copy the ID to a file named as the ID.

booru_danbooru_meta() {
	declare -a base
	base[0]="$source/posts/"
	base[1]="${1}.json"

	if [ ! -e "meta_${base[1]}" ]; then
		message="fetch ${1}"
		spinner "${DONE} -> ${LINES}"
		message=""

		dl_booru_page "$(echo ${base[@]} | tr -d ' ')" "meta_${base[1]}"
	fi
}

booru_danbooru_content() {
	declare -a base
	base[0]="$source/posts/"
	base[1]="${1}.json"

	url_img="$(cat "${META_DIR}/meta_${base[1]}" | tr ',' "\n" | grep '"file_url"' | sed -e 's|.*:||g' -e "s|\"||g")"
	file_ext="$(cat "${META_DIR}/meta_${base[1]}" | tr ',' "\n" | grep '"file_ext"' | sed 's|.*:||g')"

	if [ "$file_ext" = "" ] || [ "$url_img" = "" ]; then
		spinner_done
		echo "[Booru] ID:${1} appears to be deleted/hidden. Skipping."

		# ID is deleted. Skip.
		return
	fi

	if [ ! -e "image_${1}.${file_ext}" ]; then
		dl_booru_page "${source}${url_img}" "image_${1}.${file_ext}"
	fi
}
#############################################
#####@ORIGINAL-FILE 'modules/booru'
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# This is another generic; also, there's a bit of extra here. You can provide either a URL
# or a known Booru's name, and use the filter for a tag.

# e.g. scangrab booru danbooru touhou
# will fetch everything from the touhou tag. (which is expensive. How many damned pages are there?)

# Past that, you can also specify how many pages, like so:
#   scangrab booru danbooru touhou:50
#   (downloads 50 pages)
# Or how many images you want:
#   scangrab booru danbooru touhou::100
#   (downloads 100 images)
# You can also specify ranges if you want to get specific; like so:
#   scangrab booru danbooru touhou:20-30:10-150
#   (starts at page twenty, downloads till page 30. Skips 10 images, and downloads 140.
# Because of this, it's recommended to avoid passing URLs. They get cut up for parameters, anyways.

# Danbooru also functions differently; it is the only grabber which DOES NOT zip up things.
# By default, things are stored in folders by the tag you grab by, and named according to the image's
# MD5 (as danbooru does.) A file is stored with it that has meta-info from danbooru.

booru_longname="*Booru"
booru_url="Generic"
booru_state=2
booru_filt=1


auto_booru() {
	if [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
		# Danbooru / Safebooru
		return 1
	elif [ "$1" = "danbooru" ]; then
		# Danbooru direct.
		return 1
	elif [ "$1" = "testbooru" ]; then
		# Danbooru direct.
		return 1
	elif [ "$1" = "safebooru" ]; then
		# Safebooru direct.
		return 1
	fi

	return 0
}

# Because the syntax is hell.
booru_syntax_help() {
	echo "The booru downloader requires parameters. For reference, this is after the booru name/url:"
	echo "    Usage: booru NAME TAGLIST SPEC [FOLDERNAME]"
	echo "TAGLIST:"
	echo "  Here's some quick help: after the booru name, you should specify some of these:"
	echo "     tagname                   - Specify a tag."
	echo "     tagname1,tagname2,...     - Specify multiple tags."
	echo "     pool:poolname             - Download from a pool."
	echo ""
	echo "  The same syntax as with boorus can be used with tags. For example:"
	echo "     art:tagname               - Search by artist."
	echo "     char:tagname              - Search by character."
	echo "     copy:tagname              - Search by copyright."
	echo "     user:username             - Anything uploaded by username."
	echo "     fav:username              - Anything favorited by username."
	echo "     ordfav:username           - Anything favorited by username (chronological.)"
	echo "     md5:hash                  - Posts with a specific MD5 hash."
	echo "     rating:rate               - Safety rating, e.g. safe, questionable, explicit..."
	echo "     And so on. See http://safebooru.donmai.us/wiki_pages/43049 for a more exhaustive reference."
	echo ""
	echo "  And a quick reference on modifiers:"
	echo "     tag1,-tag2        - Anything matching tag1, but not tag2."
	echo "     ~tag1,~tag2       - Anything marked tag1 or tag2."
	echo "     tag1,tag2         - Anything marked with both tag1 AND tag2."
	echo "     *tag*             - All tags containing 'tag'."
	echo ""
	echo "SPEC:"
	echo "  Syntax:"
	echo "    p10                - Download page 10."
	echo "    l50,p10            - 50 images per page, download page 10. (This is a lowercase L if your font doesn't distinguish.)"
	echo "    i100               - Download 100 images."
	echo "    i5-100             - Download images 5-100."
	echo "    p10-50             - Download from page 10 to 50 inclusive."
	echo "    l50,p10,i100       - Pages with 50 images, save the first 100 images starting at page 10."
	echo "    l50,p5-10,i100     - Pages with 50 images, save the first 100 images starting at page 5."
	echo "                         This syntax is legal, but -10 is ignored."
	echo "    pages:50           - Download 50 pages. Long syntax."
	echo "    images:100         - Download 100 images. Long syntax."
	echo "    limit:50           - 50 images on a page. Long syntax."
	echo "    resume             - Resume a previous Ctrl+C'd download."
	echo "    r                  - Short for resume."
	echo "  A minor note is that *technically* limit:N/lN is a tag for booru."
	echo "  It can be specified as a tag, but may screw up download code, so do that here."
	echo ""
	echo "FOLDERNAME (optional):"
	echo "  If not specified, it uses the format 'NAME - TAGLIST'."
	echo ""
	echo "Also, please note that the following sites are either alternate source bases or NOT boorus,"
	echo "but will eventually be supported here due to some structural similiarities:"
	echo " Moebooru-based (https://github.com/moebooru/moebooru)"
	echo "   yande.re (yandere)"
	echo "   konachan.com (konachan_g) / konachan.net (konachan)"
	echo " Shimmie-based (https://github.com/shish/shimmie2)"
	echo "   shimmie.katawa-shoujo.com (mishimme)"
	echo " Custom / Needs research"
	echo "   zerochan.net (zerochan)"
	echo " All the same, dunno what they run"
	echo "   *.booru.net (Many different things)"
	echo "   safebooru.net (Not the same as safebooru)"
	echo "   gelbooru.net"
	exit 1
}

# Range to download.
declare -a pagerange
declare -a imagerange
declare -a tagarray
declare -a sizearray
declare -a final
TAGCOUNT=0
page_index=0
# base URL
final[0]="$source/posts?tags="
# Tags
final[1]=""
# Images per page. Blank if no.
final[2]=""
# page=
final[3]="&page="
# Page Number
final[4]=1
RESUME=0

pagerange[0]=1
pagerange[1]=1

# Paged mode by default.
# 1=paged, 2=images
mode=1
source=""
name=""

is_pool=0

# Computes final tags.
booru_tag_crunch() {
	# Tag array can be pasted together.
	for (( i=0 ; i < ${#tagarray[@]} ; i++ )); do
		if [ ! "$i" = "0" ] && [ ! "$((i + 1))" = "${#tagarray}" ]; then
			final[1]="${final[1]}+"
		fi

		# This is a pool. Ignore the other things.
		if [[ "${tagarray[i]}" == pool:* ]]; then
			echo "[Booru] This is apparently a pool."
			is_pool=1
			TAGCOUNT=1
			final[1]="$(echo ${tagarray[i]} | sed -e 's|pool:||g')"
			break
		fi

		tag="$(echo ${tagarray[i]} | sed -e 's|:|%3A|g')"

		final[1]="${final[1]}${tag}"
		if [[ ! "$tag" == *:* ]]; then
			TAGCOUNT=$((TAGCOUNT + 1))
		fi
	done
}

booru_size_crunch() {
	# No settings for a pool.
	if [ $is_pool = 1 ]; then
		return
	fi

	for (( i=0 ; i < ${#sizearray[@]} ; i++ )); do
		if [[ "${sizearray[i]}" == pages:* ]] || [[ "${sizearray[i]}" == p* ]]; then
			pagerange=($(echo "${sizearray[i]}" | tr -d 'p' | tr '-' ' '))
			if [ "${pagerange[1]}" = "" ]; then
				pagerange[1]=${pagerange[0]}
			fi
			final[4]=${pagerange[0]}
		elif [[ "${sizearray[i]}" == limit:* ]] || [[ "${sizearray[i]}" == l* ]]; then
			if [[ ! "${sizearray[i]}" == "limit:*" ]]; then
				final[2]="+$(echo ${sizearray[i]} | sed 's|l|limit:|')"
			else
				final[2]="+${sizearray[i]}"
			fi
			final[2]="$(echo ${final[2]} | sed -e 's|:|%3A|g')"
		elif [[ "${sizearray[i]}" == images:* ]] || [[ "${sizearray[i]}" == i* ]]; then
			imagerange=($(echo "${sizearray[i]}" | tr -d 'i' | tr '-' ' '))
			if [ "${imagerange[1]}" = "" ]; then
				imagerange[1]=${imagerange[0]}
				imagerange[0]=1
			fi
			mode=2
		elif [[ "${sizearray[i]}" == "all" ]]; then
			# 1st page.
			pagerange[0]=1
			# Arbitrarily high number that will never be reached. Maybe. Dunno.
			# Point is, we stop when no images can be extracted from the page.
			pagerange[1]=$MAX_INT

			final[4]=${pagerange[0]}
		elif [[ "${sizearray[i]}" == "resume" ]] || [[ "${sizearray[i]}" == "r" ]]; then
			# Resume.
			RESUME=1
		fi
	done
}

dl_booru_page() {
	url="$1"
	name_out="$2"

	FETCH_RESULT=1
	while [ ! $FETCH_RESULT = 0 ]; do
		fetch "$url" "$name_out"

		if [ ! $FETCH_RESULT = 0 ]; then
			FAILS=$((FAILS + 1))
			message=" :<"
			if [ ! $FAILS = 0 ]; then
				spinner_done "!!!"

				echo -en "[Booru] Server refused us. Waiting a bit... "
				SLEEP=$((60 * FAILS))
				while [ ! $SLEEP = 0 ]; do
					sleep 1s
					spinner "${SLEEP}s"
					SLEEP=$((SLEEP - 1))
				done
				
				spinner_done "DONE"

				echo -en "[Booru] Continuing... "
			fi
		fi
		
		# This is for safety.
		# A lot of boorus get mad at you for hammering the database repeatedly and punish you
		# by denying connections for a while. This ensures we aren't hammering *too* hard.
		sleep 2s
	done
	FAILS=0
}

dl_booru() {
	if [ "$2" = "" ] && [ "$3" = "" ] || [ "$1" = "" ]; then
		booru_syntax_help
	fi

	if [ -n "$(echo $1 | grep 'safebooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "safebooru" ]; then
		# Safebooru directly. NEVER MOVE THIS CHECK BENEATH DANBOORU. THE GREP FOR DANBOORU WILL MATCH.
		source="http://safebooru.donmai.us"
		name="danbooru"
		site="safebooru"
	elif [ -n "$(echo $1 | grep 'testbooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "testbooru" ]; then
		# Testbooru (which has four images and is a good test.) NEVER MOVE THIS CHECK BENEATH DANBOORU. THE GREP FOR DANBOORU WILL MATCH.
		source="http://testbooru.donmai.us"
		name="danbooru"
		site="testbooru"
	elif [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "danbooru" ]; then
		# Danbooru
		source="http://danbooru.donmai.us"
		name="danbooru"
		site="danbooru"
	else
		# Unknown, but apparently a booru.
		source="$1"
		echo "[Booru] This type of Booru isn't specified as supported."
		echo "[Booru] It may work, depending on whether the autodetect picks up a valid scheme,"
		echo "[Booru] or it may completely bomb out. If it works, please submit a bug report"
		echo "[Booru] so I can add it to the supported list."
		name="unknown"
		site="$source"
	fi

	booru_${name}_init

	shift

	tagarray=($(echo "$1" | tr ',' ' '))
	sizearray=($(echo "$2" | tr ',' ' '))

	booru_tag_crunch
	booru_size_crunch

	echo "[Booru] Info:"
	echo "[Booru]   Fetching URL $(echo ${final[0]}${final[1]}${final[2]}${final[3]} | tr -d ' ')"
	echo -n "[Booru]   Will download "
	if [ $mode = 1 ]; then
		if [ "${pagerange[0]}" = "${pagerange[1]}" ]; then
			echo "page ${pagerange[0]}."
		else
			TO=${pagerange[1]}
			if [ $TO = $MAX_INT ]; then
				TO="Max"
			fi
			echo "pages ${pagerange[0]} to $TO."
		fi
	else
		# XXX - Why in god's name would a user do this? Dunno, but handle their idiocy anyways.
		if [ "${imagerange[0]}" = "${imagerange[1]}" ]; then
			echo -n "image ${imagerange[0]}"
		else
			echo -n "images ${imagerange[0]} to ${imagerange[1]}"
		fi
		echo ", starting from page ${pagerange[1]}."
	fi

	if [ $TAGCOUNT = 0 ] && [ ${pagerange[1]} == $MAX_INT ]; then
		# Holy fucking shit. The user is nuts. Downloading a whole booru? WTF?
		# Ask them to make damn well sure that they meant that.

		echo "[Booru] Holy fucking shit. You're telling me to download a whole booru? Are you insane?"
		echo "[Booru] Not to mention, I didn't even DOCUMENT this syntax on purpose."
		echo "[Booru] Type 'Yes, I am sane.' after the colon if you really meant to do this."
		echo -n "[Booru] You sane? : "
		read sanequery
		if [ "$sanequery" = "Yes, I am sane." ]; then
			echo "[Booru] Okay, nutjob, if you say so. Don't come complaining later."
		else
			echo "[Booru] Yeah, thought so. Whew~"
			exit 1
		fi
	fi

	if [ ! "${final[2]}" = "" ]; then
		echo "[Booru]   Requesting $(echo ${final[2]} | sed 's|+limit%3A||') images per page."
	fi
	if (( $TAGCOUNT >= 2 )); then
		echo "[Booru]   Warning, more than two normal tags. Some boorus don't like this."
	fi

	if [ ! "$3" = "" ]; then
		tagdir="$3"
	else
		tagdir="$site - $1"
	fi

	echo "[Booru]   Output to folder '$tagdir'."

	mkdir -p "$tagdir"
	cd "$tagdir"

	mkdir -p meta content

	# The first file is expected to be in the root tagdir.

	# Download page data and make a list of IDs.
	# Also, ideally, extract metadata.
	if [ $RESUME = 0 ]; then
		echo -n '' > meta.txt
		echo -n "[Booru] Fetching pages... "
		for (( range=${pagerange[0]} ; range <= ${pagerange[1]} ; range++ )); do
			spinner "${range} -> ${pagerange[0]}/${pagerange[1]}"
			booru_${name}_page "$range"
			R=$?

			lines=$(wc -l meta.txt | sed 's| .*||g')
			if [ $mode = 2 ] && (( $lines > ${imagerange[1]} )); then
				break
			fi

			if [ $R = 1 ]; then
				break
			fi
		done
	fi

	spinner_done

	lines=$(wc -l meta.txt | sed 's| .*||g')

	DONE=0
	LINES=$(wc -l meta.txt | sed 's| .*||g')

	# All metadata files should be placed in tagdir/meta.

	cd meta

	echo -n "[Booru] Checking and refetching metadata... "

	# Ideally metadata should have been taken care of above.
	# Some sites don't provide all info in a page API query though.
	while read meta_id; do
		SKIP=0
		if [ $mode = 2 ]; then
			if (( $DONE < ${imagerange[0]} )); then
				SKIP=1
			fi

			if (( $DONE > ${imagerange[1]} )); then
				break
			fi
			TOTAL="$RANGE"
		elif [ $RESUME = 1 ]; then
			DONE=$((DONE - 1))
			if [ $DONE = 0 ]; then
				RESUME=0
				DONE=$PRE
			fi 
		else
			TOTAL="$LINES"
			spinner "${DONE} -> ${LINES}"
			booru_${name}_meta "$meta_id"
			DONE=$((DONE + 1))
		fi
	done < ../meta.txt

	spinner_done

	echo -n "[Booru] Downloading content... "

	META_DIR="$(pwd)"

	# All images should be in tagdir/content
	cd ../content

	DONE=0

	# Download images.
	while read meta_id; do
		SKIP=0
		if [ $mode = 2 ]; then
			if (( $DONE < ${imagerange[0]} )); then
				SKIP=1
			fi

			if (( $DONE > ${imagerange[1]} )); then
				break
			fi
			TOTAL="$RANGE"
		elif [ $RESUME = 1 ]; then
			DONE=$((DONE - 1))
			if [ $DONE = 0 ]; then
				RESUME=0
				DONE=$PRE
			fi 
		else
			TOTAL="$LINES"
			spinner "${DONE} -> ${LINES}"
			booru_${name}_content "$meta_id"
			DONE=$((DONE + 1))
		fi
	done < ../meta.txt

	rm ../meta.txt

	spinner_done

	echo "[Booru] Done with download. If you don't care about the metadata, it is no longer needed."
}

scrape_booru() {
	exit 1
}
#############################################
#####@ORIGINAL-FILE 'main'
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Determine operation.

if [ "$1" = "auto" -o "$1" = "a" ]; then
	# Common operation - Automatic Module Select.
	shift
	auto "$@"
	exit 0
elif [ "$1" = "batch" -o "$1" = "l" ]; then
	# Common operation - typed batch.
	shift
	batch "$@"
	exit 0
elif [ "$1" = "autobatch" -o "$1" = "b" ]; then
	# Common operation - auto batch.
	shift
	autobatch "$@"
	exit 0
elif [ "$1" = "scrape" -o "$1" = "s" ]; then
	# Link scraper.
	shift
	scrape "$@"
	exit 0
elif [ "$1" = "login" -o "$1" = "u" ]; then
	# Site login.
	shift
	mod_login "$@"
	exit 0
elif [ "$1" = "upgrade" ]; then
	# Upgrade self.
	upgrade_self
	exit 0
else
	# Not a common operation - either invalid, a module, or a lazy URL.

	# Detect whether it is a module operation
	MATCH=""
	for module in ${MODS[@]}; do
		if [ "$1" = "$module" ]; then
			shift
			dl_${module} "$@"
			exit 0
		fi
	done

	# Try to see if this was a lazy URL as first parameter.
	auto "$@"
	R=$?
	if [ $R = 1 ]; then
		# All checks failed. Usage~
		help
	fi
fi
#############################################
