#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# These values can be read but should not be written by a module.
COOKIEJAR="$(pwd)/cookiejar"
HAVE_IMAGICK=0
FETCH_RESULT=0

# These values are used by support/core but are controlled by modules.
message=""
use_cookies=0

# Anything which starts with an underscore is internal and shouldn't be touched.
_COLOR=0
_FETCH_CMD=""
_CHECK_VALID=0
_FETCHTOOL=0
_BUSYBOX=0
_SPINNER_CHAR="|"
_NUM=0
_FIRST=1
_MOST=0

# Tools. Use these variables instead of actual command names.
_tput="$(which tput 2>/dev/null)"
_identify="$(which identify 2>/dev/null)"
_convert="$(which convert 2>/dev/null)"
_wget="$(which wget 2>/dev/null)"
_curl="$(which curl 2>/dev/null)"
_aria="$(which aria2c 2>/dev/null)"

for ((i = 1, n = 2;; n = 1 << ++i)); do
	if [[ ${n:0:1} == '-' ]]; then
		MAX_INT=$(((1 << i) - 1))
		break
	fi
done

if [ "$TERM" = "xterm" ] || [ "$TERM" = "linux" ] || [ "$TERM" = "screen" ]; then
	_COLOR=1
	if [ -f $_tput ]; then
		# Better method.
		_COLOR=2
	fi
fi

if [ -f "$_identify" ]; then
	_CHECK_VALID=1
fi

if [ -f "$_identify" ]; then
	HAVE_IMAGICK=1
fi

if [ ! "$_wget" = "" ]; then
	common_opts=" --quiet --no-cache --content-disposition --user-agent=\"Mozilla/4.0\" -c -t 1 -T 10 --random-wait "

	if [ ! "$($_wget --help 2>&1 | grep busybox)" = "" ]; then
		echo "[Warning] Your system wget is busybox, which can't actually do some things like reject cache and retry."
		common_opts=" -q -c -U \"Mozilla/4.0\""
		_BUSYBOX=1
	fi

	_FETCH_CMD="$_wget $common_opts"
	_FETCHTOOL=1
else
	if [ ! "$_curl" = "" ]; then
			_FETCH_CMD=$_curl
			_FETCHTOOL=2
	else
		if [ ! "$_aria" = "" ]; then
				_FETCH_CMD=$_aria
				_FETCHTOOL=3
		fi
	fi
fi	

type() {
	if [ $_COLOR = 1 ]; then
		echo -ne "\x1b[$1m"
	elif [ $_COLOR = 2 ]; then
		if [ $1 = 0 ]; then
			tput sgr0
		elif [ $1 = 1 ]; then
			tput bold
		elif [ $1 = 2 ]; then
			tput dim
		fi
	fi
}

color() { 
	if [ $_COLOR = 1 ]; then
		echo -ne "\x1b[3$1m"
	elif [ $_COLOR = 2 ]; then
		tput setaf $1
	fi
}

cbz_make() {
	echo -e "[Post] Making CBZ..."

	# Check and make sure the folder has something, and is actually a folder.
	if [ ! -d "$1" ]; then
		echo -e "\n[Error] Not a folder. Something went wrong."
		exit 1
	fi
	
	if [ "$(ls "$1")" = "" ]; then
		echo "[Error] No files? Download failed."
		exit 1
	fi

	zip -r "$1.zip" "$1" > /dev/null 2>&1
	mv "$1.zip" "$1.cbz" > /dev/null 2>&1
	
	echo -e "[Post] Cleanup..."
	rm -rf "$1"
	echo -e "[Post] Finished!"
}

# Checks if an image is uncorrupted. If you don't have the tools, this never happens.
	# Returns 0 on OK / no tool
	# Returns error code on corrupt
verify() {
	if [ -f $_identify ]; then
		$_identify -verbose -regard-warnings "$1" 2>&1 >/dev/null
		_IDRES=$?
		return $_IDRES
	fi

	return 0
}

spinner() {
	if [ $_FIRST = 1 ]; then
		_FIRST=0
	else
		echo -ne "\b\b\b\b"
	fi

	if [ "$_SPINNER_CHAR" = "|" ]; then
		_SPINNER_CHAR="/"
	elif [ "$_SPINNER_CHAR" = "/" ]; then
		_SPINNER_CHAR="-"
	elif [ "$_SPINNER_CHAR" = "-" ]; then
		_SPINNER_CHAR="\\"
	elif [ "$_SPINNER_CHAR" = "\\" ]; then
		_SPINNER_CHAR="|"
	fi

	_NUM=${#1}
	if (( _NUM > _MOST )); then
		_MOST=$_NUM
	fi

	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne "\b"
	done

	for (( i=0 ; i < _MOST ; i++ )); do
		echo -ne " "
	done

	for (( i=0 ; i < _MOST ; i++ )); do
		echo -ne "\b"
	done

	echo -ne "[$1 $_SPINNER_CHAR]"

	_STS=${#message[@]}
	echo -ne "$message"
	for (( i=1 ; i < _STS ; i++ )); do
		echo -ne "\b"
	done
}

spinner_done() {
	# Move back.

	for (( i=0 ; i < _NUM ; i++ )); do
		echo -ne "\b"
	done

	for (( i=0 ; i < _MOST ; i++ )); do
		echo -ne " "
	done

	for (( i=0 ; i < _MOST ; i++ )); do
		echo -ne "\b"
	done

	_SPINNER_CHAR="|"
	_NUM=0
	_FIRST=1
	_MOST=0
	echo -e "\b\b\b\b[OK]"
}

mimetype() {
	echo "$(file --mime-type "$1" | sed 's/.* //g')"
}


# This creates a cookie jar.
# $1 - Username field
# $2 - Password field
# $3 - Username
# $4 - Password
# $5 - URL
# $6 - extra shit
s_login() {
	if [ $_FETCHTOOL = 1 ]; then
		# WGET
		_CMD="$_FETCH_CMD --post-data='$1=$3&$2=$4&$6' \"$5\"  --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies -O/dev/null"

	elif [ $_FETCHTOOL = 2 ]; then
		# CURL
		_CMD="$_FETCH_CMD -d '$1=$3&$2=$4&$6' $5 -b $COOKIEJAR -c $COOKIEJAR >/dev/null"

	elif [ $_FETCHTOOL = 3 ]; then
		#ARIA2C
		# How the fuck do I post? Maybe I can't with araia2c...meh.
		echo "[Warn] aria2c can't post at the moment; this will fail to get required cookies."
		_CMD="$_FETCH_CMD $5 --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR -o/dev/null"

	fi
	
	# echo -e "\n$_CMD"
	eval " $_CMD" 2>/dev/null
	FETCH_RESULT=$?
}

# AVOID CHANGING THIS FUNCTION IF AT ALL POSSIBLE.
# THINGS WILL BREAK IN EVERYTHING IF THIS ONE BREAKS.
fetch() {
	if [ $_FETCHTOOL = 1 ]; then

		_CMD="$_FETCH_CMD \"$1\""
		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies"
		fi

		# Wget uses content-disposition to hopefully get a good name in abscence of one.

		if [ "$2" = "-" ]; then
			_CMD="$_CMD -O -"
		elif [ ! "$2" = "" ]; then
			_CMD="$_CMD -O \"$2\""
		fi

	elif [ $_FETCHTOOL = 2 ]; then

		_CMD="$_FETCH_CMD $1"
		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD -b $COOKIEJAR -c $COOKIEJAR"
		fi

		if [ "$2" = "" ]; then
			_CMD="$_CMD > $(basename "$1")"
		elif [ "$2" = "-" ]; then
			_CMD="$_CMD"
		else
			_CMD="$_CMD > \"$2\""
		fi

	elif [ $_FETCHTOOL = 3 ]; then

		_CMD="$_FETCH_CMD $1"
		if [ $use_cookies = 1 ]; then
			_CMD="$_CMD --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR"
		fi

		if [ "$2" = "" ]; then
			_CMD="$_CMD -o $(basename "$1")"
		elif [ "$2" = "-" ]; then
			_CMD="$_CMD -o -"
		else
			_CMD="$_CMD -o \"$2\""
		fi
	fi
	
	# echo -e "\n$_CMD"
	eval " $_CMD" 2>/dev/null
	FETCH_RESULT=$?
	
	# If this is an image, check validity.
	MIME="$(mimetype "$_FILE")"
	if [ "$MIME" = "image/jpeg" ] || [ "$MIME" = "image/png" ] || [ "$MIME" = "image/gif" ] || [ "$MIME" = "image/bmp" ]; then
		verify "$_FILE"
		VALID=$?
		if [ ! $VALID = 0 ]; then
			echo "[WARN] File '$_FILE' is corrupted."
		fi
	fi

	return $FETCH_RESULT
}

entity_to_char() {
	# This probably doesn't handle every case. It should be enough.
	# It also handles the case of illegal characters on windows/FAT/NTFS.
	# And the case of a slash in the name which is the only illegal one on linux.

	sed \
		-e "s/&#32;/ /g" \
		-e "s/&nbsp;/ /g" \
		-e "s/&#33;/\!/g" \
		-e "s/&#34;/\"/g" \
		-e "s/&#35;/\#/g" \
		\
		-e "s/&#36;/\$/g" \
		-e "s/&#37;/\%/g" \
		-e "s/&amp;/\&/g" \
		-e "s/&#38;/\&/g" \
		-e "s/&#39;/'/g" \
		\
		-e "s/&#40;/\(/g" \
		-e "s/&#41;/\)/g" \
		-e "s/&#42;/\*/g" \
		-e "s/&#43;/\+/g" \
		-e "s/&#44;/\,/g" \
		\
		-e "s/&#45;/\-/g" \
		-e "s/&#46;/\./g" \
		-e "s/&#58;/\:/g" \
		-e "s/&#59;/\;/g" \
		-e "s/&lt;/\</g" \
		\
		-e "s/&#60;/\</g" \
		-e "s/&gt/\>/g" \
		-e "s/&#61;/\>/g" \
		-e "s/&#63;/\?/g" \
		-e "s/&#64;/\@/g" \
		\
		-e "s/&#91;/\[/g" \
		-e "s/&#92;/\\\\/g" \
		-e "s/&#93;/\]/g" \
		-e "s/&#94;/\^/g" \
		-e "s/&#95;/\_/g" \
		\
		-e "s/&#123;/\{/g" \
		-e "s/&#124;/\|/g" \
		-e "s/&#125;/\}/g" \
		-e "s/&#126;/\~/g" \
		-e "s/&yen;/¥/g" \
		\
		-e "s/&#165;/¥/g" \
		-e "s/&sup2;/²/g" \
		-e "s/&#178;/²/g" \
		-e "s/&sup3;/³/g" \
		-e "s/&#179;/³/g" \
		\
		-e "s/&frac14;/¼/g" \
		-e "s/&#188;/¼/g" \
		-e "s/&frac12;/½/g" \
		-e "s/&#189;/½/g" \
		-e "s/&frac34;/¾/g" \
		\
		-e "s/&#190;/¾/g" \
		-e "s/&spades;/♠/g" \
		-e "s/&#9824;/♠/g" \
		-e "s/&clubs;/♣/g" \
		-e "s/&#9827;/♣/g" \
		\
		-e "s/&hearts;/♥/g" \
		-e "s/&#9829;/♥/g" \
		-e "s/&diams;/♦/g" \
		-e "s/&#9830;/♦/g"

}

remove_illegal() {

	sed \
		-e "s/|/-/g" \
		-e "s|/|-|g"

}

reverse_lines() {
    readarray -t LINES
    for (( I = ${#LINES[@]}; I; )); do
        echo "${LINES[--I]}"
    done
}
rev=97089b0dd9d7873466ce33048f5b1d64b545b36c
branch=master
MODS=(batoto booru dynsc eh fakku foolsl mangabox mpark mread niconicoseiga)
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

batoto_longname="Batoto"
batoto_url="http://bato.to/"
batoto_state=1
batoto_filt=1
batoto_note="Their page template is highly unstable."

auto_batoto() {
	if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Batoto
		return 1
	fi

	return 0
}

dl_batoto() {
	# Batoto requires a different strategy.
	# The URLs are not preloaded like the former, so the fetch one page done thing won't work.
	# Unfortunately, short of grabbing pages until an image 404's, there's no way of knowing when we're done.
	
	folder="$(fetch "$1" "-" | grep -C0 "<title>" | sed -e "s/^[[:space:]]*<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e "s/[[:space:]]*$/\n/" | entity_to_char)"

	mkdir -p "$folder"
	cd "$folder"

	CUR=0
	PAGES=0
	RET=0
	
	base="$1"
	if [ ! "${base:${#base}-1}" = "/" ]; then
		base="${base}/"
	fi

	echo -n "[Batoto] Downloading '$folder' "

	while [ "$RET" = "0" ]; do
		# Increment CUR.
		CUR=$(( CUR + 1 ))

		# On batoto, two slashes is a syntax error as of Jun 13, 2015.

		# We also need to fetch to a file here unfortunately, because possible stupidity.
		fetch "${base}${CUR}" "$CUR.htm"

		# Batoto sometimes gives out gunzips. We need to account for that... =_=

		if [ "$(mimetype $CUR.htm)" = "application/x-gzip" ]; then
			mv $CUR.htm $CUR.htm.gz
			gunzip $CUR.htm.gz
			message=" :/"
		fi

		img="$(grep -C0 'z-index: 1003' $CUR.htm | sed -e 's/^[[:space:]]*<img src="//g' -e 's/".*$//g')"

		ext="${img##*.}"

		# If this 404's, fetch will return non-zero. Thus, loop breaks.
		fetch "$img" "${CUR}_${folder}.${ext}"
		RET=$?

		rm $CUR.htm
	
		spinner "$CUR"
	done

	PAGES=$(( CUR - 1 ))

	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_batoto() {
	notice_batoto

	echo -n "[Batoto] Scraping Chapters..."

	fetch "$1" scrape.htm
		
	# Batoto sometimes gives out gunzips. We need to account for that... =_=
	if [ "$(mimetype scrape.htm)" = "application/x-gzip" ]; then
		mv $CUR.htm $CUR.htm.gz
		gunzip $CUR.htm.gz
		message=" :/"
	fi

	grep -A 2 'Sort:' scrape.htm >> batch.txtr

	# Delete the useless lines.
	sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr

	# Remove Language lines.
	sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
	sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr

	# Edit up URL.
	sed -i "s|<a href=\"||g" batch.txtr
	sed -i "s|\" title=.*||g" batch.txtr

	# Delete blank lines/space lines
	sed -i '/^[[:space:]]*$/d' batch.txtr

	# Strip.
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	sed -i 's|^--$||g' batch.txtr

	# Delete Blank lines.
	sed -i '/^$/d' batch.txtr

	cat batch.txtr | reverse_lines > batch.txtf

	if [ "$2" = "" ]; then
		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf
		
		cat batch.txtf >> batch.txt
	else
		echo -ne "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Applying Language Filter '$2'..."

		grep -A 1 "$2" batch.txtf > batch.txtf2
		
		# Delete '--' lines
		sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2

		# Delete Blank lines.
		sed -i '/^$/d' batch.txtf2

		# Delete Language text, leaving urls
		sed -ni '0~2p' batch.txtf2

		cat batch.txtf2 >> batch.txt
	fi

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr batch.txtf*
	
	echo -en "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"

	for ((n=0;n < ${#2}; n++)); do
		echo -en '\b'
	done

	echo -e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# This is another generic; also, there's a bit of extra here. You can provide either a URL
# or a known Booru's name, and use the filter for a tag.

# e.g. scangrab booru danbooru touhou
# will fetch everything from the touhou tag. (which is expensive. How many damned pages are there?)

# Past that, you can also specify how many pages, like so:
#   scangrab booru danbooru touhou:50
#   (downloads 50 pages)
# Or how many images you want:
#   scangrab booru danbooru touhou::100
#   (downloads 100 images)
# You can also specify ranges if you want to get specific; like so:
#   scangrab booru danbooru touhou:20-30:10-150
#   (starts at page twenty, downloads till page 30. Skips 10 images, and downloads 140.
# Because of this, it's recommended to avoid passing URLs. They get cut up for parameters, anyways.

# Danbooru also functions differently; it is the only grabber which DOES NOT zip up things.
# By default, things are stored in folders by the tag you grab by, and named according to the image's
# MD5 (as danbooru does.) A file is stored with it that has meta-info from danbooru.

booru_longname="*Booru"
booru_url="Generic"
booru_state=2
booru_filt=1

auto_booru() {
	if [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
		# Danbooru / Safebooru
		return 1
	elif [ "$1" = "danbooru" ]; then
		# Danbooru direct.
		return 1
	elif [ "$1" = "safebooru" ]; then
		# Safebooru direct.
		return 1
	fi

	return 0
}

# Because the syntax is hell.
booru_syntax_help() {
	echo "The booru downloader requires parameters. For reference, this is after the booru name/url:"
	echo "    Usage: booru NAME TAGLIST SPEC [FOLDERNAME]"
	echo "TAGLIST:"
	echo "  Here's some quick help: after the booru name, you should specify some of these:"
	echo "     tagname                   - Specify a tag."
	echo "     tagname1,tagname2,...     - Specify multiple tags."
	echo "     pool:poolname             - Download from a pool."
	echo ""
	echo "  The same syntax as with boorus can be used with tags. For example:"
	echo "     art:tagname               - Search by artist."
	echo "     char:tagname              - Search by character."
	echo "     copy:tagname              - Search by copyright."
	echo "     user:username             - Anything uploaded by username."
	echo "     fav:username              - Anything favorited by username."
	echo "     ordfav:username           - Anything favorited by username (chronological.)"
	echo "     md5:hash                  - Posts with a specific MD5 hash."
	echo "     rating:rate               - Safety rating, e.g. safe, questionable, explicit..."
	echo "     And so on. See http://safebooru.donmai.us/wiki_pages/43049 for a more exhaustive reference."
	echo ""
	echo "  And a quick reference on modifiers:"
	echo "     tag1,-tag2        - Anything matching tag1, but not tag2."
	echo "     ~tag1,~tag2       - Anything marked tag1 or tag2."
	echo "     tag1,tag2         - Anything marked with both tag1 AND tag2."
	echo "     *tag*             - All tags containing 'tag'."
	echo ""
	echo "SPEC:"
	echo "  Syntax:"
	echo "    p10                - Download page 10."
	echo "    l50,p10            - 50 images per page, download page 10. (This is a lowercase L if your font doesn't distinguish.)"
	echo "    i100               - Download 100 images."
	echo "    i5-100             - Download images 5-100."
	echo "    p10-50             - Download from page 10 to 50 inclusive."
	echo "    l50,p10,i100       - Pages with 50 images, save the first 100 images starting at page 10."
	echo "    l50,p5-10,i100     - Pages with 50 images, save the first 100 images starting at page 5."
	echo "                         This syntax is legal, but -10 is ignored."
	echo "    pages:50           - Download 50 pages. Long syntax."
	echo "    images:100         - Download 100 images. Long syntax."
	echo "    limit:50           - 50 images on a page. Long syntax."
	echo "  A minor note is that *technically* limit:N/lN is a tag for booru."
	echo "  It can be specified as a tag, but may screw up download code, so do that here."
	echo ""
	echo "FOLDERNAME (optional):"
	echo "  If not specified, it uses the format 'NAME - TAGLIST'."
	exit 1
}

dl_booru() {
	if [ "$1" = "" ] || [ "$2" = "" ] || [ "$3" = "" ]; then
		booru_syntax_help
	fi

	source=""
	if [ -n "$(echo $1 | grep 'safebooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "safebooru" ]; then
		# Safebooru directly. NEVER MOVE THIS CHECK BENEATH DANBOORU. THE GREP FOR DANBOORU WILL MATCH.
		source="http://safebooru.donmai.us"
		name="safebooru"
	elif [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "danbooru" ]; then
		# Danbooru
		source="http://danbooru.donmai.us"
		name="danbooru"
	else
		# Unknown, but apparently a booru.
		source="$1"
		echo "[Booru] This type of Booru isn't specified as supported."
		echo "[Booru] It may work, or it may completely bomb out."
		echo "[Booru] If it works, please submit a bug report so I can add it to the supported list."
		name="$(echo $1 | sed -e 's|https://||g' -e 's|http://||g' -e 's|/.*||g')"
	fi

	# Example URL transform: danbooru / touhou,highres / l50,10-20
	#   BASE_URL=danbooru.donmai.us/posts?tags=touhou+highres+limit%3A50&page=
	#   START_INDEX=10
	#   END_INDEX=20

	shift

	declare -a tagarray
	declare -a sizearray
	tagarray=($(echo "$1" | tr ',' ' '))
	sizearray=($(echo "$2" | tr ',' ' '))

	declare -a final
	page_index=0
	# URL
	final[0]="$source/posts?utf8=✓&tags="
	# Tags
	final[1]=""
	# Images per page. Blank if no.
	final[2]=""
	# page=
	final[3]="&page="
	# Page Number
	final[4]=1

	# Range to download.
	declare -a pagerange
	pagerange[0]=1
	pagerange[1]=1
	declare -a imagerange

	# Paged mode by default.
	# 1=paged, 2=images
	mode=1

	TAGCOUNT=0
	# Tag array can be pasted together.
	for (( i=0 ; i < ${#tagarray[@]} ; i++ )); do
		if [ ! "$i" = "0" ] && [ ! "$((i + 1))" = "${#tagarray}" ]; then
			final[1]="${final[1]}+"
		fi
		tag="$(echo ${tagarray[i]} | sed -e 's|:|%3A|g')"
		final[1]="${final[1]}${tag}"
		if [[ ! "$tag" == *:* ]]; then
			TAGCOUNT=$((TAGCOUNT + 1))
		fi
	done

	for (( i=0 ; i < ${#sizearray[@]} ; i++ )); do
		if [[ "${sizearray[i]}" == pages:* ]] || [[ "${sizearray[i]}" == p* ]]; then
			pagerange=($(echo "${sizearray[i]}" | tr -d 'p' | tr '-' ' '))
			if [ "${pagerange[1]}" = "" ]; then
				pagerange[1]=${pagerange[0]}
			fi
			final[4]=${pagerange[0]}
		elif [[ "${sizearray[i]}" == limit:* ]] || [[ "${sizearray[i]}" == l* ]]; then
			if [[ ! "${sizearray[i]}" == "limit:*" ]]; then
				final[2]="+$(echo ${sizearray[i]} | sed 's|l|limit:|')"
			else
				final[2]="+${sizearray[i]}"
			fi
			final[2]="$(echo ${final[2]} | sed -e 's|:|%3A|g')"
		elif [[ "${sizearray[i]}" == images:* ]] || [[ "${sizearray[i]}" == i* ]]; then
			imagerange=($(echo "${sizearray[i]}" | tr -d 'i' | tr '-' ' '))
			if [ "${imagerange[1]}" = "" ]; then
				imagerange[1]=${imagerange[0]}
				imagerange[0]=1
			fi
			mode=2
		elif [[ "${sizearray[i]}" == "all" ]]; then
			# 1st page.
			pagerange[0]=1
			# Arbitrarily high number that will never be reached. Maybe. Dunno.
			# Point is, we stop when no images can be extracted from the page.
			pagerange[1]=$MAX_INT

			final[4]=${pagerange[0]}
		fi
	done

	echo "[Booru] Info:"
	echo "[Booru]   Fetching URL $(echo ${final[0]}${final[1]}${final[2]}${final[3]} | tr -d ' ')"
	echo -n "[Booru]   Will download "
	if [ $mode = 1 ]; then
		if [ "${pagerange[0]}" = "${pagerange[1]}" ]; then
			echo "page ${pagerange[0]}."
		else
			TO=${pagerange[1]}
			if [ $TO = $MAX_INT ]; then
				TO="Max"
			fi
			echo "pages ${pagerange[0]} to $TO."
		fi
	else
		# XXX - Why in god's name would a user do this? Dunno, but handle their idiocy anyways.
		if [ "${imagerange[0]}" = "${imagerange[1]}" ]; then
			echo -n "image ${imagerange[0]}"
		else
			echo -n "images ${imagerange[0]} to ${imagerange[1]}"
		fi
		echo ", starting from page ${pagerange[1]}."
	fi

	if [ ! "${final[2]}" = "" ]; then
		echo "[Booru]   Requesting $(echo ${final[2]} | sed 's|+limit%3A||') images per page."
	fi
	if (( $TAGCOUNT >= 2 )); then
		echo "[Booru]   Warning, more than two normal tags. Some boorus don't like this."
	fi


	if [ ! "$3" = "" ]; then
		tagdir="$3"
	else
		tagdir="$name - $1"
	fi

	echo "[Booru]   Output to folder '$tagdir'."

	mkdir -p "$tagdir"
	cd "$tagdir"

	echo -n "[Booru] Fetching pages... "

	if [ $mode = 1 ]; then
		echo -n '' > meta.txt

		for (( range=${pagerange[0]} ; range <= ${pagerange[1]} ; range++ )); do
			final[4]=$range

			spinner "${range} -> ${pagerange[0]}/${pagerange[1]}"

			FETCH_RESULT=1
			FAILS=0
			while [ ! $FETCH_RESULT = 0 ]; do
				fetch "$(echo ${final[@]} | tr -d ' ')" "page${range}.htm"
				
				if [ ! $FETCH_RESULT = 0 ]; then
					FAILS=$((FAILS + 1))
					message=" :<"
					spinner "${range} -> ${pagerange[0]}/${pagerange[1]}"
					if (( $FAILS >= 5 )); then
						echo -e "\n[Booru] Too many failures. Aborting, sorry."
						exit 1
					fi
				fi
			done

			BEFORE=$(wc -l meta.txt | sed 's| .*||g')
			grep '<article id' -A19 "page${range}.htm" | sed -e 's|="|: |g' -e 's|data-||g' -e 's|".*||g' -e 's|^[[:space:]]*||g' -e '/<article id/d' -e '/--/d' >> meta.txt
			rm "page${range}.htm"
			AFTER=$(wc -l meta.txt | sed 's| .*||g')
			
			if (( BEFORE == AFTER )); then
				break
			fi
		done

		spinner_done
	elif [ $mode = 2 ]; then
		echo -n '' > meta-tmp.txt

		quit=0
		for (( range=${pagerange[0]} ; quit == 0 ; range++ )); do
			final[4]=$range

			TO=${pagerange[1]}
			if [ $TO = $MAX_INT ]; then
				TO="Max"
			fi
			spinner "${range} -> ${pagerange[0]}/$TO"

			FETCH_RESULT=1
			FAILS=0
			while [ ! $FETCH_RESULT = 0 ]; do
				fetch "$(echo ${final[@]} | tr -d ' ')" "page${range}.htm"
				
				if [ ! $FETCH_RESULT = 0 ]; then
					FAILS=$((FAILS + 1))
					message=" :<"
					spinner "${range} -> ${pagerange[0]}/${pagerange[1]}"
					if (( $FAILS >= 5 )); then
						echo -e "\n[Booru] Too many failures. Aborting, sorry."
						exit 1
					fi
				fi
			done

			BEFORE=$(wc -l meta-tmp.txt | sed 's| .*||g')

			grep '<article id' -A19 "page${range}.htm" | sed -e 's|="|: |g' -e 's|data-||g' -e 's|".*||g' -e 's|^[[:space:]]*||g' -e '/<article id/d' -e '/--/d' >> meta-tmp.txt
			rm "page${range}.htm"
			
			lines=$(wc -l meta-tmp.txt | sed 's| .*||g')

			if (( BEFORE == lines )); then
				quit=1
			fi
			
			lines=$((lines / 19))

			if (( $lines >= ${imagerange[1]} )); then
				quit=1
			fi
		done

		spinner_done

		echo "[Booru] Downloaded pages, processing..."
		min=${imagerange[0]}
		min=$((min - 1))
		min=$((min * 19))
		max=${imagerange[1]}
		max=$((max - min))
		max=$((max * 19))
		cat meta-tmp.txt | tail -n +${min} | head -n ${max} > meta.txt
	fi

	echo "[Booru] Splitting metadata file... "
	split -l19 -d meta.txt meta

	rm meta.txt

	echo -n "[Booru] Renaming metadata files to match Booru IDs... "
	COUNT=1
	for file in meta*; do
		id="$(cat $file | grep 'id: ' | sed 's|^id: ||g' | head -n1)"
		spinner "$COUNT"
		mv "$file" "meta_${id}.txt" 2>/dev/null
		COUNT=$((COUNT + 1))
	done

	spinner_done

	IMAGES=$COUNT

	THROTTLE=0
	if (( IMAGES > 100 )); then
		echo "[Booru] You're downloading a very large amount of images."
		echo "[Booru] I'm throttling to one fetch attempt every 5s. You don't want to get ip banned, do you?"
		THROTTLE=1
	fi

	echo -n "[Booru] Metadata set up. Downloading images now... "
	COUNT=1
	for file in meta_*.txt; do
		url="$source$(cat $file | grep '^file-url: ' | sed 's|^file-url: ||g' | head -n1)"
		id="$(cat $file | grep 'id: ' | sed 's|^id: ||g' | head -n1)"
		ext="$(cat $file | grep '^file-ext: ' | sed 's|^file-ext: ||g' | head -n1)"
		spinner "$COUNT"
		FETCH_RESULT=1
		while [ ! $FETCH_RESULT = 0 ]; do
			fetch "$url" "img_${id}.${ext}"
			if [ $THROTTLE = 1 ]; then
				spinner "$COUNT zzz"
				sleep 5s
			fi
		done
		COUNT=$((COUNT + 1))
	done

	spinner_done

	# echo "[Booru] Done! Also, if you don't like the metadata, now would be a good time to delete it."
}

scrape_booru() {
	exit 1
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

dynsc_longname="Dynasty Scans"
dynsc_url="http://dynasty-scans.com/"
dynsc_state=1
# No filter
dynsc_filt=0

auto_dynsc() {
	if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Dynasty Scans.
		return 1
	fi

	return 0
}

dl_dynsc() {
	
	# Now loop-de-loop. First, make a decent name. Dynasty always has
	# a short-title at the end of the URL.

	PAGEDATA="$(fetch "$1" "-")"

	folder="$(echo "$PAGEDATA" | grep "<title>" | sed -e 's/<title>Dynasty Reader &raquo; //g' -e 's|</title>||g')"

	mkdir -p "$folder"
	cd "$folder"

	PAGELIST="$(echo "$PAGEDATA" | grep "var pages")"

	# This set of seds cuts up the pagelist in a manner
	# that makes it identical to a bash array.
	# So we're essentially modifying the webpage into a dl-script.
	# Cool, eh?

	PAGETMP="$(echo $PAGELIST | sed -e "s/\"image\"\://g" -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" -e "s/\}\]/\)/g" -e "s/{//g" -e "s/}//g" -e "s/;//g" -e "s/ //g" -e "s/varpages=\[/pages=\(/g" -e "s/,/ /g")"

	# One possible nasty. Spaces.
	# sed -i "s/\%20/ /g" tmp.1

	# Load in the array.
	eval "$PAGETMP"

	echo -n "[DynastyScans] Downloading '$folder' "

	CUR=0

	for image in "${pages[@]}"; do
		fetch "http://dynasty-scans.com$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done
	
	cd ..

	cbz_make "$folder"
}

scrape_dynsc() {
	echo -n "[DynastyScans] Scraping Chapters..."

	# URLS are local.
	fetch "$1" "-" | 			\
	grep 'class="name"' | 		\
	sed -e 's|^.*href="||g' 	\
		-e 's|" class=.*||g' 	\
		-e "s/^[[:space:]]*//" 	\
		-e "s/[[:space:]]*$//" 	\
		-e "s|^|http://dynasty-scans.com|g" >> batch.txt

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

eh_longname="E-Hentai"
eh_url="http://e-hentai.org/"
eh_state=1
eh_filt=0
eh_note="Logging in will result in HQ images and less H@H peer-related issues =_=;"

eh_uselogin=1

login_eh() {
	use_cookies=1

	# Lots of extra shat they use to prevent bots. If it gets updated; let me know.
	
	s_login "UserName" "PassWord" "$1" "$2" "http://forums.e-hentai.org/index.php?act=Login&CODE=01" \
		"CookieDate=1&b=&bt=&referer=http://forums.e-hentai.org/?act=idx"
	user="$(fetch "http://forums.e-hentai.org/?act=idx" "-" | grep 'Logged in as' | sed -e 's|.*showuser=||' -e 's|<.*||' -e 's|.*>||g')"
	if [ ! "$user" = "" ]; then
		echo "[E-H] Logged in as: $user"
		exit 0
	fi

	exit 1
}

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# E-H
		return 1
	fi

	return 0
}

dl_eh() {
	LOGGEDIN=0
	use_cookies=0
	if [ -e "cookiejar" ]; then
		grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
		RES=$?
		if [ $RES = 0 ]; then
			echo "[E-H] We seem to have cookies...using them."
			LOGGEDIN=1
			use_cookies=1
		fi
	fi

	sitepage=$1

	echo "[E-H] Fetching index page..."

	# This is so that we don't get the 'offensive' warning.
	fetch "${sitepage}/?nw=always" tmp.1

	while [ ! $FETCH_RESULT = 0 ]; do
		echo "[E-H] Whoa. Failed to download index. Going again..."
		fetch "${sitepage}/?nw=always" tmp.1
	done

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
	# the last sed deals with names like fate/stay night which are invalid
	folder="$(cat tmp.1 | grep 'title>' | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/ - E-Hentai Galleries//g' | sed 's|/|_|g' | entity_to_char)"
	mkdir -p "$folder"
	cd "$folder"
	
	page=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/' | sed 's/"><img alt.*//g' | grep 'url:' | sed 's/url://g')

	rm ../tmp.1

	echo -n "[E-H] Downloading '$folder' "
	if [ $LOGGEDIN = 1 ]; then
		echo -n "[HQ] "
	else
		echo -n "[LQ] "		
	fi

	doneyet=0

	CDNFAIL=0

	CUR=1
	while [ $doneyet == 0 ]; do
		fetch "$page" "$CUR.htm"
		lq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'keystamp' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
		echo $lq
		hq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
		next_cnt=$((CUR + 1))
		next=$(cat $CUR.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

		extra=""

		if [ "$next" == "" ]; then
			doneyet=1
		fi

		if [ $CDNFAIL -ge 3 ]; then
			FETCH_RESULT=1
		else
			if [ $LOGGEDIN = 0 ]; then
				fetch "$lq" "$(basename $lq)"
			else
				# HQ version; this is the 'Download source resolution' button.
				fetch "$hq"
			fi
		fi

		if [ ! $FETCH_RESULT = 0 ]; then

			message=" :("

			CDNFAIL=$((CDNFAIL + 1))
			if [ $CDNFAIL -ge 3 ]; then
				message=" >:("
			fi

			notload=$(cat $CUR.htm | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")
			
			fetch "$page?nl=$notload" "${CUR}nl.htm"
			lq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'image.php' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
			hq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
			next=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)

			if [ $LOGGEDIN = 0 ]; then
				fetch "$lq"
			else
				fetch "$hq"
			fi

			rm "${CUR}nl.htm"

			if [ ! $FETCH_RESULT = 0 ]; then
				message=" >:["
			else
				rm $CUR.htm
				CUR=$(( CUR + 1 ))
				if [ ! "$next" = "" ]; then
					page="$next"
				fi
			fi
		else
			rm $CUR.htm
			CUR=$(( CUR + 1 ))
			if [ ! "$next" = "" ]; then
				page="$next"
			fi
		fi

		spinner "$CUR"
	done
	
	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	echo -e "[E-H] This isn't supported, considering there's really zero categorization here."
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Preface - code does not discriminate. Plus, if you read manga, let's be honest;
# you've made the mistake of ending up here accidentally once.

fakku_longname="FAKKU"
fakku_url="https://fakku.net/"
fakku_state=1
fakku_filt=0
fakku_note="They've been shuffling things around recently."

auto_fakku() {
	if [ -n "$(echo $1 | grep 'fakku.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
		# Fakku
		return 1
	fi

	return 0
}

dl_fakku() {
	PAGE="$(fetch "$1/read" "-")"

	# First. Escape fixups. Nuke escaped forward slashes
	# Next. Reformat decl.
	# Next. Nuke '.thumb'
	# Next. Nuke commas. Still, they can occur in names so we'll carefully filter.
	# Last transform. '/thumbs/' to '/images/'
	PAGEDATA="$(echo "$PAGE" | grep "window.params.thumbs =" | sed -e 's/\\\//\//g' -e 's/\];/)/g' -e 's/window.params.thumbs = \[/pages=(/g' -e 's/\.thumb//g' -e 's/","/" "/g' -e 's/\/thumbs\//\/images\//g')"

	# Because some pages have unicode escapes (javascript thing ugh) we have to echo the contents of the file
	# to itself to escape them. UGHHHHH

	DATA="$(printf "$PAGEDATA")"

	folder="$(echo "$PAGE" | grep '<title>' | sed -e 's/[[:space:]]*<title>Read //g' -e 's|</title>||g' | entity_to_char)"
	mkdir -p "$folder"
	cd "$folder"

	# Load in the array.
	eval "$DATA"

	# Download loop-de-loop.

	echo -n "[Fakku] Downloading '$folder' "

	CUR=0
	for image in "${pages[@]}"; do
		fetch "https:$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done

	spinner_done

	cd ..
	
	cbz_make "$folder"
}

scrape_fakku() {
	echo -n "[Fakku] Scraping Chapters..."
	
	fetch "$1" scrape.htm

	grep 'class="content-title"' scrape.htm > batch.txtf

	sed -i 's|^.*href="||g' batch.txtf
	sed -i 's|" title=.*||g' batch.txtf
	sed -i "s/^[[:space:]]*//" batch.txtf
	sed -i "s/[[:space:]]*$//" batch.txtf

	# Links are local.
	sed -i "s|^|https://fakku.net|g" batch.txtf

	# Don't bother doing any re-ordering here, since orders are chaotic.
	cat batch.txtf >> batch.txt

	# Clean up.
	rm scrape.htm batch.txtf

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Fakku] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Unlike all of the other plugins, this is a generic downloader for foolslide based sites.
# Therefore, instead of keeping it a generic and needing to specifically ask for this plugin,
# I use a compatibility list.

# Sites using foolslide:
#   vortex-scans.com
#   foolrulez.org

foolsl_longname="FoolSlide"
foolsl_url="Generic"
foolsl_state=1
foolsl_filt=0

auto_foolsl() {
	if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# FoolRulez
		return 1
	elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# VortexScans
		return 1
	fi

	return 0
}

dl_foolsl() {
	# Attempt a lazy download first. Only image scrape if rejected.

	LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`

	echo -n "[FoolSlide] Attempting Lazy Download..."

	FAILED=0

	fetch "$LAZYURL" || export FAILED=1

	if [ $FAILED = 1 ]; then
		echo "Requesting zip failed."
	else
		echo "[OK]"
	fi
}

scrape_foolsl() {

	echo -n "[Foolslide] Scraping Chapters..."

	fetch "$1" "-" |							\
	grep '<div class="title"><a href='			\
	sed -e 's|<div class="title"><a href="||g'	\
	-e 's|" title=.*||g'						\
	-e "s/^[[:space:]]*//"						\
	-e "s/[[:space:]]*$//"					  | \
	reverse_lines >> batch.txt
	
	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Foolslide] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# I need to add an option later for page splitting with imagemagick.
# The whole two-in-one thing is annoying.

mangabox_longname="Mangabox"
mangabox_url="https://www.mangabox.me/"
mangabox_state=1
mangabox_filt=0
mangabox_note="Automatically splits pages with IM. The PC webpage is broken badly. Don't report it."

auto_mangabox() {
	if [ -n "`echo $1 | grep 'mangabox.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# MangaBox
		return 1
	fi

	return 0
}

dl_mangabox() {
	
	echo "[MangaBox] Getting list of pages..."

	fetch "$1" "tmp.htm"

	fullname="$(cat tmp.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e 's|".*||g' | entity_to_char)"

	declare -a list
	list=($(cat tmp.htm | grep 'class="jsNext"' | sed -e 's|.*<li><img src="||g' -e 's|?.*||g'))

	mkdir -p "$fullname"
	cd "$fullname"

	echo -n "[MangaBox] Downloading '$fullname'... "

	NUM=0
	for page in ${list[@]}; do
		NUM=$((NUM + 1))
		VALID=1
		while [ ! $VALID = 0 ]; do
			fetch $page
			verify $(basename $page)
			VALID=$?
		done

		if [ $HAVE_IMAGICK = 1 ]; then
			spinner "Download:$NUM"
			
			$_convert $(basename $page) -crop 2x1@ +repage +adjoin "${NUM}_%d.png"

			spinner "Reformat:$NUM"

			rm $(basename $page)

			spinner "Collat:$NUM"			

			mv ${NUM}_0.png ${NUM}_b.png
			mv ${NUM}_1.png ${NUM}_a.png
		else
			spinner "$NUM"
		fi
	done

	cd ..

	spinner_done

	rm tmp.htm

	cbz_make "$fullname"
}

scrape_mangabox() {

	echo "[MangaBox] Not yet supported, sorry."
	exit 1
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

mpark_longname="MangaPark"
mpark_url="http://mangapark.me/"
# Broken
mpark_state=1
# No filter
mpark_filt=0

auto_mpark() {
	if [ -n "`echo $1 | grep 'mangapark.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Mangapark
		return 1
	fi

	return 0
}

dl_mpark() {

	sitepage="$1"

	# We need a specific type of URL - the 'all on one page' type. Remove specifiers.
	sitepage=`echo $sitepage | sed "s|/1$||g" | sed "s|/3-1$||g" | sed "s|/6-1$||g" | sed "s|/10-1$||g"`
	
	FETCH="$(fetch "$sitepage" "-")"

	folder="$(echo "$FETCH" | grep '<title>' | sed -e 's/<title>//g' -e 's/ Online For Free.*$//g' -e 's/.* - Read //g')"
	mkdir -p "$folder"
	cd "$folder"

	declare -a DATA
	DATA=$(echo "$FETCH" | grep 'target="_blank"' - | sed -e '1d' -e 's|^[[:space:]]*<a.*target="_blank" href=||g' -e "s/ title=.*$//" -e "s/\"//g"| tr '\n' ' ')

	echo -n "[Mangapark] Downloading '$folder' "

	CUR=0
	for image in ${DATA[@]}; do
		fetch "$image"
		spinner "$CUR"
		CUR=$(( CUR + 1 ))
	done
	
	spinner_done

	cd ..

	cbz_make "$folder"
}

scrape_mpark() {
	echo -e "[Mangapark] Scraping Chapters..."

	fetch "$1" scrape.htm

	grep 'class="ch sts"' scrape.htm > batch.txtr

	sed -i 's|^.*href="||g' batch.txtr
	sed -i 's|">.*||g' batch.txtr
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	# URLS are local.
	sed -i "s|^|http://mangapark.com|g" batch.txtr

	# We need a specific type of URL - the 'all on one page' type. Remove specifiers.
	sed -i "s|/1$||g" batch.txtr
	sed -i "s|/3-1$||g" batch.txtr
	sed -i "s|/6-1$||g" batch.txtr
	sed -i "s|/10-1$||g" batch.txtr

	# Lines are reverse order.
	cat batch.txtr | reverse_lines >> batch.txt

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr

	echo -e "[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

mread_longname="Mangareader"
mread_url="http://www.mangareader.net/"
# Broken
mread_state=0
# No filter
mread_filt=0

auto_mread() {
	if [ -n "`echo $1 | grep 'mangareader.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Mangareader
		return 1
	fi

	return 0
}

dl_mread() {

	# Folder edit works different. URLs are consistent with mangareader. e.g aho-girl/1
	# I'm using pipes for clarity.
	folder="$(echo $1 | sed -e "s|http\:\/\/||g" | sed -e "s|www\.mangareader\.net\/||g" | sed -e "s|\/|-|g" | entity_to_char)"
	mkdir -p "$folder"
	cd "$folder"

	echo -n "[Mangareader] Blindly Downloading '$folder' "

	IS_404=0
	CUR=0
	PAGES=0

	while [ $IS_404 = 0 ]; do
		# Increment CUR.
		CUR=$(( CUR + 1 ))
	
		PAGEDATA=$(fetch "$1/$CUR" "$CUR.htm")
		IS_404=$?

		if [ $IS_404 = 0 ]; then
			src=`grep '<div id="imgholder">' $CUR.htm | sed "s/^.*src=\"//g" | sed "s/\" alt=.*//g"`
	
			echo $src

			# Get extension of src.
			EXT="${src##*.}"

			fetch "$src" "$CUR.$EXT"
	
			spinner
		fi
	done

	PAGES=$(( CUR - 1 ))

	spinner_done

	rm *.htm

	cd ..

	cbz_make "$folder"

}

scrape_mread() {
	echo "[MangaReader] NYI, sorry."
}
#!/bin/bash
# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# NiconicoSeiga is odd in that an account is required. Therefore, I had to implement cookiejars.

niconicoseiga_longname="Niconico Seiga"
niconicoseiga_url="http://seiga.nicovideo.jp"
niconicoseiga_state=1
niconicoseiga_filt=0
niconicoseiga_note="Please login first, or make a cookiejar file."

# Need to login.
niconicoseiga_uselogin=1

auto_niconicoseiga() {
	if [ -n "`echo $1 | grep 'seiga.nicovideo.jp' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Niconico Seiga
		return 1
	fi

	return 0
}

login_niconicoseiga() {
	use_cookies=1
	s_login "mail_tel" "password" "$1" "$2" "https://secure.nicovideo.jp/secure/login"
}

dl_niconicoseiga() {
	use_cookies=1

	name="$(basename $1)"

	fetch $1 "$name.htm"

	fullname="$(cat $name.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e 's|".*||g' | entity_to_char | remove_illegal)"

	if [ ! -f "$name.htm" ]; then
		echo "[Niconico] No page found, aborting."
		return 1
	fi

	echo "[Niconico] Extracting image list..."

	cat $name.htm | grep 'data-image-id' | sed -e 's|^[[:space:]]*data-image-id="||g' -e 's|"[[:space:]]*$||g' > $name.lst
	
	if [ "$(cat $name.lst)" = "" ]; then
		echo "[Niconico] No images in page. Did you login?"
		return 1
	fi

	mkdir -p "$fullname"
	cd "$fullname"

	echo -n "[Niconico] Downloading '$fullname' (from $name) "

	COUNT=0
	while read ID; do
		COUNT=$((COUNT + 1))
		fetch "http://lohas.nicoseiga.jp/thumb/${ID}p" "${COUNT}.jpg"
		spinner $COUNT
	done < ../$name.lst

	spinner_done

	cd ..
	rm $name.lst $name.htm
	
	cbz_make "$fullname"
}

scrape_niconicoseiga() {
	COOKIES=1

	echo "[Niconico] Scraping chapters..."

	fetch "$1" "-" | grep 'class="episode"' | sed -e 's|.*<a href="||g' -e 's|?.*||g' -e 's|/watch|http://seiga.nicovideo.jp/watch|g' >> batch.txt

	echo "[Niconico] Done."
}
#!/bin/bash

# Copyright (C) 2015  Jon Feldman/@chaoskagami
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>

# Determine operation.

auto() {
	for module in ${MODS[@]}; do
		auto_${module} "$@"
		RETCHECK=$?
		if [ $RETCHECK = 1 ]; then
			dl_${module} "$@"
			exit 0
		fi
	done
}

batch() {
	# $2 is a file. Read it in line by line as $1 and $2.
	IFS=$'\n' read -d '' -r -a LINES < $1
	NEW=""
	for chunk in "${LINES[@]}"; do
		NEW="$NEW$0 $chunk ;"
	done
	$NEW
}

autobatch() {
	# $2 is a file. Read it in line by line as $1 and $2.
	IFS=$'\n' read -d '' -r -a LINES < $1
	NEW=""
	for chunk in "${LINES[@]}"; do
		NEW="$NEW$0 auto $chunk ;"
	done
	$NEW
}

scrape() {
	for module in ${MODS[@]}; do
		auto_${module} "$@"
		RETCHECK=$?
		if [ $RETCHECK = 1 ]; then
			scrape_${module} "$@"
			exit 0
		fi
	done
}

mod_login() {
	for module in ${MODS[@]}; do
		if [ "$1" = "$module" ]; then
			if [ "$( eval echo \$${module}_uselogin )" = "1" ]; then
				if [ ! "$3" = "" ]; then
					echo "[Warn] Caveat; you probably need to wipe your shell history now. Try not to do it like this."
					username="$2"
					password="$3"
				elif [ ! "$2" = "" ]; then
					username="$2"
					echo -ne "[$module] Password for $username (will not echo): "
					read -s password
				else
					echo -ne "[$module] $username for site: "
					read username
					echo -ne "[$module] Password for $username (will not echo): "
					read -s password
				fi
				login_${module} "$username" "$password"
				exit 0
			else
				echo "$module does not need login."
				exit 0
			fi
		fi
	done
	echo "No such module."
	exit 1
}

help() {
	type 1
	echo -e "Usage:"
	type 0
	echo -ne "     $0     "
	color 3
	echo -ne "OPERATION     "
	color 5
	echo -e "[PARAMS]"
	type 0
	type 1
	echo -e "Operations:"
	type 0
	color 3
	echo -e "     auto (a)"
	type 0
	echo -e "          Chooses module based on URL"
	color 3
	echo -e "     batch (l)"
	type 0
	echo -e "          Takes a file with a list of types and URLs"
	color 3
	echo -e "     autobatch (b)"
	type 0
	echo -e "          Takes a file with URLs which will be run with auto."
	color 3
	echo -e "     scrape (s)"
	type 0
	echo -e "          Will take a manga's page and scrape chapters to"
	echo -e "          a file named batch.txt"
	color 3
	echo -e "     login (u)"
	type 0
	echo -e "          Pass the module name, your username and password."
	echo -e "          This will generate a cookie jar as ./cookiejar"
	echo ""
	echo -e "     You can also specify a module name followed by"
	echo -e "     the URL instead of using the auto-detect."
	echo ""
	echo -e "     If you don't specify an operation and pass only a URL"
	echo -e "     then we assume you want auto (a)."
	type 1
	echo -e "Download Modules:"
	type 0
	for mod in "${MODS[@]}"; do
		longname=$(temp=\$${mod}_longname && eval echo $temp)
		url=$(temp=\$${mod}_url && eval echo $temp)
		broke=$(temp=\$${mod}_state && eval echo $temp)
		filter=$(temp=\$${mod}_filt && eval echo $temp)
		note=$(temp=\$${mod}_note && eval echo $temp)

		echo -ne "     Module Name:                "
		color 3
		echo -e "$mod"

		type 0
		echo -ne "          Long Name:             "
		color 4
		echo -e "$longname"

		type 0
		echo -ne "          Site(s) Used with:     "
		color 5
		echo -e "$url"
		type 0

		type 0
		echo -ne "          Site(s) Current state: "
		if [ "$broke" = "1" ]; then
			color 2
			echo -e "Works"
		elif [ "$broke" = "2" ]; then
			color 3
			echo -e "InDev"
		else
			color 1
			echo -e "Broken"
		fi

		if [ ! "$note" = "" ]; then
			type 0
			echo -e "          Site Note:             $note"
		fi

		type 0
		if [ "$filter" = "1" ]; then
			echo -e "          Supports filters for scrape"
		fi

		echo ""
	done
	type 1
	echo -e "Misc Info"
	type 0
	echo -e "     If you see an emote in the output, it means we had to deal"
	echo -e "     with a retrieval quirk."
	echo -e "\n     [ :/ ]       Given GZip'd data even though we said it wasn't"
	echo -e "                  supported in the GET."
	type 2
	echo -e "                  This happens frequently with batoto when doing"
	echo -e "                  multiple fetches. :/"
	type 0
	echo -e "\n     [ :( ]       Site didn't respond and so the DL failed"
	echo -e "                  We had to revert to a fallback method."

	echo -e "\n     [ >:( ]      Too many normal requests failed; we reverted"
	echo -e "                  to using entirely the fallback method."
	echo ""
	echo -e "     Some modules accept an extra parameter. Usually, this"
	echo -e "     is a filter. Try values like 'English' or 'French'."
	type 1
	echo -e "System Tools"
	type 0
	if [ ! "$_wget" = "" ]; then
		echo -e "     wget:                $_wget"
	fi
	if [ ! "$_curl" = "" ]; then
		echo -e "     curl:                $_curl"
	fi
	if [ ! "$_aria" = "" ]; then
		echo -e "     aria2c:              $_aria"
	fi
	echo -ne "     Will use:            "
	if [ $_FETCHTOOL = 1 ]; then
		echo -ne "wget"
		if [ $_BUSYBOX = 1 ]; then
			echo -ne ", busybox"
		else
			echo -ne ""
		fi
	elif [ $_FETCHTOOL = 2 ]; then
		echo -ne "curl"
	elif [ $_FETCHTOOL = 3 ]; then
		echo -ne "aria2c"
	else
		echo -ne "no tool. Install one of: "
	fi
	echo " (wget, curl, aria2c)"
	echo -ne "     Check broken images: "
	if [ $_CHECK_VALID = 1 ]; then
		if [ -f $_identify ]; then
			echo -ne "imagemagick ($_identify)"
		fi
		echo ""
	else
		echo "no"
	fi
	echo -ne "     Color:               "
	if [ $_COLOR = 1 ]; then
		color 1
		echo -ne "y"
		color 2
		echo -ne "e"
		color 3
		echo -ne "s"
		color 4
		echo -ne " "
		echo -ne "("
		color 5
		echo -ne "d"
		color 6
		echo -ne "u"
		color 1
		echo -ne "m"
		color 2
		echo -ne "b"
		color 3
		echo ")"
		type 0
	elif [ $_COLOR = 2 ]; then
		color 1
		echo -ne "y"
		color 2
		echo -ne "e"
		color 3
		echo -ne "s"
		color 4
		echo -ne " "
		echo -ne "("
		color 5
		echo -ne "t"
		color 6
		echo -ne "p"
		color 1
		echo -ne "u"
		color 2
		echo -ne "t"
		color 3
		echo ")"
		type 0
	else
		echo "no (TERM='$TERM')"
	fi
	type 1
	echo -ne "License / Info"
	type 0
	echo ""
	echo -e "     Copyright (C) 2015     Jon Feldman/@chaoskagami"
	echo ""
	echo -e "     This program is free software: you can redistribute it and/or modify"
	echo -e "     it under the terms of the GNU General Public License as published by"
	echo -e "     the Free Software Foundation, either version 3 of the License, or"
	echo -e "     (at your option) any later version."
	echo ""
	echo -e "     This program is distributed in the hope that it will be useful,"
	echo -e "     but WITHOUT ANY WARRANTY; without even the implied warranty of"
	echo -e "     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the"
	echo -e "     GNU General Public License for more details."
	echo ""
	echo -e "     You should have received a copy of the GNU General Public License"
	echo -e "     along with this program.  If not, see <http://www.gnu.org/licenses/>"
	echo ""
	echo -e "     The latest version of scangrab can always be found at the github"
	echo -e "     page here: https://github.com/chaoskagami/scangrab"
	echo ""
	echo -e "     Build: $branch, $rev"
}

if [ "$1" = "auto" -o "$1" = "a" ]; then
	# Common operation - Automatic Module Select.
	shift
	auto "$@"
elif [ "$1" = "batch" -o "$1" = "l" ]; then
	# Common operation - typed batch.
	shift
	batch "$@"
elif [ "$1" = "autobatch" -o "$1" = "b" ]; then
	# Common operation - auto batch.
	shift
	autobatch "$@"
elif [ "$1" = "scrape" -o "$1" = "s" ]; then
	# Link scraper.
	shift
	scrape "$@"
elif [ "$1" = "login" -o "$1" = "u" ]; then
	# Site login.
	shift
	mod_login "$@"
else
	# Not a common operation - either invalid, a module, or a lazy URL.

	# Detect whether it is a module operation
	MATCH=""
	for module in ${MODS[@]}; do
		if [ "$1" = "$module" ]; then
			shift
			dl_${module} "$@"
			exit 0
		fi
	done

	# Try to see if this was a lazy URL as first parameter.
	auto "$@"

	# All checks failed. Usage~
	help
fi
