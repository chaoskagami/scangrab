#!/bin/bash
o() {
echo "$@"
}
n() {
echo -ne "$@"
}
e() {
echo -e "$@"
}
B="\b\b\b\b\b\b\b\b\b\b"
#!/bin/bash
#############################################
#####@ORIGINAL-FILE 'rev'
rev=85be49a1ca434441680939830d1501771730baed
branch=master
#############################################
#####@ORIGINAL-FILE 'functions/s_login'
#!/bin/bash
#
#
s_login() {
if [ $_FETCHTOOL = 1 ]; then
_CMD="$_FETCH_CMD --post-data='$1=$3&$2=$4&$6' \"$5\"  --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies -O/dev/null"
elif [ $_FETCHTOOL = 2 ]; then
_CMD="$_FETCH_CMD -d '$1=$3&$2=$4&$6' $5 -b $COOKIEJAR -c $COOKIEJAR >/dev/null"
elif [ $_FETCHTOOL = 3 ]; then
#ARIA2C
o "[Warn] aria2c can't post at the moment; this will fail to get required cookies."
_CMD="$_FETCH_CMD $5 --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR -o/dev/null"
fi
if [ $VERBOSE = 1 ]; then
e "\n$_CMD"
return
fi
eval " $_CMD" 2>/dev/null
FETCH_RESULT=$?
}
#############################################
#####@ORIGINAL-FILE 'functions/ss_clear'
#!/bin/bash
#
#
ss_clear() {
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
for (( i=0 ; i < _NUM ; i++ )); do
n " "
done
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
}
#############################################
#####@ORIGINAL-FILE 'functions/fetch'
#!/bin/bash
#
#
fetch() {
if [ $_FETCHTOOL = 1 ]; then
if [ $SAFETY_HACKS = 1 ] || [ "$1" = "-" ]; then
_CMD="$_FETCH_CMD \"$1\""
else
_CMD="$_FETCH_CMD --content-disposition \"$1\""
fi
if [ ! $CLOBBER = 1 ]; then
_CMD="$_CMD -c"
fi
if [ $use_cookies = 1 ]; then
_CMD="$_CMD --load-cookies=$COOKIEJAR"
fi
STDOUT=0
if [ "$2" = "-" ]; then
_CMD="$_CMD -O -"
STDOUT=1
elif [ ! "$2" = "" ]; then
_CMD="$_CMD -O \"$2\""
fi
elif [ $_FETCHTOOL = 2 ]; then
_CMD="$_FETCH_CMD $1"
if [ $use_cookies = 1 ]; then
_CMD="$_CMD -b $COOKIEJAR -c $COOKIEJAR"
fi
if [ "$2" = "" ]; then
_CMD="$_CMD > $(basename "$1")"
elif [ "$2" = "-" ]; then
_CMD="$_CMD"
else
_CMD="$_CMD > \"$2\""
fi
elif [ $_FETCHTOOL = 3 ]; then
_CMD="$_FETCH_CMD $1"
if [ $use_cookies = 1 ]; then
_CMD="$_CMD --load-cookies=$COOKIEJAR"
fi
if [ "$2" = "" ]; then
_CMD="$_CMD -o $(basename "$1")"
elif [ "$2" = "-" ]; then
_CMD="$_CMD -o -"
else
_CMD="$_CMD -o \"$2\""
fi
fi
if [ $VERBOSE = 1 ]; then
e "\n$_CMD"
fi
if [ $CLOBBER = 1 ] && [ $_FETCHTOOL = 1 ] && [ ! $STDOUT = 1 ]; then
mkdir -p "wget.tmp"
cd wget.tmp
fi
eval $_CMD 2>/dev/null
FETCH_RESULT=$?
if [ $CLOBBER = 1 ] && [ $_FETCHTOOL = 1 ] && [ ! $STDOUT = 1 ]; then
mv ./* ../ 2>/dev/null
cd ..
rm -rf wget.tmp
fi
MIME="$(mimetype "$_FILE")"
if [ "$MIME" = "image/jpeg" ] || [ "$MIME" = "image/png" ] || [ "$MIME" = "image/gif" ] || [ "$MIME" = "image/bmp" ]; then
verify "$_FILE"
VALID=$?
if [ ! $VALID = 0 ]; then
o "[WARN] File '$_FILE' is corrupted."
fi
fi
return $FETCH_RESULT
}
#############################################
#####@ORIGINAL-FILE 'functions/type'
#!/bin/bash
#
#
type() {
if [ $_COLOR = 1 ]; then
n "\x1b[$1m"
elif [ $_COLOR = 2 ]; then
if [ $1 = 0 ]; then
tput sgr0
elif [ $1 = 1 ]; then
tput bold
elif [ $1 = 2 ]; then
tput dim
fi
fi
}
#############################################
#####@ORIGINAL-FILE 'functions/ss'
#!/bin/bash
#
#
ss() {
if [ $VERBOSE = 1 ]; then
o "Status: $1 - $message"
return
fi
if [ "$_SC" = "|" ]; then
_SC="/"
elif [ "$_SC" = "/" ]; then
_SC="-"
elif [ "$_SC" = "-" ]; then
_SC="\\"
elif [ "$_SC" = "\\" ]; then
_SC="|"
fi
ss_clear
STR="[$1 $_SC] $message"
_NUM="${#STR}"
n "$STR"
}
#############################################
#####@ORIGINAL-FILE 'functions/mimetype'
#!/bin/bash
#
#
mimetype() {
o "$(file --mime-type "$1" | sed 's/.* //g')"
}
#############################################
#####@ORIGINAL-FILE 'functions/verify'
#!/bin/bash
#
#
verify() {
if [ -f $_identify ]; then
$_identify -verbose -regard-warnings "$1" 2>&1 >/dev/null
_IDRES=$?
return $_IDRES
fi
return 0
}
#############################################
#####@ORIGINAL-FILE 'functions/fetch_no_ops'
#!/bin/bash
#
#
fetch_no_ops() {
if [ $_FETCHTOOL = 1 ]; then
_CMD="wget -q \"$1\" -O \"$2\""
elif [ $_FETCHTOOL = 2 ]; then
_CMD="curl \"$1\" > \"$2\""
elif [ $_FETCHTOOL = 3 ]; then
_CMD="aria2c \"$1\" -o \"$2\""
fi
if [ $VERBOSE = 1 ]; then
e "\n$_CMD"
fi
eval $_CMD 2>/dev/null
FETCH_RESULT=$?
return $FETCH_RESULT
}
#############################################
#####@ORIGINAL-FILE 'functions/bash_get'
#!/bin/bash
#
#
bash_get() {
if [ "$1" = "" ]; then
o "Usage: bget url [output_file]"
return 1
fi
BASE="$(echo -ne $1 | sed -e 's|http://||g' -e 's|https://||g' -e 's|/.*||g')"
GET="$(echo -ne $1 | sed -e 's|http://||g' -e 's|https://||g' -e "s|$BASE||g")"
STDOUT=0
if [ "$2" = "-" ]; then
STDOUT=1
OUT="stdout"
elif [ "$2" = "" ]; then
OUT="$(basename $GET | sed 's|?.*||g')"
GET_L=$((${#GET}-1))
if [ "${GET:$GET_L:1}" = "/" ]; then
OUT="index.htm"
fi
else
OUT="$2"
fi
exec 3<>/dev/tcp/${BASE}/80
printf "GET ${GET} HTTP/1.1\r\nhost: ${BASE}\r\nConnection: close\r\n\r\n" >&3
( cat <&3 | cat > $OUT ) 2>&1 >/dev/null
exec 3>&-
STAT_CODE=0
LINE=0
while read line; do
if [ "$(echo -n $line | tr -d "\r")" = "" ]; then
break
fi
if [ $LINE = 0 ]; then
STAT_CODE="$(echo $line | sed -e 's|HTTP/1.1 ||g' -e 's| .*||g')"
else
o $line | grep "Set-Cookie" 2>&1 >/dev/null
R=$?
if [ $R = 0 ]; then
cookie=$(echo $line | sed 's|Set-Cookie: ||g')
fi
fi
LINE=$((LINE + 1))
done < $OUT
LINE=$((LINE + 1))
sed -i "1,${LINE}d" $OUT
if [ "$STDOUT" = "1" ]; then
cat $OUT
rm $OUT
fi
if [ $STAT_CODE = 200 ]; then
return 0
else
return 1
fi
}
#############################################
#####@ORIGINAL-FILE 'functions/ss_reset'
#!/bin/bash
#
#
ss_reset() {
if [ $VERBOSE = 1 ]; then
o "Status: $1 - $message"
return
fi
ss_clear
_SC="|"
_NUM=0
}
#############################################
#####@ORIGINAL-FILE 'functions/remove_illegal'
#!/bin/bash
#
#
remove_illegal() {
sed \
-e "s/|/-/g" \
-e "s|/|-|g" \
-e "s|?|？|g"
}
#############################################
#####@ORIGINAL-FILE 'functions/color'
#!/bin/bash
#
#
color() {
if [ $_COLOR = 1 ]; then
n "\x1b[3$1m"
elif [ $_COLOR = 2 ]; then
tput setaf $1
fi
}
#############################################
#####@ORIGINAL-FILE 'functions/entity_to_char'
#!/bin/bash
#
#
entity_to_char() {
sed                     \
-e "s/&#32;/ /g"    \
-e "s/&nbsp;/ /g"   \
-e "s/&#33;/\!/g"   \
-e "s/&#34;/\"/g"   \
-e "s/&#35;/\#/g"   \
\
-e "s/&#36;/\$/g"   \
-e "s/&#37;/\%/g"   \
-e "s/&amp;/\&/g"   \
-e "s/&#38;/\&/g"   \
-e "s/&#39;/'/g"    \
\
-e "s/&#40;/\(/g"   \
-e "s/&#41;/\)/g"   \
-e "s/&#42;/\*/g"   \
-e "s/&#43;/\+/g"   \
-e "s/&#44;/\,/g"   \
\
-e "s/&#45;/\-/g"   \
-e "s/&#46;/\./g"   \
-e "s/&#58;/\:/g"   \
-e "s/&#59;/\;/g"   \
-e "s/&lt;/\</g"    \
\
-e "s/&#60;/\</g"   \
-e "s/&gt/\>/g"     \
-e "s/&#61;/\>/g"   \
-e "s/&#63;/\?/g"   \
-e "s/&#64;/\@/g"   \
\
-e "s/&#91;/\[/g"   \
-e "s/&#92;/\\\\/g" \
-e "s/&#93;/\]/g"   \
-e "s/&#94;/\^/g"   \
-e "s/&#95;/\_/g"   \
\
-e "s/&#123;/\{/g"  \
-e "s/&#124;/\|/g"  \
-e "s/&#125;/\}/g"  \
-e "s/&#126;/\~/g"  \
-e "s/&yen;/¥/g"    \
\
-e "s/&#165;/¥/g"   \
-e "s/&sup2;/²/g"   \
-e "s/&#178;/²/g"   \
-e "s/&sup3;/³/g"   \
-e "s/&#179;/³/g"   \
\
-e "s/&frac14;/¼/g" \
-e "s/&#188;/¼/g"   \
-e "s/&frac12;/½/g" \
-e "s/&#189;/½/g"   \
-e "s/&frac34;/¾/g" \
\
-e "s/&#190;/¾/g"   \
-e "s/&spades;/♠/g" \
-e "s/&#9824;/♠/g"  \
-e "s/&clubs;/♣/g"  \
-e "s/&#9827;/♣/g"  \
\
-e "s/&hearts;/♥/g" \
-e "s/&#9829;/♥/g"  \
-e "s/&diams;/♦/g"  \
-e "s/&#9830;/♦/g"
}
#############################################
#####@ORIGINAL-FILE 'functions/reverse_lines'
#!/bin/bash
#
#
reverse_lines() {
readarray -t LINES
for (( I = ${#LINES[@]}; I; )); do
o "${LINES[--I]}"
done
}
#############################################
#####@ORIGINAL-FILE 'functions/ss_done'
#!/bin/bash
#
#
ss_done() {
ss_reset
_MESG="OK"
if [ ! "$1" = "" ]; then
_MESG="$1"
fi
e "[$_MESG]"
}
#############################################
#####@ORIGINAL-FILE 'functions/is_done'
#!/bin/bash
#
#
is_done() {
if [ -e "${1}.cbz" ]; then
return 1
fi
return 0
}
#############################################
#####@ORIGINAL-FILE 'functions/cbz_make'
#!/bin/bash
#
#
cbz_make() {
e "[Post] Making CBZ..."
if [ ! -d "$1" ]; then
e "\n[Error] Not a folder. Something went wrong."
exit 1
fi
if [ "$(ls "$1")" = "" ]; then
o "[Error] No files? Download failed."
exit 1
fi
zip -r "$1.zip" "$1" > /dev/null 2>&1
mv "$1.zip" "$1.cbz" > /dev/null 2>&1
e "[Post] Cleanup..."
rm -rf "$1"
e "[Post] Finished!"
}
#############################################
#####@ORIGINAL-FILE 'operations/batch'
#!/bin/bash
#
#
batch() {
while read chunk; do
eval $chunk
done < $1
}
#############################################
#####@ORIGINAL-FILE 'operations/scrape'
#!/bin/bash
#
#
scrape() {
for module in ${MODS[@]}; do
a_${module} "$@"
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
s_${module} "$@"
fi
done
}
#############################################
#####@ORIGINAL-FILE 'operations/help'
#!/bin/bash
#
#
help() {
type 1
e "Usage:"
type 0
n "     $0     "
color 3
n "OPERATION     "
color 5
e "[PARAMS]"
type 0
type 1
e "Operations:"
type 0
color 3
e "     auto (a)"
type 0
e "          Chooses module based on URL"
color 3
e "     batch (l)"
type 0
e "          Takes a file with a list of types and URLs"
color 3
e "     autobatch (b)"
type 0
e "          Takes a file with URLs which will be run with auto."
color 3
e "     scrape (s)"
type 0
e "          Will take a manga's page and scrape chapters to"
e "          a file named batch.txt"
color 3
e "     login (u)"
type 0
e "          Pass the module name, your username and password."
e "          This will generate a cookie jar as ./cookiejar"
color 3
e "     upgrade"
type 0
e "          Checks github for a newer copy of scangrab and"
e "          updates itself. Make sure you have write permission."
e "          There is no short form of this action."
o ""
e "     You can also specify a module name followed by"
e "     the URL instead of using the auto-detect."
o ""
e "     If you don't specify an operation and pass only a URL"
e "     then we assume you want auto (a)."
type 1
e "Download Modules:"
type 0
for mod in "${MODS[@]}"; do
longname=$(temp=\$${mod}_l && eval echo $temp)
url=$(temp=\$${mod}_url && eval echo $temp)
broke=$(temp=\$${mod}_state && eval echo $temp)
filter=$(temp=\$${mod}_filt && eval echo $temp)
note=$(temp=\$${mod}_note && eval echo $temp)
n "     Module Name:                "
color 3
e "$mod"
type 0
n "          Long Name:             "
color 4
e "$longname"
type 0
n "          Site(s) Used with:     "
color 5
e "$url"
type 0
type 0
n "          Site(s) Current state: "
if [ "$broke" = "1" ]; then
color 2
e "Works"
elif [ "$broke" = "2" ]; then
color 3
e "InDev"
else
color 1
e "Broken"
fi
if [ ! "$note" = "" ]; then
type 0
e "          Site Note:             $note"
fi
type 0
if [ "$filter" = "1" ]; then
e "          Supports filters for scrape"
fi
o ""
done
type 1
e "Misc Info"
type 0
e "     If you see an emote in the output, it means we had to deal"
e "     with a retrieval quirk."
e "\n     [ :/ ]       Given GZip'd data even though we said it wasn't"
e "                  supported in the GET."
type 2
e "                  This happens frequently with batoto when doing"
e "                  multiple fetches. :/"
type 0
e "\n     [ :( ]       Site didn't respond and so the DL failed"
e "                  We had to revert to a fallback method."
e "\n     [ >:( ]      Too many normal requests failed; we reverted"
e "                  to using entirely the fallback method."
o ""
e "     Some modules accept an extra parameter. Usually, this"
e "     is a filter. Try values like 'English' or 'French'."
type 1
e "System Tools"
type 0
if [ ! "$_wget" = "" ]; then
e "     wget:                $_wget"
fi
if [ ! "$_curl" = "" ]; then
e "     curl:                $_curl"
fi
if [ ! "$_aria" = "" ]; then
e "     aria2c:              $_aria"
fi
n "     Will use:            "
if [ $_FETCHTOOL = 1 ]; then
n "wget"
if [ $_BUSYBOX = 1 ]; then
n ", busybox"
else
n ""
fi
elif [ $_FETCHTOOL = 2 ]; then
n "curl"
elif [ $_FETCHTOOL = 3 ]; then
n "aria2c"
else
n "no tool. Install one of: "
fi
o " (wget, curl, aria2c)"
n "     Check broken images: "
if [ $_CHECK_VALID = 1 ]; then
if [ -f $_identify ]; then
n "imagemagick ($_identify)"
fi
o ""
else
o "no"
fi
n "     Color:               "
if [ $_COLOR = 1 ]; then
color 1
n "y"
color 2
n "e"
color 3
n "s"
color 4
n " "
n "("
color 5
n "d"
color 6
n "u"
color 1
n "m"
color 2
n "b"
color 3
o ")"
type 0
elif [ $_COLOR = 2 ]; then
color 1
n "y"
color 2
n "e"
color 3
n "s"
color 4
n " "
n "("
color 5
n "t"
color 6
n "p"
color 1
n "u"
color 2
n "t"
color 3
o ")"
type 0
else
o "no (TERM='$TERM')"
fi
type 1
n "License / Info"
type 0
o ""
e "     Copyright (C) 2015     Jon Feldman/@chaoskagami"
o ""
e "     This program is free software: you can redistribute it and/or modify"
e "     it under the terms of the GNU General Public License as published by"
e "     the Free Software Foundation, either version 3 of the License, or"
e "     (at your option) any later version."
o ""
e "     This program is distributed in the hope that it will be useful,"
e "     but WITHOUT ANY WARRANTY; without even the implied warranty of"
e "     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the"
e "     GNU General Public License for more details."
o ""
e "     You should have received a copy of the GNU General Public License"
e "     along with this program.  If not, see <http://www.gnu.org/licenses/>"
o ""
e "     The latest version of scangrab can always be found at the github"
e "     page here: https://github.com/chaoskagami/scangrab"
o ""
e "     Build: $type, $branch, $rev"
}
#############################################
#####@ORIGINAL-FILE 'operations/auto'
#!/bin/bash
#
#
auto() {
for module in ${MODS[@]}; do
a_${module} "$@"
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
dl_${module} "$@"
return 0
fi
done
return 1
}
#############################################
#####@ORIGINAL-FILE 'operations/configure_env'
#!/bin/bash
#
#
COOKIEJAR="$(pwd)/cookiejar"
HAVE_IMAGICK=0
FETCH_RESULT=0
if [ "$VERBOSE" = "" ]; then
VERBOSE=0
fi
message=""
use_cookies=0
SAFETY_HACKS=0
CLOBBER=0
_COLOR=0
_FETCH_CMD=""
_CHECK_VALID=0
_FETCHTOOL=0
_BUSYBOX=0
_SC="|"
_NUM=0
_tput="$(which tput 2>/dev/null)"
_identify="$(which identify 2>/dev/null)"
_convert="$(which convert 2>/dev/null)"
_wget="$(which wget 2>/dev/null)"
_curl="$(which curl 2>/dev/null)"
_aria="$(which aria2c 2>/dev/null)"
for ((i = 1, n = 2;; n = 1 << ++i)); do
if [[ ${n:0:1} == '-' ]]; then
MAX_INT=$(((1 << i) - 1))
break
fi
done
type="min"
if [ "$TERM" = "xterm" ] || [ "$TERM" = "linux" ] || [ "$TERM" = "screen" ]; then
_COLOR=1
if [ -f $_tput ]; then
_COLOR=2
fi
fi
if [ -f "$_identify" ]; then
_CHECK_VALID=1
fi
if [ -f "$_identify" ]; then
HAVE_IMAGICK=1
fi
if [ ! "$_wget" = "" ]; then
common_opts=" --quiet --no-cache --user-agent=\"Mozilla/5.0\" -t 1 -T 10 --random-wait "
if [ ! "$($_wget --help 2>&1 | grep busybox)" = "" ]; then
o "[Warning] Your system wget is busybox, which can't actually do some things like reject cache and retry."
common_opts=" -q -U \"Mozilla/5.0\""
_BUSYBOX=1
fi
_FETCH_CMD="$_wget $common_opts"
_FETCHTOOL=1
else
if [ ! "$_curl" = "" ]; then
_FETCH_CMD=$_curl
_FETCHTOOL=2
else
if [ ! "$_aria" = "" ]; then
_FETCH_CMD=$_aria
_FETCHTOOL=3
fi
fi
fi
#############################################
#####@ORIGINAL-FILE 'operations/upgrade_self'
#!/bin/bash
#
#
upgrade_self() {
if [ "$type" = "repo" ]; then
o "[Upgrade] You're in a git repo. Use 'git pull' instead."
exit 0
fi
URL="https://raw.githubusercontent.com/chaoskagami/scangrab/$branch/dist/scangrab.$type"
LOG="https://raw.githubusercontent.com/chaoskagami/scangrab/$branch/CHANGES-SNAP"
o "[Upgrade] Checking this scangrab's sha256sum..."
fetch_no_ops "${URL}.sha256sum" "${0}.new.sha256sum"
if [ ! $FETCH_RESULT = 0 ]; then
o "[Upgrade] Error fetching sha256. Aborting."
fi
this_sha256="$( cat "${0}" | sha256sum )"
gith_sha256="$( cat "${0}.new.sha256sum" )"
rm "${0}.new.sha256sum"
if [ "$this_sha256" = "$gith_sha256" ]; then
o "[Upgrade] Not required. Same sha256 as upstream."
exit 0
else
o "[Upgrade] Doesn't match upstream. Fetching..."
fetch_no_ops "$URL" "${0}.new"
if [ ! $FETCH_RESULT = 0 ]; then
o "[Upgrade] Fetch failed. Error code: $R. Do you have write permission?"
rm "${0}.new" "${0}.new.sha256sum" 2>/dev/null
exit $R
fi
o "[Upgrade] Downloaded replacement. Checking replacement's sha256sum..."
new_sha256="$( cat "${0}.new" | sha256sum )"
if [ "$new_sha256" = "$gith_sha256" ]; then
o "[Upgrade] Matched. Replacing self..."
cp -f "${0}.new" "${0}"
R=$?
if [ ! $R = 0 ]; then
o "[Upgrade] Couldn't replace. cp error: $R. Abort."
rm "${0}.new" "${0}.new.sha256sum" 2>/dev/null
exit $R
fi
o "[Upgrade] Sync to avoid race conditions..."
sync
o "[Upgrade] Checking to make sure it has been replaced..."
rm "${0}.new" "${0}.new.sha256" 2>/dev/null
new_sha256="$(cat "${0}" | sha256sum )"
if [ ! "$new_sha256" = "$gith_sha256" ]; then
o "[Upgrade] Sanity check failed. This is not normal."
exit 1
fi
o "[Upgrade] Succeeded. Displaying changelog, then exiting..."
o "--- Changelog ----------------------"
fetch_no_ops "$LOG" "/tmp/scangrab-changelog-$(echo ${new_sha256} | sed 's/ .*//g')"
if [ ! $FETCH_RESULT = 0 ]; then
o "[Upgrade] Failed to fetch changelog."
exit 0
fi
cat "/tmp/scangrab-changelog-$(echo ${new_sha256} | sed 's/ .*//g')" | sed -n '/== START ==/,/== END ==/p' | head -n-1 | tail -n+2
rm "/tmp/scangrab-changelog-$(echo ${new_sha256} | sed 's/ .*//g')" 2>/dev/null
exit 0
else
o "[Upgrade] Failed. sha256 does not match."
fi
fi
exit 1
}
#############################################
#####@ORIGINAL-FILE 'operations/mod_login'
#!/bin/bash
#
#
mod_login() {
for module in ${MODS[@]}; do
if [ "$1" = "$module" ]; then
if [ "$( eval echo \$${module}_uselogin )" = "1" ]; then
if [ ! "$3" = "" ]; then
o "[Warn] Caveat; you probably need to wipe your shell history now. Try not to do it like this."
username="$2"
password="$3"
elif [ ! "$2" = "" ]; then
username="$2"
n "[$module] Password for $username (will not echo): "
read -s password
else
n "[$module] Username: "
read username
n "[$module] Password for $username (will not echo): "
read -s password
fi
login_${module} "$username" "$password"
exit 0
else
o "$module does not need login."
exit 0
fi
fi
done
o "No such module."
exit 1
}
#############################################
#####@ORIGINAL-FILE 'operations/autobatch'
#!/bin/bash
#
#
autobatch() {
while read chunk; do
eval auto $chunk
done < $1
}
#############################################
#####@AUTOGENERATED 'MODS'
MODS=(mangabox batoto niconicoseiga eh fakku dynsc foolsl mpark booru)
#############################################
#############################################
#####@ORIGINAL-FILE 'modules/mangabox'
#!/bin/bash
#
#
#
mangabox_l="Mangabox"
mangabox_url="www.mangabox.me"
mangabox_state=1
mangabox_filt=0
mangabox_note="Automatically splits pages with IM. The PC webpage is broken badly. Don't report it."
a_mangabox() {
if [ -n "`echo $1 | grep 'mangabox.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
dl_mangabox() {
o "[MangaBox] Getting list of pages..."
fetch "$1" "tmp.htm"
fullname="$(cat tmp.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char)"
is_done "$fullname"
R=$?
if [ $R = 1 ]; then
o "[MangaBox] Already downloaded. Skipping."
return 0
fi
declare -a list
list=($(cat tmp.htm | grep 'class="jsNext"' | sed -e 's|.*<li><img src="||g' -e 's|?.*||g'))
mkdir -p "$fullname"
cd "$fullname"
o -n "[MangaBox] Downloading '$fullname'... "
NUM=0
for page in ${list[@]}; do
NUM=$((NUM + 1))
VALID=1
while [ ! $VALID = 0 ]; do
fetch $page
verify $(basename $page)
VALID=$?
done
if [ $HAVE_IMAGICK = 1 ]; then
ss "Download:$NUM"
$_convert $(basename $page) -crop 2x1@ +repage +adjoin "${NUM}_%d.png"
ss "Reformat:$NUM"
rm $(basename $page)
ss "Collat:$NUM"
mv ${NUM}_0.png ${NUM}_b.png
mv ${NUM}_1.png ${NUM}_a.png
else
ss "$NUM"
fi
done
cd ..
ss_done
rm tmp.htm
cbz_make "$fullname"
}
s_mangabox() {
o "[MangaBox] Not yet supported, sorry."
exit 1
}
#############################################
#####@ORIGINAL-FILE 'modules/booru.mods/danbooru'
#!/bin/bash
#
#
#
booru_danbooru_init() {
final[0]="$source/posts.json?tags="
}
booru_danbooru_page() {
final[4]=$1
if [ $RESUME = 1 ]; then
return 1
fi
if [ $is_pool = 0 ]; then
dl_booru_page "$(echo ${final[@]} | tr -d ' ')" "page${range}.json"
BEFORE=$(wc -l meta.txt | sed 's| .*||g')
cat "page${range}.json" | tr ',' "\n" | grep '"id"' | sed 's|.*:||g' >> meta.txt
AFTER=$(wc -l meta.txt | sed 's| .*||g')
if [ "$BEFORE" = '' ]; then
BEFORE=0
fi
PAGE_SIZE=$((AFTER - BEFORE))
sed -i "s|},{|}]\n[{|g" "page${range}.json"
o '' >> "page${range}.json"
LEN=0
while read line; do
FIRST=$((PAGE_SIZE - LEN))
id="$(cat meta.txt | tail -n $FIRST | head -n 1)"
o "$line" > meta/meta_${id}.json
LEN=$((LEN + 1))
done < "page${range}.json"
rm "page${range}.json"
if (( BEFORE == AFTER )); then
return 1
fi
return 0
else
final[0]="${source}/pools.json?commit=Search&search[order]=updated_at&search[name_matches]="
dl_booru_page "$(echo ${final[@]} | tr -d ' ')" "${final[1]}.json"
cat "${final[1]}.json" | tr ',' "\n" | grep '"post_ids"' | sed -e 's|.*:||g' -e 's|"||g' | tr ' ' "\n" >> meta.txt
fi
}
booru_danbooru_meta() {
declare -a base
base[0]="$source/posts/"
base[1]="${1}.json"
if [ ! -e "meta_${base[1]}" ]; then
message="fetch ${1}"
ss "${DONE} -> ${LINES}"
message=""
dl_booru_page "$(echo ${base[@]} | tr -d ' ')" "meta_${base[1]}"
fi
}
booru_danbooru_content() {
declare -a base
base[0]="$source/posts/"
base[1]="${1}.json"
url_img="$(cat "${META_DIR}/meta_${base[1]}" | tr ',' "\n" | grep '"file_url"' | sed -e 's|.*:||g' -e "s|\"||g")"
file_ext="$(cat "${META_DIR}/meta_${base[1]}" | tr ',' "\n" | grep '"file_ext"' | sed 's|.*:||g')"
if [ "$file_ext" = "" ] || [ "$url_img" = "" ]; then
ss_done
o "[Booru] ID:${1} appears to be deleted/hidden. Skipping."
return
fi
if [ ! -e "image_${1}.${file_ext}" ]; then
dl_booru_page "${source}${url_img}" "image_${1}.${file_ext}"
fi
}
#############################################
#####@ORIGINAL-FILE 'modules/batoto'
#!/bin/bash
#
#
#
batoto_l="Batoto"
batoto_url="bato.to"
batoto_state=1
batoto_filt=1
batoto_note="Their page template is highly unstable."
a_batoto() {
if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
dl_batoto() {
folder="$(fetch "$1" "-" | grep -C0 "<title>" | sed -e "s/^[[:space:]]*<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e "s/[[:space:]]*$/\n/" | entity_to_char)"
is_done "$folder"
R=$?
if [ $R = 1 ]; then
o "[Batoto] Already downloaded. Skipping."
return 0
fi
mkdir -p "$folder"
cd "$folder"
CUR=0
PAGES=0
RET=0
base="$1"
if [ ! "${base:${#base}-1}" = "/" ]; then
base="${base}/"
fi
o -n "[Batoto] Downloading '$folder' "
while [ "$RET" = "0" ]; do
CUR=$(( CUR + 1 ))
fetch "${base}${CUR}" "$CUR.htm"
if [ "$(mimetype $CUR.htm)" = "application/x-gzip" ]; then
mv $CUR.htm $CUR.htm.gz
gunzip $CUR.htm.gz
message=" :/"
fi
img="$(grep -C0 'z-index: 1003' $CUR.htm | sed -e 's/^[[:space:]]*<img src="//g' -e 's/".*$//g')"
ext="${img##*.}"
fetch "$img" "${CUR}_${folder}.${ext}"
RET=$?
rm $CUR.htm
ss "$CUR"
done
PAGES=$(( CUR - 1 ))
ss_done
cd ..
cbz_make "$folder"
}
s_batoto() {
notice_batoto
o -n "[Batoto] Scraping Chapters..."
fetch "$1" scrape.htm
if [ "$(mimetype scrape.htm)" = "application/x-gzip" ]; then
mv $CUR.htm $CUR.htm.gz
gunzip $CUR.htm.gz
message=" :/"
fi
grep -A 2 'Sort:' scrape.htm >> batch.txtr
sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr
sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr
sed -i "s|<a href=\"||g" batch.txtr
sed -i "s|\" title=.*||g" batch.txtr
sed -i '/^[[:space:]]*$/d' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i 's|^--$||g' batch.txtr
sed -i '/^$/d' batch.txtr
cat batch.txtr | reverse_lines > batch.txtf
if [ "$2" = "" ]; then
sed -ni '0~2p' batch.txtf
cat batch.txtf >> batch.txt
else
n "$B$B\b Applying Language Filter '$2'..."
grep -A 1 "$2" batch.txtf > batch.txtf2
sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2
sed -i '/^$/d' batch.txtf2
sed -ni '0~2p' batch.txtf2
cat batch.txtf2 >> batch.txt
fi
rm scrape.htm batch.txtr batch.txtf*
n "$B$B$B\b"
for ((n=0;n < ${#2}; n++)); do
n '\b'
done
e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/niconicoseiga'
#!/bin/bash
#
#
#
niconicoseiga_l="Niconico Seiga"
niconicoseiga_url="seiga.nicovideo.jp"
niconicoseiga_state=1
niconicoseiga_filt=0
niconicoseiga_note="Please login first."
niconicoseiga_uselogin=1
a_niconicoseiga() {
if [ -n "`echo $1 | grep 'seiga.nicovideo.jp' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
login_niconicoseiga() {
use_cookies=1
s_login "mail_tel" "password" "$1" "$2" "https://secure.nicovideo.jp/secure/login"
}
dl_niconicoseiga() {
use_cookies=1
name="$(basename $1)"
fetch $1 "$name.htm"
fullname="$(cat $name.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char | remove_illegal)"
if [ ! -f "$name.htm" ]; then
o "[Niconico] No page found, aborting."
return 1
fi
o "[Niconico] Extracting image list..."
cat $name.htm | grep 'data-image-id' | sed -e 's|^[[:space:]]*data-image-id="||g' -e 's|"[[:space:]]*$||g' > $name.lst
if [ "$(cat $name.lst)" = "" ]; then
o "[Niconico] No images in page. Did you login?"
return 1
fi
is_done "$folder"
R=$?
if [ $R = 1 ]; then
o "[Niconico] Already downloaded. Skipping."
return 0
fi
mkdir -p "$fullname"
cd "$fullname"
o -n "[Niconico] Downloading '$fullname' (from $name) "
COUNT=0
while read ID; do
COUNT=$((COUNT + 1))
fetch "http://lohas.nicoseiga.jp/thumb/${ID}p" "${COUNT}.jpg"
ss $COUNT
done < ../$name.lst
ss_done
cd ..
rm $name.lst $name.htm
cbz_make "$fullname"
}
s_niconicoseiga() {
COOKIES=1
o "[Niconico] Scraping chapters..."
fetch "$1" "-" | grep 'class="episode"' | sed -e 's|.*<a href="||g' -e 's|?.*||g' -e 's|/watch|http://seiga.nicovideo.jp/watch|g' >> batch.txt
o "[Niconico] Done."
}
#############################################
#####@ORIGINAL-FILE 'modules/eh'
#!/bin/bash
#
#
#
eh_l="E-H / ExH"
eh_url="g.e-hentai.org / exhentai.org"
eh_state=1
eh_filt=0
eh_note="Logging in will result in HQ images and less H@H peer-related issues =_=;"
eh_uselogin=1
login_eh() {
use_cookies=1
s_login "UserName" "PassWord" "$1" "$2" "http://forums.e-hentai.org/index.php?act=Login&CODE=01" \
"CookieDate=1&b=&bt=&referer=http://forums.e-hentai.org/?act=idx"
user="$(fetch "http://forums.e-hentai.org/?act=idx" "-" | grep 'Logged in as' | sed -e 's|.*showuser=||' -e 's|<.*||' -e 's|.*>||g')"
if [ ! "$user" = "" ]; then
e "\n[E-H] Logged in as: $user"
exit 0
fi
exit 1
}
limitcheck_eh() {
use_cookies=1
line="$(fetch "http://g.e-hentai.org/home.php" "-" | grep "<p>You are currently at")"
LIMIT_AT="$(echo $line | sed -e 's|.*<p>You are currently at <strong>||g' -e 's|</strong>.*||g')"
LIMIT_OF="$(echo $line | sed -e 's|.*</strong> towards a limit of <strong>||g' -e 's|</strong>.*||g')"
LIMIT_REGEN="$(echo $line | sed -e 's|.*</strong>. This regenerates at a rate of <strong>||g' -e 's|</strong>.*||g')"
}
a_eh() {
if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
check_eh() {
if [ "$check_eh_done" = "1" ]; then
return 0
fi
LOGGEDIN=0
use_cookies=0
if [ -e "cookiejar" ]; then
if [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
n "[E-H] Checking for Ex cookies..."
grep 'exhentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
RES=$?
if [ $RES = 0 ]; then
o "yes"
use_cookies=1
n "[E-H] Checking for access..."
fetch "exhentai.org" "exchk"
cat exchk | grep "The X Makes It Sound Cool" >/dev/null 2>&1
RES=$?
if [ ! $RES = 0 ]; then
o -n "no, "
if [ "$(sha256sum exchk | sed 's| .*||g')" = "a279e4ccd74cffbf20baa41459a17916333c5dd55d23a518e7f10ae1c288644f" ]; then
o "panda."
else
o "not cool."
fi
exit 1
else
LOGGEDIN=1
o "yes"
fi
else
o "no"
n "[E-H] Checking for E-H cookies..."
grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
RES=$?
if [ $RES = 0 ]; then
o "yes, copying them"
grep 'e-hentai.org' $COOKIEJAR >> cookiejar.edit
sed 's|e-hentai.org|exhentai.org|g' cookiejar.edit >> cookiejar
rm cookiejar.edit
n "[E-H] Fixed cookies. Checking for access..."
use_cookies=1
fetch "exhentai.org" "-" | grep "The X Makes It Sound Cool" >/dev/null 2>&1
RES=$?
if [ ! $RES = 0 ]; then
o -n "no, "
if [ "$(sha256sum exchk | sed 's| .*||g')" = "a279e4ccd74cffbf20baa41459a17916333c5dd55d23a518e7f10ae1c288644f" ]; then
o "panda."
else
o "not cool."
fi
exit 1
else
LOGGEDIN=1
o "yes"
fi
else
o "no."
o "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
exit 1
fi
fi
else
grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
RES=$?
if [ $RES = 0 ]; then
o "[E-H] We seem to have cookies...using them."
LOGGEDIN=1
use_cookies=1
fi
fi
elif [ -n "`echo $1 | grep 'exhentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
o "[E-H] No cookies, and you're attempting to fetch an Ex link. Abort."
exit 1
fi
if [ $LOGGEDIN = 0 ]; then
o "[E-H] Not logged in. Expect random failures at anoymous quota cap."
fi
check_eh_done=1
return 0
}
dl_eh_wait() {
limitcheck_eh
LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))
if (( $LIMIT_LEFT < 100 )); then
message=":#"
zzz=$(( LIMIT_REGEN * 60 ))
while [ ! $zzz = 0 ]; do
ss "Over, wait ${zzz}s"
sleep 1s
zzz=$(( zzz - 1 ))
done
limitcheck_eh
LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))
fi
}
dl_eh() {
sitepage=$1
check_eh "$1"
if [ $LOGGEDIN = 1 ]; then
limitcheck_eh
LIMIT_LEFT=$(( LIMIT_OF - LIMIT_AT ))
THIS_COUNT=$MAXPAGES
if [ $LOGGEDIN = 1 ]; then
THIS_COUNT=$(( MAXPAGES * 100 ))
fi
o "[E-H] Limit check: ${LIMIT_AT} / ${LIMIT_OF}, +${LIMIT_REGEN}/min"
fi
o "[E-H] Fetching index page..."
[ -e 'tmp.1' ] && rm tmp.1
fetch "${sitepage}/?nw=always" tmp.1
while [ ! $FETCH_RESULT = 0 ]; do
fetch "${sitepage}/?nw=always" tmp.1
done
folder="$(cat tmp.1 | grep 'title>' | sed -e 's/<title>//g'  -e 's/<\/title>//g' -e 's/ - E-Hentai Galleries//g' -e 's/ - ExHentai.org//g' -e 's|/|_|g' | entity_to_char | remove_illegal)"
is_done "$folder"
R=$?
if [ $R = 1 ]; then
o "[E-H] Already downloaded. Skipping."
rm tmp.1
return 0
fi
mkdir -p "$folder"
cd "$folder"
page=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/' | sed 's/"><img alt.*//g' | grep 'url:' | sed 's/url://g')
MAXPAGES=$(cat ../tmp.1 | grep 'Length:' | sed -e 's|.*Length:</td><td class="gdt2">||g' -e 's| pages.*||g')
rm ../tmp.1
o "[E-H] Downloading '$folder'... "
doneyet=0
CDNFAIL=0
CUR=1
while [ $doneyet = 0 ]; do
dl_eh_wait
fetch "$page" "$CUR.htm"
while [ ! $FETCH_RESULT = 0 ]; do
message=":O"
ss "RETR $CUR / $MAXPAGES"
dl_eh_wait
fetch "$page" "$CUR.htm"
done
lq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'keystamp' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
hq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
next_cnt=$((CUR + 1))
next=$(cat $CUR.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)
extra=""
ss "IMG $CUR / $MAXPAGES"
if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
dl_eh_wait
fetch "$lq" "$(basename $lq)"
else
dl_eh_wait
fetch "$hq"
fi
if [ ! $FETCH_RESULT = 0 ]; then
message=" Alt"
while [ ! $FETCH_RESULT = 0 ]; do
CLOBBER=1
dl_eh_wait
fetch "$page" "${CUR}.htm"
notload=$(cat ${CUR}.htm | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")
dl_eh_wait
ss "FAL $CUR / $MAXPAGES"
fetch "$page?nl=$notload" "${CUR}nl.htm"
CLOBBER=0
done
lq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'image.php' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
hq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
next=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)
ss "IMG $CUR / $MAXPAGES"
if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
dl_eh_wait
fetch "$lq"
else
dl_eh_wait
fetch "$hq"
fi
rm "${CUR}nl.htm"
if [ $FETCH_RESULT = 0 ]; then
CUR=$(( CUR + 1 ))
if [ ! "$next" = "" ]; then
page="$next"
fi
fi
else
rm $CUR.htm
CUR=$(( CUR + 1 ))
if [ ! "$next" = "" ]; then
page="$next"
fi
fi
if (( $CUR > $MAXPAGES )); then
break
fi
ss "$CUR / $MAXPAGES"
done
ss_done
cd ..
cbz_make "$folder"
}
s_eh() {
e "[E-H] Not yet implemented."
}
#############################################
#####@ORIGINAL-FILE 'modules/fakku'
#!/bin/bash
#
#
#
fakku_l="FAKKU"
fakku_url="fakku.net/"
fakku_state=1
fakku_filt=0
fakku_note="They've been shuffling things around recently."
a_fakku() {
if [ -n "$(echo $1 | grep 'fakku.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
return 1
fi
return 0
}
dl_fakku() {
SAFETY_HACKS=1
PAGE="$(fetch "$1/read" "-")"
PAGEDATA="$(echo "$PAGE" | grep "window.params.thumbs =" | sed -e 's/\\\//\//g' -e 's/\];/)/g' -e 's/window.params.thumbs = \[/pages=(/g' -e 's/\.thumb//g' -e 's/","/" "/g' -e 's/\/thumbs\//\/images\//g')"
DATA="$(printf "$PAGEDATA")"
folder="$(echo -n "$PAGE" | grep '<title>' | sed -e 's/[[:space:]]*<title>Read //g' -e 's|</title>||g' | entity_to_char)"
is_done "$folder"
R=$?
if [ $R = 1 ]; then
o "[Fakku] Already downloaded. Skipping."
return 0
fi
mkdir -p "$folder"
cd "$folder"
eval "$DATA"
o -n "[Fakku] Downloading '$folder' "
CUR=0
for image in "${pages[@]}"; do
fetch "https:$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
ss_done
cd ..
cbz_make "$folder"
SAFETY_HACKS=0
}
s_fakku() {
o -n "[Fakku] Scraping Chapters..."
fetch "$1" scrape.htm
grep 'class="content-title"' scrape.htm > batch.txtf
sed -i 's|^.*href="||g' batch.txtf
sed -i 's|" title=.*||g' batch.txtf
sed -i "s/^[[:space:]]*//" batch.txtf
sed -i "s/[[:space:]]*$//" batch.txtf
sed -i "s|^|https://fakku.net|g" batch.txtf
cat batch.txtf >> batch.txt
rm scrape.htm batch.txtf
e "$B$B$B\b\b[Fakku] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/dynsc'
#!/bin/bash
#
#
#
dynsc_l="Dynasty Scans"
dynsc_url="dynasty-scans.com/"
dynsc_state=1
dynsc_filt=0
a_dynsc() {
if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
dl_dynsc() {
PAGEDATA="$(fetch "$1" "-")"
folder="$(echo "$PAGEDATA" | grep "<title>" | sed -e 's/<title>Dynasty Reader &raquo; //g' -e 's|</title>||g')"
is_done "$folder"
R=$?
if [ $R = 1 ]; then
o "[DynastyScans] Already downloaded. Skipping."
return 0
fi
mkdir -p "$folder"
cd "$folder"
PAGELIST="$(echo "$PAGEDATA" | grep "var pages")"
PAGETMP="$(echo "$PAGELIST" | sed -e "s/\"image\"\://g" -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" -e "s/\}\]/\)/g" -e "s/{//g" -e "s/}//g" -e "s/;//g" -e "s/ //g" -e "s/varpages=\[/pages=\(/g" -e "s/,/ /g")"
eval "$PAGETMP"
o -n "[DynastyScans] Downloading '$folder' "
CUR=0
for image in "${pages[@]}"; do
fetch "http://dynasty-scans.com$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
ss_done
cd ..
cbz_make "$folder"
}
s_dynsc() {
o -n "[DynastyScans] Scraping Chapters..."
fetch "$1" "-" | 			\
grep 'class="name"' | 		\
sed -e 's|^.*href="||g' 	\
-e 's|" class=.*||g' 	\
-e "s/^[[:space:]]*//" 	\
-e "s/[[:space:]]*$//" 	\
-e "s|^|http://dynasty-scans.com|g" >> batch.txt
e "$B$B$B\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/foolsl'
#!/bin/bash
#
#
#
foolsl_l="FoolSlide"
foolsl_url="Generic"
foolsl_state=1
foolsl_filt=0
a_foolsl() {
if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
dl_foolsl() {
LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`
o -n "[FoolSlide] Attempting Lazy Download..."
FAILED=0
fetch "$LAZYURL" || export FAILED=1
if [ $FAILED = 1 ]; then
o "Requesting zip failed."
else
o "[OK]"
fi
}
s_foolsl() {
o -n "[Foolslide] Scraping Chapters..."
fetch "$1" "-" |							\
grep '<div class="title"><a href='			\
sed -e 's|<div class="title"><a href="||g'	\
-e 's|" title=.*||g'						\
-e "s/^[[:space:]]*//"						\
-e "s/[[:space:]]*$//"					  | \
reverse_lines >> batch.txt
e "$B$B$B\b\b[Foolslide] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/mpark'
#!/bin/bash
#
#
#
mpark_l="MangaPark"
mpark_url="mangapark.me"
mpark_state=1
mpark_filt=0
a_mpark() {
if [ -n "`echo $1 | grep 'mangapark.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
dl_mpark() {
sitepage="$1"
sitepage=`echo $sitepage | sed "s|/1$||g" | sed "s|/3-1$||g" | sed "s|/6-1$||g" | sed "s|/10-1$||g"`
FETCH="$(fetch "$sitepage" "-")"
folder="$(echo "$FETCH" | grep '<title>' | sed -e 's/<title>//g' -e 's/ Online For Free.*$//g' -e 's/.* - Read //g')"
is_done "$folder"
R=$?
if [ $R = 1 ]; then
o "[MangaPark] Already downloaded. Skipping."
return 0
fi
mkdir -p "$folder"
cd "$folder"
declare -a DATA
DATA=$(echo "$FETCH" | grep 'target="_blank"' - | sed -e '1d' -e 's|^[[:space:]]*<a.*target="_blank" href=||g' -e "s/ title=.*$//" -e "s/\"//g"| tr '\n' ' ')
o -n "[Mangapark] Downloading '$folder' "
CUR=0
for image in ${DATA[@]}; do
fetch "$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
ss_done
cd ..
cbz_make "$folder"
}
s_mpark() {
e "[Mangapark] Scraping Chapters..."
fetch "$1" scrape.htm
grep 'class="ch sts"' scrape.htm > batch.txtr
sed -i 's|^.*href="||g' batch.txtr
sed -i 's|">.*||g' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i "s|^|http://mangapark.com|g" batch.txtr
sed -i "s|/1$||g" batch.txtr
sed -i "s|/3-1$||g" batch.txtr
sed -i "s|/6-1$||g" batch.txtr
sed -i "s|/10-1$||g" batch.txtr
cat batch.txtr | reverse_lines >> batch.txt
rm scrape.htm batch.txtr
e "[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
#############################################
#####@ORIGINAL-FILE 'modules/booru'
#!/bin/bash
#
#
#
booru_l="*Booru"
booru_url="Generic"
booru_state=2
booru_filt=1
a_booru() {
if [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
return 1
elif [ "$1" = "danbooru" ]; then
return 1
elif [ "$1" = "testbooru" ]; then
return 1
elif [ "$1" = "safebooru" ]; then
return 1
fi
return 0
}
booru_syntax_help() {
o "The booru downloader requires parameters. For reference, this is after the booru name/url:"
o "    Usage: booru NAME TAGLIST SPEC [FOLDERNAME]"
o "TAGLIST:"
o "  Here's some quick help: after the booru name, you should specify some of these:"
o "     tagname                   - Specify a tag."
o "     tagname1,tagname2,...     - Specify multiple tags."
o "     pool:poolname             - Download from a pool."
o ""
o "  The same syntax as with boorus can be used with tags. For example:"
o "     art:tagname               - Search by artist."
o "     char:tagname              - Search by character."
o "     copy:tagname              - Search by copyright."
o "     user:username             - Anything uploaded by username."
o "     fav:username              - Anything favorited by username."
o "     ordfav:username           - Anything favorited by username (chronological.)"
o "     md5:hash                  - Posts with a specific MD5 hash."
o "     rating:rate               - Safety rating, e.g. safe, questionable, explicit..."
o "     And so on. See http://safebooru.donmai.us/wiki_pages/43049 for a more exhaustive reference."
o ""
o "  And a quick reference on modifiers:"
o "     tag1,-tag2        - Anything matching tag1, but not tag2."
o "     ~tag1,~tag2       - Anything marked tag1 or tag2."
o "     tag1,tag2         - Anything marked with both tag1 AND tag2."
o "     *tag*             - All tags containing 'tag'."
o ""
o "SPEC:"
o "  Syntax:"
o "    p10                - Download page 10."
o "    l50,p10            - 50 images per page, download page 10. (This is a lowercase L if your font doesn't distinguish.)"
o "    i100               - Download 100 images."
o "    i5-100             - Download images 5-100."
o "    p10-50             - Download from page 10 to 50 inclusive."
o "    l50,p10,i100       - Pages with 50 images, save the first 100 images starting at page 10."
o "    l50,p5-10,i100     - Pages with 50 images, save the first 100 images starting at page 5."
o "                         This syntax is legal, but -10 is ignored."
o "    pages:50           - Download 50 pages. Long syntax."
o "    images:100         - Download 100 images. Long syntax."
o "    limit:50           - 50 images on a page. Long syntax."
o "    resume             - Resume a previous Ctrl+C'd download."
o "    r                  - Short for resume."
o "  A minor note is that *technically* limit:N/lN is a tag for booru."
o "  It can be specified as a tag, but may screw up download code, so do that here."
o ""
o "FOLDERNAME (optional):"
o "  If not specified, it uses the format 'NAME - TAGLIST'."
o ""
o "Also, please note that the following sites are either alternate source bases or NOT boorus,"
o "but will eventually be supported here due to some structural similiarities:"
o " Moebooru-based (https://github.com/moebooru/moebooru)"
o "   yande.re (yandere)"
o "   konachan.com (konachan_g) / konachan.net (konachan)"
o " Shimmie-based (https://github.com/shish/shimmie2)"
o "   shimmie.katawa-shoujo.com (mishimme)"
o " Custom / Needs research"
o "   zerochan.net (zerochan)"
o " All the same, dunno what they run"
o "   *.booru.net (Many different things)"
o "   safebooru.net (Not the same as safebooru)"
o "   gelbooru.net"
exit 1
}
declare -a pagerange
declare -a imagerange
declare -a tagarray
declare -a sizearray
declare -a final
TAGCOUNT=0
page_index=0
final[0]="$source/posts?tags="
final[1]=""
final[2]=""
final[3]="&page="
final[4]=1
RESUME=0
pagerange[0]=1
pagerange[1]=1
mode=1
source=""
name=""
is_pool=0
booru_tag_crunch() {
for (( i=0 ; i < ${#tagarray[@]} ; i++ )); do
if [ ! "$i" = "0" ] && [ ! "$((i + 1))" = "${#tagarray}" ]; then
final[1]="${final[1]}+"
fi
if [[ "${tagarray[i]}" == pool:* ]]; then
o "[Booru] This is apparently a pool."
is_pool=1
TAGCOUNT=1
final[1]="$(echo ${tagarray[i]} | sed -e 's|pool:||g')"
break
fi
tag="$(echo ${tagarray[i]} | sed -e 's|:|%3A|g')"
final[1]="${final[1]}${tag}"
if [[ ! "$tag" == *:* ]]; then
TAGCOUNT=$((TAGCOUNT + 1))
fi
done
}
booru_size_crunch() {
if [ $is_pool = 1 ]; then
return
fi
for (( i=0 ; i < ${#sizearray[@]} ; i++ )); do
if [[ "${sizearray[i]}" == pages:* ]] || [[ "${sizearray[i]}" == p* ]]; then
pagerange=($(echo "${sizearray[i]}" | tr -d 'p' | tr '-' ' '))
if [ "${pagerange[1]}" = "" ]; then
pagerange[1]=${pagerange[0]}
fi
final[4]=${pagerange[0]}
elif [[ "${sizearray[i]}" == limit:* ]] || [[ "${sizearray[i]}" == l* ]]; then
if [[ ! "${sizearray[i]}" == "limit:*" ]]; then
final[2]="+$(echo ${sizearray[i]} | sed 's|l|limit:|')"
else
final[2]="+${sizearray[i]}"
fi
final[2]="$(echo ${final[2]} | sed -e 's|:|%3A|g')"
elif [[ "${sizearray[i]}" == images:* ]] || [[ "${sizearray[i]}" == i* ]]; then
imagerange=($(echo "${sizearray[i]}" | tr -d 'i' | tr '-' ' '))
if [ "${imagerange[1]}" = "" ]; then
imagerange[1]=${imagerange[0]}
imagerange[0]=1
fi
mode=2
elif [[ "${sizearray[i]}" == "all" ]]; then
pagerange[0]=1
pagerange[1]=$MAX_INT
final[4]=${pagerange[0]}
elif [[ "${sizearray[i]}" == "resume" ]] || [[ "${sizearray[i]}" == "r" ]]; then
RESUME=1
fi
done
}
dl_booru_page() {
url="$1"
name_out="$2"
FETCH_RESULT=1
while [ ! $FETCH_RESULT = 0 ]; do
fetch "$url" "$name_out"
if [ ! $FETCH_RESULT = 0 ]; then
FAILS=$((FAILS + 1))
message=" :<"
if [ ! $FAILS = 0 ]; then
ss_done "!!!"
n "[Booru] Server refused us. Waiting a bit... "
SLEEP=$((60 * FAILS))
while [ ! $SLEEP = 0 ]; do
sleep 1s
ss "${SLEEP}s"
SLEEP=$((SLEEP - 1))
done
ss_done "DONE"
n "[Booru] Continuing... "
fi
fi
sleep 2s
done
FAILS=0
}
dl_booru() {
if [ "$2" = "" ] && [ "$3" = "" ] || [ "$1" = "" ]; then
booru_syntax_help
fi
if [ -n "$(echo $1 | grep 'safebooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "safebooru" ]; then
source="http://safebooru.donmai.us"
name="danbooru"
site="safebooru"
elif [ -n "$(echo $1 | grep 'testbooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "testbooru" ]; then
source="http://testbooru.donmai.us"
name="danbooru"
site="testbooru"
elif [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "danbooru" ]; then
source="http://danbooru.donmai.us"
name="danbooru"
site="danbooru"
else
source="$1"
o "[Booru] This type of Booru isn't specified as supported."
o "[Booru] It may work, depending on whether the autodetect picks up a valid scheme,"
o "[Booru] or it may completely bomb out. If it works, please submit a bug report"
o "[Booru] so I can add it to the supported list."
name="unknown"
site="$source"
fi
booru_${name}_init
shift
tagarray=($(echo "$1" | tr ',' ' '))
sizearray=($(echo "$2" | tr ',' ' '))
booru_tag_crunch
booru_size_crunch
o "[Booru] Info:"
o "[Booru]   Fetching URL $(echo ${final[0]}${final[1]}${final[2]}${final[3]} | tr -d ' ')"
o -n "[Booru]   Will download "
if [ $mode = 1 ]; then
if [ "${pagerange[0]}" = "${pagerange[1]}" ]; then
o "page ${pagerange[0]}."
else
TO=${pagerange[1]}
if [ $TO = $MAX_INT ]; then
TO="Max"
fi
o "pages ${pagerange[0]} to $TO."
fi
else
if [ "${imagerange[0]}" = "${imagerange[1]}" ]; then
o -n "image ${imagerange[0]}"
else
o -n "images ${imagerange[0]} to ${imagerange[1]}"
fi
o ", starting from page ${pagerange[1]}."
fi
if [ $TAGCOUNT = 0 ] && [ ${pagerange[1]} == $MAX_INT ]; then
o "[Booru] Holy fucking shit. You're telling me to download a whole booru? Are you insane?"
o "[Booru] Not to mention, I didn't even DOCUMENT this syntax on purpose."
o "[Booru] Type 'Yes, I am sane.' after the colon if you really meant to do this."
o -n "[Booru] You sane? : "
read sanequery
if [ "$sanequery" = "Yes, I am sane." ]; then
o "[Booru] Okay, nutjob, if you say so. Don't come complaining later."
else
o "[Booru] Yeah, thought so. Whew~"
exit 1
fi
fi
if [ ! "${final[2]}" = "" ]; then
o "[Booru]   Requesting $(echo ${final[2]} | sed 's|+limit%3A||') images per page."
fi
if (( $TAGCOUNT >= 2 )); then
o "[Booru]   Warning, more than two normal tags. Some boorus don't like this."
fi
if [ ! "$3" = "" ]; then
tagdir="$3"
else
tagdir="$site - $1"
fi
o "[Booru]   Output to folder '$tagdir'."
mkdir -p "$tagdir"
cd "$tagdir"
mkdir -p meta content
if [ $RESUME = 0 ]; then
o -n '' > meta.txt
o -n "[Booru] Fetching pages... "
for (( range=${pagerange[0]} ; range <= ${pagerange[1]} ; range++ )); do
ss "${range} -> ${pagerange[0]}/${pagerange[1]}"
booru_${name}_page "$range"
R=$?
lines=$(wc -l meta.txt | sed 's| .*||g')
if [ $mode = 2 ] && (( $lines > ${imagerange[1]} )); then
break
fi
if [ $R = 1 ]; then
break
fi
done
fi
ss_done
lines=$(wc -l meta.txt | sed 's| .*||g')
DONE=0
LINES=$(wc -l meta.txt | sed 's| .*||g')
cd meta
o -n "[Booru] Checking and refetching metadata... "
while read meta_id; do
SKIP=0
if [ $mode = 2 ]; then
if (( $DONE < ${imagerange[0]} )); then
SKIP=1
fi
if (( $DONE > ${imagerange[1]} )); then
break
fi
TOTAL="$RANGE"
elif [ $RESUME = 1 ]; then
DONE=$((DONE - 1))
if [ $DONE = 0 ]; then
RESUME=0
DONE=$PRE
fi
else
TOTAL="$LINES"
ss "${DONE} -> ${LINES}"
booru_${name}_meta "$meta_id"
DONE=$((DONE + 1))
fi
done < ../meta.txt
ss_done
o -n "[Booru] Downloading content... "
META_DIR="$(pwd)"
cd ../content
DONE=0
while read meta_id; do
SKIP=0
if [ $mode = 2 ]; then
if (( $DONE < ${imagerange[0]} )); then
SKIP=1
fi
if (( $DONE > ${imagerange[1]} )); then
break
fi
TOTAL="$RANGE"
elif [ $RESUME = 1 ]; then
DONE=$((DONE - 1))
if [ $DONE = 0 ]; then
RESUME=0
DONE=$PRE
fi
else
TOTAL="$LINES"
ss "${DONE} -> ${LINES}"
booru_${name}_content "$meta_id"
DONE=$((DONE + 1))
fi
done < ../meta.txt
rm ../meta.txt
ss_done
o "[Booru] Done with download. If you don't care about the metadata, it is no longer needed."
}
s_booru() {
exit 1
}
#############################################
#####@ORIGINAL-FILE 'main'
#!/bin/bash
#
#
#
set +H
if [ "$1" = "auto" -o "$1" = "a" ]; then
shift
auto "$@"
exit 0
elif [ "$1" = "batch" -o "$1" = "l" ]; then
shift
batch "$@"
exit 0
elif [ "$1" = "autobatch" -o "$1" = "b" ]; then
shift
autobatch "$@"
exit 0
elif [ "$1" = "scrape" -o "$1" = "s" ]; then
shift
scrape "$@"
exit 0
elif [ "$1" = "login" -o "$1" = "u" ]; then
shift
mod_login "$@"
exit 0
elif [ "$1" = "upgrade" ]; then
upgrade_self
exit 0
else
MATCH=""
for module in ${MODS[@]}; do
if [ "$1" = "$module" ]; then
shift
dl_${module} "$@"
exit 0
fi
done
auto "$@"
R=$?
if [ $R = 1 ]; then
help
fi
fi
#############################################
