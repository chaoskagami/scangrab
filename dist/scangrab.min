#!/bin/bash
o() {
echo "$@"
}
n() {
echo -ne "$@"
}
e() {
echo -e "$@"
}
B="\b\b\b\b\b\b\b\b\b\b"
COOKIEJAR="$(pwd)/cookiejar"
HAVE_IMAGICK=0
FETCH_RESULT=0
message=""
use_cookies=0
_COLOR=0
_FETCH_CMD=""
_CHECK_VALID=0
_FETCHTOOL=0
_BUSYBOX=0
_SC="|"
_NUM=0
_tput="$(which tput 2>/dev/null)"
_identify="$(which identify 2>/dev/null)"
_convert="$(which convert 2>/dev/null)"
_wget="$(which wget 2>/dev/null)"
_curl="$(which curl 2>/dev/null)"
_aria="$(which aria2c 2>/dev/null)"
for ((i = 1, n = 2;; n = 1 << ++i)); do
if [[ ${n:0:1} == '-' ]]; then
MAX_INT=$(((1 << i) - 1))
break
fi
done
if [ "$TERM" = "xterm" ] || [ "$TERM" = "linux" ] || [ "$TERM" = "screen" ]; then
_COLOR=1
if [ -f $_tput ]; then
_COLOR=2
fi
fi
if [ -f "$_identify" ]; then
_CHECK_VALID=1
fi
if [ -f "$_identify" ]; then
HAVE_IMAGICK=1
fi
if [ ! "$_wget" = "" ]; then
common_opts=" --quiet --no-cache --content-disposition --user-agent=\"Mozilla/4.0\" -c -t 1 -T 10 --random-wait "
if [ ! "$($_wget --help 2>&1 | grep busybox)" = "" ]; then
o "[Warning] Your system wget is busybox, which can't actually do some things like reject cache and retry."
common_opts=" -q -c -U \"Mozilla/4.0\""
_BUSYBOX=1
fi
_FETCH_CMD="$_wget $common_opts"
_FETCHTOOL=1
else
if [ ! "$_curl" = "" ]; then
_FETCH_CMD=$_curl
_FETCHTOOL=2
else
if [ ! "$_aria" = "" ]; then
_FETCH_CMD=$_aria
_FETCHTOOL=3
fi
fi
fi
type() {
if [ $_COLOR = 1 ]; then
n "\x1b[$1m"
elif [ $_COLOR = 2 ]; then
if [ $1 = 0 ]; then
tput sgr0
elif [ $1 = 1 ]; then
tput bold
elif [ $1 = 2 ]; then
tput dim
fi
fi
}
color() {
if [ $_COLOR = 1 ]; then
n "\x1b[3$1m"
elif [ $_COLOR = 2 ]; then
tput setaf $1
fi
}
cbz_make() {
e "[Post] Making CBZ..."
if [ ! -d "$1" ]; then
e "\n[Error] Not a folder. Something went wrong."
exit 1
fi
if [ "$(ls "$1")" = "" ]; then
o "[Error] No files? Download failed."
exit 1
fi
zip -r "$1.zip" "$1" > /dev/null 2>&1
mv "$1.zip" "$1.cbz" > /dev/null 2>&1
e "[Post] Cleanup..."
rm -rf "$1"
e "[Post] Finished!"
}
verify() {
if [ -f $_identify ]; then
$_identify -verbose -regard-warnings "$1" 2>&1 >/dev/null
_IDRES=$?
return $_IDRES
fi
return 0
}
ss() {
if [ "$_SC" = "|" ]; then
_SC="/"
elif [ "$_SC" = "/" ]; then
_SC="-"
elif [ "$_SC" = "-" ]; then
_SC="\\"
elif [ "$_SC" = "\\" ]; then
_SC="|"
fi
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
for (( i=0 ; i < _NUM ; i++ )); do
n " "
done
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
STR="[$1 $_SC] $message"
_NUM="${#STR}"
n "$STR"
}
ss_done() {
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
for (( i=0 ; i < _NUM ; i++ )); do
n " "
done
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
_SC="|"
_NUM=0
e "[OK]"
}
mimetype() {
o "$(file --mime-type "$1" | sed 's/.* //g')"
}
s_login() {
if [ $_FETCHTOOL = 1 ]; then
_CMD="$_FETCH_CMD --post-data='$1=$3&$2=$4&$6' \"$5\"  --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies -O/dev/null"
elif [ $_FETCHTOOL = 2 ]; then
_CMD="$_FETCH_CMD -d '$1=$3&$2=$4&$6' $5 -b $COOKIEJAR -c $COOKIEJAR >/dev/null"
elif [ $_FETCHTOOL = 3 ]; then
o "[Warn] aria2c can't post at the moment; this will fail to get required cookies."
_CMD="$_FETCH_CMD $5 --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR -o/dev/null"
fi
eval " $_CMD" 2>/dev/null
FETCH_RESULT=$?
}
fetch() {
if [ $_FETCHTOOL = 1 ]; then
_CMD="$_FETCH_CMD \"$1\""
if [ $use_cookies = 1 ]; then
_CMD="$_CMD --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies"
fi
if [ "$2" = "-" ]; then
_CMD="$_CMD -O -"
elif [ ! "$2" = "" ]; then
_CMD="$_CMD -O \"$2\""
fi
elif [ $_FETCHTOOL = 2 ]; then
_CMD="$_FETCH_CMD $1"
if [ $use_cookies = 1 ]; then
_CMD="$_CMD -b $COOKIEJAR -c $COOKIEJAR"
fi
if [ "$2" = "" ]; then
_CMD="$_CMD > $(basename "$1")"
elif [ "$2" = "-" ]; then
_CMD="$_CMD"
else
_CMD="$_CMD > \"$2\""
fi
elif [ $_FETCHTOOL = 3 ]; then
_CMD="$_FETCH_CMD $1"
if [ $use_cookies = 1 ]; then
_CMD="$_CMD --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR"
fi
if [ "$2" = "" ]; then
_CMD="$_CMD -o $(basename "$1")"
elif [ "$2" = "-" ]; then
_CMD="$_CMD -o -"
else
_CMD="$_CMD -o \"$2\""
fi
fi
eval " $_CMD" 2>/dev/null
FETCH_RESULT=$?
MIME="$(mimetype "$_FILE")"
if [ "$MIME" = "image/jpeg" ] || [ "$MIME" = "image/png" ] || [ "$MIME" = "image/gif" ] || [ "$MIME" = "image/bmp" ]; then
verify "$_FILE"
VALID=$?
if [ ! $VALID = 0 ]; then
o "[WARN] File '$_FILE' is corrupted."
fi
fi
return $FETCH_RESULT
}
entity_to_char() {
sed \
-e "s/&#32;/ /g" \
-e "s/&nbsp;/ /g" \
-e "s/&#33;/\!/g" \
-e "s/&#34;/\"/g" \
-e "s/&#35;/\#/g" \
\
-e "s/&#36;/\$/g" \
-e "s/&#37;/\%/g" \
-e "s/&amp;/\&/g" \
-e "s/&#38;/\&/g" \
-e "s/&#39;/'/g" \
\
-e "s/&#40;/\(/g" \
-e "s/&#41;/\)/g" \
-e "s/&#42;/\*/g" \
-e "s/&#43;/\+/g" \
-e "s/&#44;/\,/g" \
\
-e "s/&#45;/\-/g" \
-e "s/&#46;/\./g" \
-e "s/&#58;/\:/g" \
-e "s/&#59;/\;/g" \
-e "s/&lt;/\</g" \
\
-e "s/&#60;/\</g" \
-e "s/&gt/\>/g" \
-e "s/&#61;/\>/g" \
-e "s/&#63;/\?/g" \
-e "s/&#64;/\@/g" \
\
-e "s/&#91;/\[/g" \
-e "s/&#92;/\\\\/g" \
-e "s/&#93;/\]/g" \
-e "s/&#94;/\^/g" \
-e "s/&#95;/\_/g" \
\
-e "s/&#123;/\{/g" \
-e "s/&#124;/\|/g" \
-e "s/&#125;/\}/g" \
-e "s/&#126;/\~/g" \
-e "s/&yen;/¥/g" \
\
-e "s/&#165;/¥/g" \
-e "s/&sup2;/²/g" \
-e "s/&#178;/²/g" \
-e "s/&sup3;/³/g" \
-e "s/&#179;/³/g" \
\
-e "s/&frac14;/¼/g" \
-e "s/&#188;/¼/g" \
-e "s/&frac12;/½/g" \
-e "s/&#189;/½/g" \
-e "s/&frac34;/¾/g" \
\
-e "s/&#190;/¾/g" \
-e "s/&spades;/♠/g" \
-e "s/&#9824;/♠/g" \
-e "s/&clubs;/♣/g" \
-e "s/&#9827;/♣/g" \
\
-e "s/&hearts;/♥/g" \
-e "s/&#9829;/♥/g" \
-e "s/&diams;/♦/g" \
-e "s/&#9830;/♦/g"
}
remove_illegal() {
sed \
-e "s/|/-/g" \
-e "s|/|-|g"
}
reverse_lines() {
readarray -t LINES
for (( I = ${#LINES[@]}; I; )); do
o "${LINES[--I]}"
done
}
rev=8f4576fd2bdb59f284ced304223d13d2fc675c6e
branch=master
MODS=(mangabox batoto niconicoseiga eh fakku dynsc foolsl mread mpark booru)
mangabox_l="Mangabox"
mangabox_u="https://www.mangabox.me/"
mangabox_state=1
mangabox_filt=0
mangabox_note="Automatically splits pages with IM. The PC webpage is broken badly. Don't report it."
a_mangabox() {
if [ -n "`echo $1 | grep 'mangabox.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_mangabox() {
o "[MangaBox] Getting list of pages..."
fetch "$1" "tmp.htm"
fullname="$(cat tmp.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char)"
declare -a list
list=($(cat tmp.htm | grep 'class="jsNext"' | sed -e 's|.*<li><img src="||g' -e 's|?.*||g'))
mkdir -p "$fullname"
cd "$fullname"
o -n "[MangaBox] Downloading '$fullname'... "
NUM=0
for page in ${list[@]}; do
NUM=$((NUM + 1))
VALID=1
while [ ! $VALID = 0 ]; do
fetch $page
verify $(basename $page)
VALID=$?
done
if [ $HAVE_IMAGICK = 1 ]; then
ss "Download:$NUM"
$_convert $(basename $page) -crop 2x1@ +repage +adjoin "${NUM}_%d.png"
ss "Reformat:$NUM"
rm $(basename $page)
ss "Collat:$NUM"
mv ${NUM}_0.png ${NUM}_b.png
mv ${NUM}_1.png ${NUM}_a.png
else
ss "$NUM"
fi
done
cd ..
ss_done
rm tmp.htm
cbz_make "$fullname"
}
s_mangabox() {
o "[MangaBox] Not yet supported, sorry."
exit 1
}
batoto_l="Batoto"
batoto_u="http://bato.to/"
batoto_state=1
batoto_filt=1
batoto_note="Their page template is highly unstable."
a_batoto() {
if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_batoto() {
folder="$(fetch "$1" "-" | grep -C0 "<title>" | sed -e "s/^[[:space:]]*<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e "s/[[:space:]]*$/\n/" | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
CUR=0
PAGES=0
RET=0
base="$1"
if [ ! "${base:${#base}-1}" = "/" ]; then
base="${base}/"
fi
o -n "[Batoto] Downloading '$folder' "
while [ "$RET" = "0" ]; do
CUR=$(( CUR + 1 ))
fetch "${base}${CUR}" "$CUR.htm"
if [ "$(mimetype $CUR.htm)" = "application/x-gzip" ]; then
mv $CUR.htm $CUR.htm.gz
gunzip $CUR.htm.gz
message=" :/"
fi
img="$(grep -C0 'z-index: 1003' $CUR.htm | sed -e 's/^[[:space:]]*<img src="//g' -e 's/".*$//g')"
ext="${img##*.}"
fetch "$img" "${CUR}_${folder}.${ext}"
RET=$?
rm $CUR.htm
ss "$CUR"
done
PAGES=$(( CUR - 1 ))
ss_done
cd ..
cbz_make "$folder"
}
s_batoto() {
notice_batoto
o -n "[Batoto] Scraping Chapters..."
fetch "$1" scrape.htm
if [ "$(mimetype scrape.htm)" = "application/x-gzip" ]; then
mv $CUR.htm $CUR.htm.gz
gunzip $CUR.htm.gz
message=" :/"
fi
grep -A 2 'Sort:' scrape.htm >> batch.txtr
sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr
sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr
sed -i "s|<a href=\"||g" batch.txtr
sed -i "s|\" title=.*||g" batch.txtr
sed -i '/^[[:space:]]*$/d' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i 's|^--$||g' batch.txtr
sed -i '/^$/d' batch.txtr
cat batch.txtr | reverse_lines > batch.txtf
if [ "$2" = "" ]; then
sed -ni '0~2p' batch.txtf
cat batch.txtf >> batch.txt
else
n "$B$B\b Applying Language Filter '$2'..."
grep -A 1 "$2" batch.txtf > batch.txtf2
sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2
sed -i '/^$/d' batch.txtf2
sed -ni '0~2p' batch.txtf2
cat batch.txtf2 >> batch.txt
fi
rm scrape.htm batch.txtr batch.txtf*
en "$B$B$B\b"
for ((n=0;n < ${#2}; n++)); do
en '\b'
done
e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
niconicoseiga_l="Niconico Seiga"
niconicoseiga_u="http://seiga.nicovideo.jp"
niconicoseiga_state=1
niconicoseiga_filt=0
niconicoseiga_note="Please login first, or make a cookiejar file."
niconicoseiga_uselogin=1
a_niconicoseiga() {
if [ -n "`echo $1 | grep 'seiga.nicovideo.jp' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
login_niconicoseiga() {
use_cookies=1
s_login "mail_tel" "password" "$1" "$2" "https://secure.nicovideo.jp/secure/login"
}
d_niconicoseiga() {
use_cookies=1
name="$(basename $1)"
fetch $1 "$name.htm"
fullname="$(cat $name.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e "s|\".*||g" | entity_to_char | remove_illegal)"
if [ ! -f "$name.htm" ]; then
o "[Niconico] No page found, aborting."
return 1
fi
o "[Niconico] Extracting image list..."
cat $name.htm | grep 'data-image-id' | sed -e 's|^[[:space:]]*data-image-id="||g' -e 's|"[[:space:]]*$||g' > $name.lst
if [ "$(cat $name.lst)" = "" ]; then
o "[Niconico] No images in page. Did you login?"
return 1
fi
mkdir -p "$fullname"
cd "$fullname"
o -n "[Niconico] Downloading '$fullname' (from $name) "
COUNT=0
while read ID; do
COUNT=$((COUNT + 1))
fetch "http://lohas.nicoseiga.jp/thumb/${ID}p" "${COUNT}.jpg"
ss $COUNT
done < ../$name.lst
ss_done
cd ..
rm $name.lst $name.htm
cbz_make "$fullname"
}
s_niconicoseiga() {
COOKIES=1
o "[Niconico] Scraping chapters..."
fetch "$1" "-" | grep 'class="episode"' | sed -e 's|.*<a href="||g' -e 's|?.*||g' -e 's|/watch|http://seiga.nicovideo.jp/watch|g' >> batch.txt
o "[Niconico] Done."
}
eh_l="E-Hentai"
eh_u="http://e-hentai.org/"
eh_state=1
eh_filt=0
eh_note="Logging in will result in HQ images and less H@H peer-related issues =_=;"
eh_uselogin=1
login_eh() {
use_cookies=1
s_login "UserName" "PassWord" "$1" "$2" "http://forums.e-hentai.org/index.php?act=Login&CODE=01" \
"CookieDate=1&b=&bt=&referer=http://forums.e-hentai.org/?act=idx"
user="$(fetch "http://forums.e-hentai.org/?act=idx" "-" | grep 'Logged in as' | sed -e 's|.*showuser=||' -e 's|<.*||' -e 's|.*>||g')"
if [ ! "$user" = "" ]; then
o "[E-H] Logged in as: $user"
exit 0
fi
exit 1
}
a_eh() {
if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_eh() {
LOGGEDIN=0
use_cookies=0
if [ -e "cookiejar" ]; then
grep 'e-hentai.org' $COOKIEJAR | grep 'ipb_member_id' >/dev/null
RES=$?
if [ $RES = 0 ]; then
o "[E-H] We seem to have cookies...using them."
LOGGEDIN=1
use_cookies=1
fi
fi
sitepage=$1
o "[E-H] Fetching index page..."
fetch "${sitepage}/?nw=always" tmp.1
while [ ! $FETCH_RESULT = 0 ]; do
o "[E-H] Whoa. Failed to download index. Going again..."
fetch "${sitepage}/?nw=always" tmp.1
done
folder="$(cat tmp.1 | grep 'title>' | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/ - E-Hentai Galleries//g' | sed 's|/|_|g' | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
page=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/' | sed 's/"><img alt.*//g' | grep 'url:' | sed 's/url://g')
rm ../tmp.1
o -n "[E-H] Downloading '$folder' "
if [ $LOGGEDIN = 1 ]; then
o -n "[HQ] "
else
o -n "[LQ] "
fi
doneyet=0
CDNFAIL=0
CUR=1
while [ $doneyet == 0 ]; do
fetch "$page" "$CUR.htm"
lq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'keystamp' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
hq=$(cat $CUR.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
next_cnt=$((CUR + 1))
next=$(cat $CUR.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)
extra=""
if [ "$next" == "" ]; then
doneyet=1
fi
if [ $CDNFAIL -ge 3 ]; then
FETCH_RESULT=1
else
if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
fetch "$lq" "$(basename $lq)"
else
fetch "$hq"
fi
fi
if [ ! $FETCH_RESULT = 0 ]; then
message=" :("
CDNFAIL=$((CDNFAIL + 1))
if [ $CDNFAIL -ge 3 ]; then
message=" >:("
fi
notload=$(cat $CUR.htm | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")
fetch "$page?nl=$notload" "${CUR}nl.htm"
lq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'image.php' | sed -e 's|.*src="||g' -e 's|".*||' | entity_to_char)
hq=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e 'a href' -e 'img src' -e 'img id="img"' | grep -e 'source' | sed -e 's|.*href="||g' -e 's|".*||g' | entity_to_char)
next=$(cat ${CUR}nl.htm | tr '<' '\n' | grep -e "-$next_cnt\"" | sed -e 's|.*href="||g' -e 's|">||g' | head -n1)
if [ $LOGGEDIN = 0 ] || [ "$hq" = "" ]; then
fetch "$lq"
else
fetch "$hq"
fi
rm "${CUR}nl.htm"
if [ ! $FETCH_RESULT = 0 ]; then
message=" >:["
else
rm $CUR.htm
CUR=$(( CUR + 1 ))
if [ ! "$next" = "" ]; then
page="$next"
fi
fi
else
rm $CUR.htm
CUR=$(( CUR + 1 ))
if [ ! "$next" = "" ]; then
page="$next"
fi
fi
ss "$CUR"
done
ss_done
cd ..
cbz_make "$folder"
}
s_eh() {
e "[E-H] This isn't supported, considering there's really zero categorization here."
}
fakku_l="FAKKU"
fakku_u="https://fakku.net/"
fakku_state=1
fakku_filt=0
fakku_note="They've been shuffling things around recently."
a_fakku() {
if [ -n "$(echo $1 | grep 'fakku.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
return 1
fi
return 0
}
d_fakku() {
PAGE="$(fetch "$1/read" "-")"
PAGEDATA="$(echo "$PAGE" | grep "window.params.thumbs =" | sed -e 's/\\\//\//g' -e 's/\];/)/g' -e 's/window.params.thumbs = \[/pages=(/g' -e 's/\.thumb//g' -e 's/","/" "/g' -e 's/\/thumbs\//\/images\//g')"
DATA="$(printf "$PAGEDATA")"
folder="$(echo "$PAGE" | grep '<title>' | sed -e 's/[[:space:]]*<title>Read //g' -e 's|</title>||g' | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
eval "$DATA"
o -n "[Fakku] Downloading '$folder' "
CUR=0
for image in "${pages[@]}"; do
fetch "https:$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
ss_done
cd ..
cbz_make "$folder"
}
s_fakku() {
o -n "[Fakku] Scraping Chapters..."
fetch "$1" scrape.htm
grep 'class="content-title"' scrape.htm > batch.txtf
sed -i 's|^.*href="||g' batch.txtf
sed -i 's|" title=.*||g' batch.txtf
sed -i "s/^[[:space:]]*//" batch.txtf
sed -i "s/[[:space:]]*$//" batch.txtf
sed -i "s|^|https://fakku.net|g" batch.txtf
cat batch.txtf >> batch.txt
rm scrape.htm batch.txtf
e "$B$B$B\b\b[Fakku] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
dynsc_l="Dynasty Scans"
dynsc_u="http://dynasty-scans.com/"
dynsc_state=1
dynsc_filt=0
a_dynsc() {
if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_dynsc() {
PAGEDATA="$(fetch "$1" "-")"
folder="$(echo "$PAGEDATA" | grep "<title>" | sed -e 's/<title>Dynasty Reader &raquo; //g' -e 's|</title>||g')"
mkdir -p "$folder"
cd "$folder"
PAGELIST="$(echo "$PAGEDATA" | grep "var pages")"
PAGETMP="$(echo $PAGELIST | sed -e "s/\"image\"\://g" -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" -e "s/\}\]/\)/g" -e "s/{//g" -e "s/}//g" -e "s/;//g" -e "s/ //g" -e "s/varpages=\[/pages=\(/g" -e "s/,/ /g")"
eval "$PAGETMP"
o -n "[DynastyScans] Downloading '$folder' "
CUR=0
for image in "${pages[@]}"; do
fetch "http://dynasty-scans.com$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
ss_done
cd ..
cbz_make "$folder"
}
s_dynsc() {
o -n "[DynastyScans] Scraping Chapters..."
fetch "$1" "-" | 			\
grep 'class="name"' | 		\
sed -e 's|^.*href="||g' 	\
-e 's|" class=.*||g' 	\
-e "s/^[[:space:]]*//" 	\
-e "s/[[:space:]]*$//" 	\
-e "s|^|http://dynasty-scans.com|g" >> batch.txt
e "$B$B$B\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
foolsl_l="FoolSlide"
foolsl_u="Generic"
foolsl_state=1
foolsl_filt=0
a_foolsl() {
if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_foolsl() {
LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`
o -n "[FoolSlide] Attempting Lazy Download..."
FAILED=0
fetch "$LAZYURL" || export FAILED=1
if [ $FAILED = 1 ]; then
o "Requesting zip failed."
else
o "[OK]"
fi
}
s_foolsl() {
o -n "[Foolslide] Scraping Chapters..."
fetch "$1" "-" |							\
grep '<div class="title"><a href='			\
sed -e 's|<div class="title"><a href="||g'	\
-e 's|" title=.*||g'						\
-e "s/^[[:space:]]*//"						\
-e "s/[[:space:]]*$//"					  | \
reverse_lines >> batch.txt
e "$B$B$B\b\b[Foolslide] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
mread_l="Mangareader"
mread_u="http://www.mangareader.net/"
mread_state=0
mread_filt=0
a_mread() {
if [ -n "`echo $1 | grep 'mangareader.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_mread() {
folder="$(echo $1 | sed -e "s|http\:\/\/||g" | sed -e "s|www\.mangareader\.net\/||g" | sed -e "s|\/|-|g" | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
o -n "[Mangareader] Blindly Downloading '$folder' "
IS_404=0
CUR=0
PAGES=0
while [ $IS_404 = 0 ]; do
CUR=$(( CUR + 1 ))
PAGEDATA=$(fetch "$1/$CUR" "$CUR.htm")
IS_404=$?
if [ $IS_404 = 0 ]; then
src=`grep '<div id="imgholder">' $CUR.htm | sed "s/^.*src=\"//g" | sed "s/\" alt=.*//g"`
o $src
EXT="${src##*.}"
fetch "$src" "$CUR.$EXT"
ss
fi
done
PAGES=$(( CUR - 1 ))
ss_done
rm *.htm
cd ..
cbz_make "$folder"
}
s_mread() {
o "[MangaReader] NYI, sorry."
}
mpark_l="MangaPark"
mpark_u="http://mangapark.me/"
mpark_state=1
mpark_filt=0
a_mpark() {
if [ -n "`echo $1 | grep 'mangapark.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_mpark() {
sitepage="$1"
sitepage=`echo $sitepage | sed "s|/1$||g" | sed "s|/3-1$||g" | sed "s|/6-1$||g" | sed "s|/10-1$||g"`
FETCH="$(fetch "$sitepage" "-")"
folder="$(echo "$FETCH" | grep '<title>' | sed -e 's/<title>//g' -e 's/ Online For Free.*$//g' -e 's/.* - Read //g')"
mkdir -p "$folder"
cd "$folder"
declare -a DATA
DATA=$(echo "$FETCH" | grep 'target="_blank"' - | sed -e '1d' -e 's|^[[:space:]]*<a.*target="_blank" href=||g' -e "s/ title=.*$//" -e "s/\"//g"| tr '\n' ' ')
o -n "[Mangapark] Downloading '$folder' "
CUR=0
for image in ${DATA[@]}; do
fetch "$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
ss_done
cd ..
cbz_make "$folder"
}
s_mpark() {
e "[Mangapark] Scraping Chapters..."
fetch "$1" scrape.htm
grep 'class="ch sts"' scrape.htm > batch.txtr
sed -i 's|^.*href="||g' batch.txtr
sed -i 's|">.*||g' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i "s|^|http://mangapark.com|g" batch.txtr
sed -i "s|/1$||g" batch.txtr
sed -i "s|/3-1$||g" batch.txtr
sed -i "s|/6-1$||g" batch.txtr
sed -i "s|/10-1$||g" batch.txtr
cat batch.txtr | reverse_lines >> batch.txt
rm scrape.htm batch.txtr
e "[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
booru_MODS=(danbooru)
d_booru_danbooru_init() {
final[0]="$source/posts.json?tags="
}
d_booru_danbooru_page() {
final[4]=$1
d_booru_page "$(echo ${final[@]} | tr -d ' ')" "page${range}.json"
BEFORE=$(wc -l meta.txt | sed 's| .*||g')
cat "page${range}.json" | tr ',' "\n" | grep '"id"' | sed 's|.*:||g' >> meta.txt
AFTER=$(wc -l meta.txt | sed 's| .*||g')
rm "page${range}.json"
if (( BEFORE == AFTER )); then
return 1
fi
return 0
}
d_booru_danbooru_data() {
declare -a base
base[0]="$source/posts/"
base[1]="${1}.json"
d_booru_page "$(echo ${base[@]} | tr -d ' ')" "meta_${base[1]}"
url_img="$(cat "meta_${base[1]}" | tr ',' "\n" | grep '"file_u"' | sed -e 's|.*:||g' -e "s|\"||g")"
file_ext="$(cat "meta_${base[1]}" | tr ',' "\n" | grep '"file_ext"' | sed 's|.*:||g')"
d_booru_page "${source}${url_img}" "image_${1}.${file_ext}"
}
booru_l="*Booru"
booru_u="Generic"
booru_state=2
booru_filt=1
a_booru() {
if [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
return 1
elif [ "$1" = "danbooru" ]; then
return 1
elif [ "$1" = "safebooru" ]; then
return 1
fi
return 0
}
booru_syntax_help() {
o "The booru downloader requires parameters. For reference, this is after the booru name/url:"
o "    Usage: booru NAME TAGLIST SPEC [FOLDERNAME]"
o "TAGLIST:"
o "  Here's some quick help: after the booru name, you should specify some of these:"
o "     tagname                   - Specify a tag."
o "     tagname1,tagname2,...     - Specify multiple tags."
o "     pool:poolname             - Download from a pool."
o ""
o "  The same syntax as with boorus can be used with tags. For example:"
o "     art:tagname               - Search by artist."
o "     char:tagname              - Search by character."
o "     copy:tagname              - Search by copyright."
o "     user:username             - Anything uploaded by username."
o "     fav:username              - Anything favorited by username."
o "     ordfav:username           - Anything favorited by username (chronological.)"
o "     md5:hash                  - Posts with a specific MD5 hash."
o "     rating:rate               - Safety rating, e.g. safe, questionable, explicit..."
o "     And so on. See http://safebooru.donmai.us/wiki_pages/43049 for a more exhaustive reference."
o ""
o "  And a quick reference on modifiers:"
o "     tag1,-tag2        - Anything matching tag1, but not tag2."
o "     ~tag1,~tag2       - Anything marked tag1 or tag2."
o "     tag1,tag2         - Anything marked with both tag1 AND tag2."
o "     *tag*             - All tags containing 'tag'."
o ""
o "SPEC:"
o "  Syntax:"
o "    p10                - Download page 10."
o "    l50,p10            - 50 images per page, download page 10. (This is a lowercase L if your font doesn't distinguish.)"
o "    i100               - Download 100 images."
o "    i5-100             - Download images 5-100."
o "    p10-50             - Download from page 10 to 50 inclusive."
o "    l50,p10,i100       - Pages with 50 images, save the first 100 images starting at page 10."
o "    l50,p5-10,i100     - Pages with 50 images, save the first 100 images starting at page 5."
o "                         This syntax is legal, but -10 is ignored."
o "    pages:50           - Download 50 pages. Long syntax."
o "    images:100         - Download 100 images. Long syntax."
o "    limit:50           - 50 images on a page. Long syntax."
o "  A minor note is that *technically* limit:N/lN is a tag for booru."
o "  It can be specified as a tag, but may screw up download code, so do that here."
o ""
o "FOLDERNAME (optional):"
o "  If not specified, it uses the format 'NAME - TAGLIST'."
o ""
o "Also, please note that the following sites are either alternate source bases or NOT boorus,"
o "but will eventually be supported here due to some structural similiarities:"
o " Moebooru-based (https://github.com/moebooru/moebooru)"
o "   yande.re (yandere)"
o "   konachan.com (konachan_g) / konachan.net (konachan)"
o " Shimmie-based (https://github.com/shish/shimmie2)"
o "   shimmie.katawa-shoujo.com (mishimme)"
o " Custom / Needs research"
o "   zerochan.net (zerochan)"
o " All the same, dunno what they run"
o "   *.booru.net (Many different things)"
o "   safebooru.net (Not the same as safebooru)"
o "   gelbooru.net"
exit 1
}
declare -a pagerange
declare -a imagerange
declare -a tagarray
declare -a sizearray
declare -a final
TAGCOUNT=0
THROTTLE=0
page_index=0
final[0]="$source/posts?tags="
final[1]=""
final[2]=""
final[3]="&page="
final[4]=1
pagerange[0]=1
pagerange[1]=1
mode=1
source=""
name=""
booru_tag_crunch() {
for (( i=0 ; i < ${#tagarray[@]} ; i++ )); do
if [ ! "$i" = "0" ] && [ ! "$((i + 1))" = "${#tagarray}" ]; then
final[1]="${final[1]}+"
fi
tag="$(echo ${tagarray[i]} | sed -e 's|:|%3A|g')"
final[1]="${final[1]}${tag}"
if [[ ! "$tag" == *:* ]]; then
TAGCOUNT=$((TAGCOUNT + 1))
fi
done
}
booru_size_crunch() {
for (( i=0 ; i < ${#sizearray[@]} ; i++ )); do
if [[ "${sizearray[i]}" == pages:* ]] || [[ "${sizearray[i]}" == p* ]]; then
pagerange=($(echo "${sizearray[i]}" | tr -d 'p' | tr '-' ' '))
if [ "${pagerange[1]}" = "" ]; then
pagerange[1]=${pagerange[0]}
fi
final[4]=${pagerange[0]}
elif [[ "${sizearray[i]}" == limit:* ]] || [[ "${sizearray[i]}" == l* ]]; then
if [[ ! "${sizearray[i]}" == "limit:*" ]]; then
final[2]="+$(echo ${sizearray[i]} | sed 's|l|limit:|')"
else
final[2]="+${sizearray[i]}"
fi
final[2]="$(echo ${final[2]} | sed -e 's|:|%3A|g')"
elif [[ "${sizearray[i]}" == images:* ]] || [[ "${sizearray[i]}" == i* ]]; then
imagerange=($(echo "${sizearray[i]}" | tr -d 'i' | tr '-' ' '))
if [ "${imagerange[1]}" = "" ]; then
imagerange[1]=${imagerange[0]}
imagerange[0]=1
fi
mode=2
elif [[ "${sizearray[i]}" == "all" ]]; then
pagerange[0]=1
pagerange[1]=$MAX_INT
final[4]=${pagerange[0]}
fi
done
}
d_booru_page() {
url="$1"
name_out="$2"
FETCH_RESULT=1
while [ ! $FETCH_RESULT = 0 ]; do
fetch "$url" "$name_out"
if [ ! $FETCH_RESULT = 0 ]; then
FAILS=$((FAILS + 1))
message=" :<"
if (( $FAILS >= 5 )); then
e "\n[Booru] Too many failures. Aborting, sorry."
exit 1
fi
fi
if [ $THROTTLE = 1 ]; then
sleep 3s
fi
done
}
d_booru() {
if [ "$1" = "" ] || [ "$2" = "" ] || [ "$3" = "" ]; then
booru_syntax_help
fi
if [ -n "$(echo $1 | grep 'safebooru.donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "safebooru" ]; then
source="http://safebooru.donmai.us"
name="danbooru"
elif [ -n "$(echo $1 | grep 'donmai.us/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ] || [ "$1" = "danbooru" ]; then
source="http://danbooru.donmai.us"
name="danbooru"
else
source="$1"
o "[Booru] This type of Booru isn't specified as supported."
o "[Booru] It may work, depending on whether the autodetect picks up a valid scheme,"
o "[Booru] or it may completely bomb out. If it works, please submit a bug report"
o "[Booru] so I can add it to the supported list."
name="unknown"
fi
d_booru_${name}_init
shift
tagarray=($(echo "$1" | tr ',' ' '))
sizearray=($(echo "$2" | tr ',' ' '))
booru_tag_crunch
booru_size_crunch
o "[Booru] Info:"
o "[Booru]   Fetching URL $(echo ${final[0]}${final[1]}${final[2]}${final[3]} | tr -d ' ')"
o -n "[Booru]   Will download "
if [ $mode = 1 ]; then
if [ "${pagerange[0]}" = "${pagerange[1]}" ]; then
o "page ${pagerange[0]}."
else
TO=${pagerange[1]}
if [ $TO = $MAX_INT ]; then
TO="Max"
fi
o "pages ${pagerange[0]} to $TO."
fi
else
if [ "${imagerange[0]}" = "${imagerange[1]}" ]; then
o -n "image ${imagerange[0]}"
else
o -n "images ${imagerange[0]} to ${imagerange[1]}"
fi
o ", starting from page ${pagerange[1]}."
fi
if [ ! "${final[2]}" = "" ]; then
o "[Booru]   Requesting $(echo ${final[2]} | sed 's|+limit%3A||') images per page."
fi
if (( $TAGCOUNT >= 2 )); then
o "[Booru]   Warning, more than two normal tags. Some boorus don't like this."
fi
if [ ! "$3" = "" ]; then
tagdir="$3"
else
tagdir="$name - $1"
fi
o "[Booru]   Output to folder '$tagdir'."
mkdir -p "$tagdir"
cd "$tagdir"
o -n "[Booru] Fetching pages... "
o -n '' > meta.txt
for (( range=${pagerange[0]} ; range <= ${pagerange[1]} ; range++ )); do
ss "${range} -> ${pagerange[0]}/${pagerange[1]}"
d_booru_${name}_page "$range"
R=$?
lines=$(wc -l meta.txt | sed 's| .*||g')
if [ $mode = 2 ] && (( $lines > ${imagerange[1]} )); then
break
fi
if [ $R = 1 ]; then
break
fi
done
ss_done
lines=$(wc -l meta.txt | sed 's| .*||g')
if (( $lines > 100 )); then
o "[Booru] You're going to download a very large amount of images."
o "[Booru] I'm throttling to one fetch every 3s. You don't want to get ip banned, do you?"
THROTTLE=1
fi
o "[Booru] Downloading images and metadata... "
DONE=0
LINES=$(wc -l meta.txt | sed 's| .*||g')
while read meta_id; do
SKIP=0
if [ $mode = 2 ]; then
if (( $DONE < ${imagerange[0]} )); then
SKIP=1
fi
if (( $DONE > ${imagerange[1]} )); then
break
fi
TOTAL="$RANGE"
else
TOTAL="$LINES"
fi
ss "${DONE} -> ${LINES}"
d_booru_${name}_data "$meta_id"
DONE=$((DONE + 1))
done < meta.txt
rm meta.txt
ss_done
o "[Booru] Done with download. If you don't care about the metadata, it is no longer needed."
}
s_booru() {
exit 1
}
auto() {
for module in ${MODS[@]}; do
a_${module} "$@"
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
d_${module} "$@"
exit 0
fi
done
}
batch() {
IFS=$'\n' read -d '' -r -a LINES < $1
NEW=""
for chunk in "${LINES[@]}"; do
NEW="$NEW$0 $chunk ;"
done
$NEW
}
autobatch() {
IFS=$'\n' read -d '' -r -a LINES < $1
NEW=""
for chunk in "${LINES[@]}"; do
NEW="$NEW$0 auto $chunk ;"
done
$NEW
}
scrape() {
for module in ${MODS[@]}; do
a_${module} "$@"
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
s_${module} "$@"
exit 0
fi
done
}
mod_login() {
for module in ${MODS[@]}; do
if [ "$1" = "$module" ]; then
if [ "$( eval echo \$${module}_uselogin )" = "1" ]; then
if [ ! "$3" = "" ]; then
o "[Warn] Caveat; you probably need to wipe your shell history now. Try not to do it like this."
username="$2"
password="$3"
elif [ ! "$2" = "" ]; then
username="$2"
n "[$module] Password for $username (will not echo): "
read -s password
else
n "[$module] $username for site: "
read username
n "[$module] Password for $username (will not echo): "
read -s password
fi
login_${module} "$username" "$password"
exit 0
else
o "$module does not need login."
exit 0
fi
fi
done
o "No such module."
exit 1
}
help() {
type 1
e "Usage:"
type 0
n "     $0     "
color 3
n "OPERATION     "
color 5
e "[PARAMS]"
type 0
type 1
e "Operations:"
type 0
color 3
e "     auto (a)"
type 0
e "          Chooses module based on URL"
color 3
e "     batch (l)"
type 0
e "          Takes a file with a list of types and URLs"
color 3
e "     autobatch (b)"
type 0
e "          Takes a file with URLs which will be run with auto."
color 3
e "     scrape (s)"
type 0
e "          Will take a manga's page and scrape chapters to"
e "          a file named batch.txt"
color 3
e "     login (u)"
type 0
e "          Pass the module name, your username and password."
e "          This will generate a cookie jar as ./cookiejar"
o ""
e "     You can also specify a module name followed by"
e "     the URL instead of using the auto-detect."
o ""
e "     If you don't specify an operation and pass only a URL"
e "     then we assume you want auto (a)."
type 1
e "Download Modules:"
type 0
for mod in "${MODS[@]}"; do
longname=$(temp=\$${mod}_l && eval echo $temp)
url=$(temp=\$${mod}_u && eval echo $temp)
broke=$(temp=\$${mod}_state && eval echo $temp)
filter=$(temp=\$${mod}_filt && eval echo $temp)
note=$(temp=\$${mod}_note && eval echo $temp)
n "     Module Name:                "
color 3
e "$mod"
type 0
n "          Long Name:             "
color 4
e "$longname"
type 0
n "          Site(s) Used with:     "
color 5
e "$url"
type 0
type 0
n "          Site(s) Current state: "
if [ "$broke" = "1" ]; then
color 2
e "Works"
elif [ "$broke" = "2" ]; then
color 3
e "InDev"
else
color 1
e "Broken"
fi
if [ ! "$note" = "" ]; then
type 0
e "          Site Note:             $note"
fi
type 0
if [ "$filter" = "1" ]; then
e "          Supports filters for scrape"
fi
o ""
done
type 1
e "Misc Info"
type 0
e "     If you see an emote in the output, it means we had to deal"
e "     with a retrieval quirk."
e "\n     [ :/ ]       Given GZip'd data even though we said it wasn't"
e "                  supported in the GET."
type 2
e "                  This happens frequently with batoto when doing"
e "                  multiple fetches. :/"
type 0
e "\n     [ :( ]       Site didn't respond and so the DL failed"
e "                  We had to revert to a fallback method."
e "\n     [ >:( ]      Too many normal requests failed; we reverted"
e "                  to using entirely the fallback method."
o ""
e "     Some modules accept an extra parameter. Usually, this"
e "     is a filter. Try values like 'English' or 'French'."
type 1
e "System Tools"
type 0
if [ ! "$_wget" = "" ]; then
e "     wget:                $_wget"
fi
if [ ! "$_curl" = "" ]; then
e "     curl:                $_curl"
fi
if [ ! "$_aria" = "" ]; then
e "     aria2c:              $_aria"
fi
n "     Will use:            "
if [ $_FETCHTOOL = 1 ]; then
n "wget"
if [ $_BUSYBOX = 1 ]; then
n ", busybox"
else
n ""
fi
elif [ $_FETCHTOOL = 2 ]; then
n "curl"
elif [ $_FETCHTOOL = 3 ]; then
n "aria2c"
else
n "no tool. Install one of: "
fi
o " (wget, curl, aria2c)"
n "     Check broken images: "
if [ $_CHECK_VALID = 1 ]; then
if [ -f $_identify ]; then
n "imagemagick ($_identify)"
fi
o ""
else
o "no"
fi
n "     Color:               "
if [ $_COLOR = 1 ]; then
color 1
n "y"
color 2
n "e"
color 3
n "s"
color 4
n " "
n "("
color 5
n "d"
color 6
n "u"
color 1
n "m"
color 2
n "b"
color 3
o ")"
type 0
elif [ $_COLOR = 2 ]; then
color 1
n "y"
color 2
n "e"
color 3
n "s"
color 4
n " "
n "("
color 5
n "t"
color 6
n "p"
color 1
n "u"
color 2
n "t"
color 3
o ")"
type 0
else
o "no (TERM='$TERM')"
fi
type 1
n "License / Info"
type 0
o ""
e "     Copyright (C) 2015     Jon Feldman/@chaoskagami"
o ""
e "     This program is free software: you can redistribute it and/or modify"
e "     it under the terms of the GNU General Public License as published by"
e "     the Free Software Foundation, either version 3 of the License, or"
e "     (at your option) any later version."
o ""
e "     This program is distributed in the hope that it will be useful,"
e "     but WITHOUT ANY WARRANTY; without even the implied warranty of"
e "     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the"
e "     GNU General Public License for more details."
o ""
e "     You should have received a copy of the GNU General Public License"
e "     along with this program.  If not, see <http://www.gnu.org/licenses/>"
o ""
e "     The latest version of scangrab can always be found at the github"
e "     page here: https://github.com/chaoskagami/scangrab"
o ""
e "     Build: $branch, $rev"
}
if [ "$1" = "auto" -o "$1" = "a" ]; then
shift
auto "$@"
elif [ "$1" = "batch" -o "$1" = "l" ]; then
shift
batch "$@"
elif [ "$1" = "autobatch" -o "$1" = "b" ]; then
shift
autobatch "$@"
elif [ "$1" = "scrape" -o "$1" = "s" ]; then
shift
scrape "$@"
elif [ "$1" = "login" -o "$1" = "u" ]; then
shift
mod_login "$@"
else
MATCH=""
for module in ${MODS[@]}; do
if [ "$1" = "$module" ]; then
shift
d_${module} "$@"
exit 0
fi
done
auto "$@"
help
fi
