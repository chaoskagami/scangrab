#!/bin/bash
o() {
echo "$@"
}
n() {
echo -ne "$@"
}
e() {
echo -e "$@"
}
B="\b\b\b\b\b\b\b\b\b\b"
COLOR=0
_tput=$(which tput 2>/dev/null)
if [ "$TERM" = "xterm" ] || [ "$TERM" = "linux" ] || [ "$TERM" = "screen" ]; then
COLOR=1
if [ -f $_tput ]; then
COLOR=2
fi
fi
type() {
if [ $COLOR = 1 ]; then
n "\x1b[$1m"
elif [ $COLOR = 2 ]; then
if [ $1 = 0 ]; then
tput sgr0
elif [ $1 = 1 ]; then
tput bold
elif [ $1 = 2 ]; then
tput dim
fi
fi
}
color() {
if [ $COLOR = 1 ]; then
n "\x1b[3$1m"
elif [ $COLOR = 2 ]; then
tput setaf $1
fi
}
cbz_make() {
n "[Post] Making CBZ..."
if [ ! -d "$1" ]; then
e "\n[Error] Not a folder. Something went wrong."
exit 1
fi
if [ "$(ls "$1")" = "" ]; then
o "[Error] No files? Download failed."
exit 1
fi
zip -r "$1.zip" "$1" > /dev/null 2>&1
mv "$1.zip" "$1.cbz" > /dev/null 2>&1
n "$B$B\b[Post] Cleanup..."
rm -rf "$1"
e "$BFinished.    "
}
_identify=$(which identify 2>/dev/null)
CHECK_VALID=0
if [ -f "$_identify" ]; then
CHECK_VALID=1
fi
verify() {
if [ -f $_identify ]; then
$_identify -verbose -regard-warnings "$1" 2>&1 >/dev/null
_IDRES=$?
return $_IDRES
fi
return 0
}
_SC="|"
_NUM=0
_FIRST=1
message=""
ss() {
if [ $_FIRST = 1 ]; then
_FIRST=0
else
n "\b\b\b\b"
fi
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
if [ "$_SC" = "|" ]; then
_SC="/"
elif [ "$_SC" = "/" ]; then
_SC="-"
elif [ "$_SC" = "-" ]; then
_SC="\\"
elif [ "$_SC" = "\\" ]; then
_SC="|"
fi
_NUM=${#1}
n "[$1 $_SC]"
_STS=${#message}
n "$message"
for (( i=0 ; i < _STS ; i++ )); do
n "\b"
done
}
se() {
for (( i=0 ; i < _NUM ; i++ )); do
n "\b"
done
e "\b\b\b[OK]"
}
mimetype() {
o "$(file --mime-type "$1" | sed 's/.* //g')"
}
FETCH_RESULT="0"
FETCH_CMD=""
_FETCHTOOL=0
_BUSYBOX=0
fetch_detect() {
_wget="$(which wget 2>/dev/null)"
_curl="$(which curl 2>/dev/null)"
_aria="$(which aria2c 2>/dev/null)"
if [ ! "$_wget" = "" ]; then
common_opts=" --quiet --no-cache --user-agent=\"Mozilla/4.0\" -c -t 1 -T 10 --random-wait "
if [ ! "$($_wget --help 2>&1 | grep busybox)" = "" ]; then
o "[Warning] Your system wget is busybox, which can't actually do some things like reject cache and retry."
common_opts=" -q -c -U \"Mozilla/4.0\""
_BUSYBOX=1
fi
FETCH_CMD="$_wget $common_opts"
_FETCHTOOL=1
else
if [ ! "$_curl" = "" ]; then
FETCH_CMD=$_curl
_FETCHTOOL=2
else
if [ ! "$_aria" = "" ]; then
FETCH_CMD=$_aria
_FETCHTOOL=3
fi
fi
fi
return $FETCH_RESULT
}
COOKIES=0
COOKIEJAR=$(pwd)/cookiejar
s_login() {
if [ $_FETCHTOOL = 1 ]; then
_CMD="$FETCH_CMD --post-data='$1=$3&$2=$4' \"$5\"  --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies -O /dev/null"
elif [ $_FETCHTOOL = 2 ]; then
_CMD="$FETCH_CMD -d '$1=$3&$2=$4' $5 -b $COOKIEJAR -c $COOKIEJAR >/dev/null"
elif [ $_FETCHTOOL = 3 ]; then
_CMD="$FETCH_CMD $5 --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR -o/dev/null"
fi
eval " $_CMD" 2>/dev/null
FETCH_RESULT=$?
}
fetch() {
if [ $_FETCHTOOL = 1 ]; then
_CMD="$FETCH_CMD \"$1\""
if [ $COOKIES = 1 ]; then
_CMD="$_CMD --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR --keep-session-cookies"
fi
if [ "$2" = "" ]; then
_CMD="$_CMD -O $(basename "$1")"
elif [ "$2" = "-" ]; then
_CMD="$_CMD -O -"
else
_CMD="$_CMD -O \"$2\""
fi
elif [ $_FETCHTOOL = 2 ]; then
_CMD="$FETCH_CMD $1"
if [ $COOKIES = 1 ]; then
_CMD="$_CMD -b $COOKIEJAR -c $COOKIEJAR"
fi
if [ "$2" = "" ]; then
_CMD="$_CMD > $(basename "$1")"
elif [ "$2" = "-" ]; then
_CMD="$_CMD"
else
_CMD="$_CMD > \"$2\""
fi
elif [ $_FETCHTOOL = 3 ]; then
_CMD="$FETCH_CMD $1"
if [ $COOKIES = 1 ]; then
_CMD="$_CMD --load-cookies=$COOKIEJAR --save-cookies=$COOKIEJAR"
fi
if [ "$2" = "" ]; then
_CMD="$_CMD -o $(basename "$1")"
elif [ "$2" = "-" ]; then
_CMD="$_CMD -o -"
else
_CMD="$_CMD -o \"$2\""
fi
fi
eval " $_CMD" 2>/dev/null
FETCH_RESULT=$?
MIME="$(mimetype "$_FILE")"
if [ "$MIME" = "image/jpeg" ] || [ "$MIME" = "image/png" ] || [ "$MIME" = "image/gif" ] || [ "$MIME" = "image/bmp" ]; then
verify "$_FILE"
VALID=$?
if [ ! $VALID = 0 ]; then
o "[WARN] File '$_FILE' is corrupted."
fi
fi
return $FETCH_RESULT
}
entity_to_char() {
sed \
-e "s/&#32;/ /g" \
-e "s/&nbsp;/ /g" \
-e "s/&#33;/\!/g" \
-e "s/&#34;/\"/g" \
-e "s/&#35;/\#/g" \
\
-e "s/&#36;/\$/g" \
-e "s/&#37;/\%/g" \
-e "s/&amp;/\&/g" \
-e "s/&#38;/\&/g" \
-e "s/&#39;/'/g" \
\
-e "s/&#40;/\(/g" \
-e "s/&#41;/\)/g" \
-e "s/&#42;/\*/g" \
-e "s/&#43;/\+/g" \
-e "s/&#44;/\,/g" \
\
-e "s/&#45;/\-/g" \
-e "s/&#46;/\./g" \
-e "s/&#58;/\:/g" \
-e "s/&#59;/\;/g" \
-e "s/&lt;/\</g" \
\
-e "s/&#60;/\</g" \
-e "s/&gt/\>/g" \
-e "s/&#61;/\>/g" \
-e "s/&#63;/\?/g" \
-e "s/&#64;/\@/g" \
\
-e "s/&#91;/\[/g" \
-e "s/&#92;/\\\\/g" \
-e "s/&#93;/\]/g" \
-e "s/&#94;/\^/g" \
-e "s/&#95;/\_/g" \
\
-e "s/&#123;/\{/g" \
-e "s/&#124;/\|/g" \
-e "s/&#125;/\}/g" \
-e "s/&#126;/\~/g" \
-e "s/&yen;/¥/g" \
\
-e "s/&#165;/¥/g" \
-e "s/&sup2;/²/g" \
-e "s/&#178;/²/g" \
-e "s/&sup3;/³/g" \
-e "s/&#179;/³/g" \
\
-e "s/&frac14;/¼/g" \
-e "s/&#188;/¼/g" \
-e "s/&frac12;/½/g" \
-e "s/&#189;/½/g" \
-e "s/&frac34;/¾/g" \
\
-e "s/&#190;/¾/g" \
-e "s/&spades;/♠/g" \
-e "s/&#9824;/♠/g" \
-e "s/&clubs;/♣/g" \
-e "s/&#9827;/♣/g" \
\
-e "s/&hearts;/♥/g" \
-e "s/&#9829;/♥/g" \
-e "s/&diams;/♦/g" \
-e "s/&#9830;/♦/g" \
\
-e "s/|/-/g" \
-e "s|/|-|g"
}
reverse_lines() {
readarray -t LINES
for (( I = ${#LINES[@]}; I; )); do
o "${LINES[--I]}"
done
}
MODS=(batoto dynsc eh fakku foolsl mpark mread niconicoseiga)
batoto_l="Batoto"
batoto_u="http://bato.to/"
batoto_state=1
batoto_filt=1
batoto_note="Their page template is highly unstable."
a_batoto() {
if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_batoto() {
folder="$(fetch "$1" "-" | grep -C0 "<title>" | sed -e "s/^[[:space:]]*<title>//" -e "s/ Page .*//" -e "s/^[[:space:]]*//" -e "s/[[:space:]]*$/\n/" | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
CUR=0
PAGES=0
RET=0
base="$1"
if [ ! "${base:${#base}-1}" = "/" ]; then
base="${base}/"
fi
o -n "[Batoto] Downloading '$folder' "
while [ "$RET" = "0" ]; do
CUR=$(( CUR + 1 ))
fetch "${base}${CUR}" "$CUR.htm"
if [ "$(mimetype $CUR.htm)" = "application/x-gzip" ]; then
mv $CUR.htm $CUR.htm.gz
gunzip $CUR.htm.gz
message=" :/"
fi
img="$(grep -C0 'z-index: 1003' $CUR.htm | sed -e 's/^[[:space:]]*<img src="//g' -e 's/".*$//g')"
ext="${img##*.}"
fetch "$img" "${CUR}_${folder}.${ext}"
RET=$?
rm $CUR.htm
ss "$CUR"
done
PAGES=$(( CUR - 1 ))
se
cd ..
cbz_make "$folder"
}
s_batoto() {
notice_batoto
o -n "[Batoto] Scraping Chapters..."
fetch "$1" scrape.htm
if [ "$(mimetype scrape.htm)" = "application/x-gzip" ]; then
mv $CUR.htm $CUR.htm.gz
gunzip $CUR.htm.gz
message=" :/"
fi
grep -A 2 'Sort:' scrape.htm >> batch.txtr
sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr
sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr
sed -i "s|<a href=\"||g" batch.txtr
sed -i "s|\" title=.*||g" batch.txtr
sed -i '/^[[:space:]]*$/d' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i 's|^--$||g' batch.txtr
sed -i '/^$/d' batch.txtr
cat batch.txtr | reverse_lines > batch.txtf
if [ "$2" = "" ]; then
sed -ni '0~2p' batch.txtf
cat batch.txtf >> batch.txt
else
n "$B$B\b Applying Language Filter '$2'..."
grep -A 1 "$2" batch.txtf > batch.txtf2
sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2
sed -i '/^$/d' batch.txtf2
sed -ni '0~2p' batch.txtf2
cat batch.txtf2 >> batch.txt
fi
rm scrape.htm batch.txtr batch.txtf*
en "$B$B$B\b"
for ((n=0;n < ${#2}; n++)); do
en '\b'
done
e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
dynsc_l="Dynasty Scans"
dynsc_u="http://dynasty-scans.com/"
dynsc_state=1
dynsc_filt=0
a_dynsc() {
if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_dynsc() {
PAGEDATA="$(fetch "$1" "-")"
folder="$(echo "$PAGEDATA" | grep "<title>" | sed -e 's/<title>Dynasty Reader &raquo; //g' -e 's|</title>||g')"
mkdir -p "$folder"
cd "$folder"
PAGELIST="$(echo "$PAGEDATA" | grep "var pages")"
PAGETMP="$(echo $PAGELIST | sed -e "s/\"image\"\://g" -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" -e "s/\}\]/\)/g" -e "s/{//g" -e "s/}//g" -e "s/;//g" -e "s/ //g" -e "s/varpages=\[/pages=\(/g" -e "s/,/ /g")"
eval "$PAGETMP"
o -n "[DynastyScans] Downloading '$folder' "
CUR=0
for image in "${pages[@]}"; do
fetch "http://dynasty-scans.com$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
se
cd ..
cbz_make "$folder"
}
s_dynsc() {
o -n "[DynastyScans] Scraping Chapters..."
fetch "$1" "-" | 			\
grep 'class="name"' | 		\
sed -e 's|^.*href="||g' 	\
-e 's|" class=.*||g' 	\
-e "s/^[[:space:]]*//" 	\
-e "s/[[:space:]]*$//" 	\
-e "s|^|http://dynasty-scans.com|g" >> batch.txt
e "$B$B$B\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
eh_l="E-Hentai"
eh_u="http://e-hentai.org/"
eh_state=1
eh_filt=0
eh_note="H@H is an unreliable CDN. Also, they count images by logged in IP."
a_eh() {
if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_eh() {
sitepage=$1
fetch "${sitepage}/?nw=always" tmp.1
folder="$(cat tmp.1 | grep 'title>' - | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/ - E-Hentai Galleries//g' | sed 's|/|_|g' | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
DATA=$(cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/g' | sed 's/"><img alt.*//g' | grep 'url:')
rm ../tmp.1
eval "urls=(`echo $DATA | tr '\n' ' ' | sed 's/url://g'`)"
page="${urls[0]}"
o -n "[e-h] Downloading '$folder' "
doneyet=0
CDNFAIL=0
CUR=0
while [ $doneyet == 0 ]; do
get=($(fetch "$page" "-" | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'keystamp'))
extra=""
if [ "${get[0]}" == "$page" ]; then
doneyet=1
fi
if [ $CDNFAIL -ge 3 ]; then
FETCH_RESULT=1
else
fetch "${get[1]}" "$extra"
fi
if [ ! $FETCH_RESULT = 0 ]; then
message=" :("
CDNFAIL=$((CDNFAIL + 1))
if [ $CDNFAIL -ge 3 ]; then
message=" >:("
fi
notload=$(fetch "$page" "-" | grep 'nl(' | sed -e "s|^.*nl('||g" -e "s|'.*$||g")
get=($(fetch "$page?nl=${notload}" "-" | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'image.php' | entity_to_char ))
extra="$(echo ${get[1]} | sed 's/.*n=//')"
fetch "${get[1]}" "$extra"
if [ ! $FETCH_RESULT = 0 ]; then
message=" >:["
else
CUR=$(( CUR + 1 ))
fi
else
CUR=$(( CUR + 1 ))
fi
if [ ! "${get[0]}" = "" ]; then
page="${get[0]}"
fi
ss "$CUR"
done
se
cd ..
cbz_make "$folder"
}
s_eh() {
e "[e-h] This isn't supported, considering there's really zero categorization here."
}
fakku_l="FAKKU"
fakku_u="https://fakku.net/"
fakku_state=1
fakku_filt=0
fakku_note="They've been shuffling things around recently."
a_fakku() {
if [ -n "$(echo $1 | grep 'fakku.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//')" ]; then
return 1
fi
return 0
}
d_fakku() {
PAGE="$(fetch "$1/read" "-")"
PAGEDATA="$(echo "$PAGE" | grep "window.params.thumbs =" | sed -e 's/\\\//\//g' -e 's/\];/)/g' -e 's/window.params.thumbs = \[/pages=(/g' -e 's/\.thumb//g' -e 's/","/" "/g' -e 's/\/thumbs\//\/images\//g')"
DATA="$(printf "$PAGEDATA")"
folder="$(echo "$PAGE" | grep '<title>' | sed -e 's/[[:space:]]*<title>Read //g' -e 's|</title>||g' | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
eval "$DATA"
o -n "[Fakku] Downloading '$folder' "
CUR=0
for image in "${pages[@]}"; do
fetch "https:$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
se
cd ..
cbz_make "$folder"
}
s_fakku() {
o -n "[Fakku] Scraping Chapters..."
fetch "$1" scrape.htm
grep 'class="content-title"' scrape.htm > batch.txtf
sed -i 's|^.*href="||g' batch.txtf
sed -i 's|" title=.*||g' batch.txtf
sed -i "s/^[[:space:]]*//" batch.txtf
sed -i "s/[[:space:]]*$//" batch.txtf
sed -i "s|^|https://fakku.net|g" batch.txtf
cat batch.txtf >> batch.txt
rm scrape.htm batch.txtf
e "$B$B$B\b\b[Fakku] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
foolsl_l="FoolSlide"
foolsl_u="Generic, n/a."
foolsl_state=1
foolsl_filt=0
a_foolsl() {
if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_foolsl() {
LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`
o -n "[FoolSlide] Attempting Lazy Download..."
FAILED=0
fetch "$LAZYURL" || export FAILED=1
if [ $FAILED = 1 ]; then
o "Requesting zip failed."
else
o "[OK]"
fi
}
s_foolsl() {
o -n "[Foolslide] Scraping Chapters..."
fetch "$1" "-" |							\
grep '<div class="title"><a href='			\
sed -e 's|<div class="title"><a href="||g'	\
-e 's|" title=.*||g'						\
-e "s/^[[:space:]]*//"						\
-e "s/[[:space:]]*$//"					  | \
reverse_lines >> batch.txt
e "$B$B$B\b\b[Foolslide] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
mpark_l="MangaPark"
mpark_u="http://mangapark.me/"
mpark_state=1
mpark_filt=0
a_mpark() {
if [ -n "`echo $1 | grep 'mangapark.me/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_mpark() {
sitepage="$1"
sitepage=`echo $sitepage | sed "s|/1$||g" | sed "s|/3-1$||g" | sed "s|/6-1$||g" | sed "s|/10-1$||g"`
FETCH="$(fetch "$sitepage" "-")"
folder="$(echo "$FETCH" | grep '<title>' | sed -e 's/<title>//g' -e 's/ Online For Free.*$//g' -e 's/.* - Read //g')"
mkdir -p "$folder"
cd "$folder"
declare -a DATA
DATA=$(echo "$FETCH" | grep 'target="_blank"' - | sed -e '1d' -e 's|^[[:space:]]*<a.*target="_blank" href=||g' -e "s/ title=.*$//" -e "s/\"//g"| tr '\n' ' ')
o -n "[Mangapark] Downloading '$folder' "
CUR=0
for image in ${DATA[@]}; do
fetch "$image"
ss "$CUR"
CUR=$(( CUR + 1 ))
done
se
cd ..
cbz_make "$folder"
}
s_mpark() {
o -n "[Mangapark] Scraping Chapters..."
fetch "$1" scrape.htm
grep 'class="ch sts"' scrape.htm > batch.txtr
sed -i 's|^.*href="||g' batch.txtr
sed -i 's|">.*||g' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i "s|^|http://mangapark.com|g" batch.txtr
sed -i "s|/1$||g" batch.txtr
sed -i "s|/3-1$||g" batch.txtr
sed -i "s|/6-1$||g" batch.txtr
sed -i "s|/10-1$||g" batch.txtr
tac batch.txtr >> batch.txt || cat batch.txtr >> batch.txt
rm scrape.htm batch.txtr
e "$B$B$B\b\b[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
mread_l="Mangareader"
mread_u="http://www.mangareader.net/"
mread_state=0
mread_filt=0
a_mread() {
if [ -n "`echo $1 | grep 'mangareader.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_mread() {
folder="$(echo $1 | sed -e "s|http\:\/\/||g" | sed -e "s|www\.mangareader\.net\/||g" | sed -e "s|\/|-|g" | entity_to_char)"
mkdir -p "$folder"
cd "$folder"
o -n "[Mangareader] Blindly Downloading '$folder' "
IS_404=0
CUR=0
PAGES=0
while [ $IS_404 = 0 ]; do
CUR=$(( CUR + 1 ))
PAGEDATA=$(fetch "$1/$CUR" "$CUR.htm")
IS_404=$?
if [ $IS_404 = 0 ]; then
src=`grep '<div id="imgholder">' $CUR.htm | sed "s/^.*src=\"//g" | sed "s/\" alt=.*//g"`
o $src
EXT="${src##*.}"
fetch "$src" "$CUR.$EXT"
ss
fi
done
PAGES=$(( CUR - 1 ))
se
rm *.htm
cd ..
cbz_make "$folder"
}
s_mread() {
o "[MangaReader] NYI, sorry."
}
niconicoseiga_l="Niconico Seiga"
niconicoseiga_u="http://seiga.nicovideo.jp"
niconicoseiga_state=1
niconicoseiga_filt=0
niconicoseiga_note="Please login first, or make a cookiejar file."
niconicoseiga_loginreq=1
a_niconicoseiga() {
if [ -n "`echo $1 | grep 'seiga.nicovideo.jp' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
login_niconicoseiga() {
COOKIES=1
s_login "mail_tel" "password" "$1" "$2" "https://secure.nicovideo.jp/secure/login"
}
d_niconicoseiga() {
COOKIES=1
name="$(basename $1)"
fetch $1 "$name.htm"
fullname="$(cat $name.htm | grep '<meta property="og:title" content="' | sed -e 's|<meta property="og:title" content="||g' -e 's|".*||g' | entity_to_char)"
if [ ! -f "$name.htm" ]; then
o "[Niconico] No page found, aborting."
return 1
fi
o "[Niconico] Extracting image list..."
cat $name.htm | grep 'data-image-id' | sed -e 's|^[[:space:]]*data-image-id="||g' -e 's|"[[:space:]]*$||g' > $name.lst
if [ "$(cat $name.lst)" = "" ]; then
o "[Niconico] No images in page. Did you login?"
return 1
fi
mkdir -p "$fullname"
cd "$fullname"
o -n "[Niconico] Downloading '$fullname' (from $name) "
COUNT=0
while read ID; do
COUNT=$((COUNT + 1))
fetch "http://lohas.nicoseiga.jp/thumb/${ID}p" "${COUNT}.jpg"
ss $COUNT
done < ../$name.lst
se
cd ..
rm $name.lst $name.htm
cbz_make "$fullname"
}
s_niconicoseiga() {
o "[Niconico] This is not supported."
}
fetch_detect
auto() {
for module in ${MODS[@]}; do
eval a_$module $1
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
eval d_$module $1
exit 0
fi
done
}
batch() {
IFS=$'\n' read -d '' -r -a LINES < $1
NEW=""
for chunk in "${LINES[@]}"; do
NEW="$NEW$0 $chunk ;"
done
eval $NEW
}
autobatch() {
IFS=$'\n' read -d '' -r -a LINES < $1
NEW=""
for chunk in "${LINES[@]}"; do
NEW="$NEW$0 auto $chunk ;"
done
eval $NEW
}
scrape() {
for module in ${MODS[@]}; do
eval a_$module $1
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
eval s_$module $1 $2
exit 0
fi
done
}
mod_login() {
for module in ${MODS[@]}; do
o "$1" = "$module"
if [ "$1" = "$module" ]; then
if [ "$( eval echo \$${module}_loginreq )" = "1" ]; then
eval login_$module \"$2\" \"$3\"
exit 0
else
o "$module does not need login."
exit 0
fi
fi
done
o "No such module."
exit 1
}
help() {
type 1
e "Usage:"
type 0
n "     $0     "
color 3
n "OPERATION     "
color 5
e "[PARAMS]"
type 0
type 1
e "Operations:"
type 0
color 3
e "     auto (a)"
type 0
e "          Chooses module based on URL"
color 3
e "     batch (l)"
type 0
e "          Takes a file with a list of types and URLs"
color 3
e "     autobatch (b)"
type 0
e "          Takes a file with URLs which will be run with auto."
color 3
e "     scrape (s)"
type 0
e "          Will take a manga's page and scrape chapters to"
e "          a file named batch.txt"
color 3
e "     login (u)"
type 0
e "          Pass the module name, your username and password."
e "          This will generate a cookie jar as ./cookiejar"
o ""
e "     You can also specify a module name followed by"
e "     the URL instead of using the auto-detect."
o ""
e "     If you don't specify an operation and pass only a URL"
e "     then we assume you want auto (a)."
type 1
e "Download Modules:"
type 0
for mod in "${MODS[@]}"; do
longname=$(temp=\$${mod}_l && eval echo $temp)
url=$(temp=\$${mod}_u && eval echo $temp)
broke=$(temp=\$${mod}_state && eval echo $temp)
filter=$(temp=\$${mod}_filt && eval echo $temp)
note=$(temp=\$${mod}_note && eval echo $temp)
n "     Module Name:                "
color 3
e "$mod"
type 0
n "          Long Name:             "
color 4
e "$longname"
type 0
n "          Site(s) Used with:     "
color 5
e "$url"
type 0
type 0
n "          Site(s) Current state: "
if [ "$broke" = "1" ]; then
color 2
e "Works"
else
color 1
e "Broken"
fi
if [ ! "$note" = "" ]; then
type 0
e "          Site Note:             $note"
fi
type 0
if [ "$filter" = "1" ]; then
e "          Supports filters for scrape"
fi
o ""
done
type 1
e "Misc Info"
type 0
e "     If you see an emote in the output, it means we had to deal"
e "     with a retrieval quirk."
e "\n     [ :/ ]       Given GZip'd data even though we said it wasn't"
e "                  supported in the GET."
type 2
e "                  This happens frequently with batoto when doing"
e "                  multiple fetches. :/"
type 0
e "\n     [ :( ]       Site didn't respond and so the DL failed"
e "                  We had to revert to a fallback method."
e "\n     [ >:( ]      Too many normal requests failed; we reverted"
e "                  to using entirely the fallback method."
o ""
e "     Some modules accept an extra parameter. Usually, this"
e "     is a filter. Try values like 'English' or 'French'."
type 1
e "System Tools"
type 0
if [ ! "$_wget" = "" ]; then
e "     wget:                $_wget"
fi
if [ ! "$_curl" = "" ]; then
e "     curl:                $_curl"
fi
if [ ! "$_aria" = "" ]; then
e "     aria2c:              $_aria"
fi
n "     Will use:            "
if [ $_FETCHTOOL = 1 ]; then
n "wget"
if [ $_BUSYBOX = 1 ]; then
n ", busybox"
else
n ""
fi
elif [ $_FETCHTOOL = 2 ]; then
n "curl"
elif [ $_FETCHTOOL = 3 ]; then
n "aria2c"
else
n "no tool. Install one of: "
fi
o " (wget, curl, aria2c)"
n "     Check broken images: "
if [ $CHECK_VALID = 1 ]; then
if [ -f $_identify ]; then
n "imagemagick ($_identify)"
fi
o ""
else
o "no"
fi
n "     Color:               "
if [ $COLOR = 1 ]; then
color 1
n "y"
color 2
n "e"
color 3
n "s"
color 4
n " "
n "("
color 5
n "d"
color 6
n "u"
color 1
n "m"
color 2
n "b"
color 3
o ")"
type 0
elif [ $COLOR = 2 ]; then
color 1
n "y"
color 2
n "e"
color 3
n "s"
color 4
n " "
n "("
color 5
n "t"
color 6
n "p"
color 1
n "u"
color 2
n "t"
color 3
o ")"
type 0
else
o "no (TERM='$TERM')"
fi
type 1
n "License / Info"
type 0
o ""
e "     Copyright (C) 2015     Jon Feldman/@chaoskagami"
o ""
e "     This program is free software: you can redistribute it and/or modify"
e "     it under the terms of the GNU General Public License as published by"
e "     the Free Software Foundation, either version 3 of the License, or"
e "     (at your option) any later version."
o ""
e "     This program is distributed in the hope that it will be useful,"
e "     but WITHOUT ANY WARRANTY; without even the implied warranty of"
e "     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the"
e "     GNU General Public License for more details."
o ""
e "     You should have received a copy of the GNU General Public License"
e "     along with this program.  If not, see <http://www.gnu.org/licenses/>"
o ""
e "     The latest version of scangrab can always be found at the github"
e "     page here: https://github.com/chaoskagami/scangrab"
}
if [ "$1" = "auto" -o "$1" = "a" ]; then
auto $2 $3
elif [ "$1" = "batch" -o "$1" = "l" ]; then
batch $2 $3
elif [ "$1" = "autobatch" -o "$1" = "b" ]; then
autobatch $2 $3
elif [ "$1" = "scrape" -o "$1" = "s" ]; then
scrape $2 $3
elif [ "$1" = "login" -o "$1" = "u" ]; then
mod_login $2 $3 $4
else # Not a common operation - either invalid or a module-op.
MATCH=""
for module in ${MODS[@]}; do
if [ "$1" = "$module" ]; then
MATCH="d_$module $2"
fi
done
if [ "$MATCH" = "" ]; then # All checks failed. Usage~
if [ ! "$(echo "$1" | grep http)" = "" ]; then
auto $1 $2
else
help
fi
else # Module operation.
eval $MATCH
fi
fi
