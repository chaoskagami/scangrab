#!/bin/bash

function auto_batoto() {
	if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Batoto
		return 1
	fi

	return 0
}

function dl_batoto() {
	# Batoto requires a different strategy.
	# The URLs are not preloaded like the former, so the fetch one page done thing won't work.
	# Unfortunately, short of grabbing pages until an image 404's, there's no way of knowing when we're done.
	
	folder="`echo $1 | sed -re 's/^.+\///'`"
	mkdir -p $folder
	cd $folder

	IS_404=0
	CUR=0
	PAGES=0
	
	echo -n "[Batoto] Downloading '$folder' "

	while [ $IS_404 == 0 ]; do
		# Increment CUR.
		CUR=$(( CUR + 1 ))
	
		wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1/$CUR" -O $CUR.htm > /dev/null 2>&1
		
		# Batoto sometimes gives out gunzips. We need to account for that... =_=

		echo `file $CUR.htm` | grep 'gzip' - > /dev/null 2>&1

		RET=$?


		# Quirk: GZip. The server ignores our request for non-gz sometimes.
		if [ $RET == 0 ]; then # Yeah, got a gz.
			echo -ne "  :/\b\b\b\b"

			mv $CUR.htm $CUR.htm.gz
			gunzip $CUR.htm.gz
		fi

		grep -A 1 'z-index: 1002' $CUR.htm | tail -n1 > $CUR.tmp
	
		# Edit magic.
		sed -i "s/[[:space:]]*<img //g" $CUR.tmp
		sed -i "s/ style=.*//g" $CUR.tmp

		# Load 'src'
		source $CUR.tmp
	
		# Get extension of src.
		EXT="${src##*.}"

		# If this 404's, wget will return non-zero. Thus, loop breaks.
		wget --no-cache -nc -t 0 -w 30 "$src" -O $CUR.$EXT > /dev/null 2>&1
		IS_404=$?
	
		spinner
	done

	PAGES=$(( CUR - 1 ))

	done_spin

	# Clean.

	rm *.htm
	rm *.tmp

	cd ..

	cbz_make $folder
}

function scrape_batoto() {

	echo -n "[Batoto] Scraping Chapters..."

	wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1" -O scrape.htm > /dev/null 2>&1
		
	# Batoto sometimes gives out gunzips. We need to account for that... =_=

	echo `file scrape.htm` | grep 'gzip' - > /dev/null 2>&1

	RET=$?

	# Quirk: GZip. The server ignores our request for non-gz sometimes.
	if [ $RET == 0 ]; then # Yeah, got a gz.
		echo -ne "  :/\b\b\b\b"

		mv scrape.htm scrape.htm.gz
		gunzip scrape.htm.gz
	fi

	grep '<img src="http://bato.to/book_open.png" style="vertical-align:middle;"' scrape.htm >> batch.txt

	sed -i "s|<a href=\"||g" batch.txt
	sed -i "s|\"><img src=.*||g" batch.txt
	sed -i "s/^[[:space:]]*//" batch.txt
	sed -i "s/[[:space:]]*$//" batch.txt

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Batoto] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
