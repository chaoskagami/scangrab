auto_dynsc() {
	if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Dynasty Scans.
		return 1
	fi

	return 0
}

dl_dynsc() {
	
	# Now loop-de-loop. First, make a decent name. Dynasty always has
	# a short-title at the end of the URL.

	folder="`echo $1 | sed -re 's/^.+\///'`"
	mkdir -p $folder
	cd $folder

	PAGEDATA=`wget --quiet $1 -O - | grep "var pages" -`

	# This set of seds cuts up the pagelist in a manner
	# that makes it identical to a bash array.
	# So we're essentially modifying the webpage into a dl-script.
	# Cool, eh?

	PAGETMP=`echo $PAGEDATA | sed -e "s/\"image\"\://g" | sed -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" | sed -e "s/\}\]/\)/g" | sed -e "s/{//g" | sed -e "s/}//g" | sed -e "s/;//g" | sed -e "s/ //g" | sed -e "s/varpages=\[/pages=\(/g" | sed -e "s/,/ /g"`

	# One possible nasty. Spaces.
	# sed -i "s/\%20/ /g" tmp.1

	# Load in the array.
	eval $PAGETMP

	echo -n "[DynastyScans] Downloading '$folder' "

	for image in "${pages[@]}"; do
		wget --no-cache -nc -t 0 -w 30 "http://dynasty-scans.com$image" > /dev/null 2>&1
		spinner
	done

	done_spin
	
	cd ..

	cbz_make $folder
}

scrape_dynsc() {
	echo -n "[DynastyScans] Scraping Chapters..."

	wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1" -O scrape.htm > /dev/null 2>&1

	grep 'class="name"' scrape.htm > batch.txtf

	sed -i 's|^.*href="||g' batch.txtf
	sed -i 's|" class=.*||g' batch.txtf
	sed -i "s/^[[:space:]]*//" batch.txtf
	sed -i "s/[[:space:]]*$//" batch.txtf

	# URLS are local.
	sed -i "s|^|http://dynasty-scans.com|g" batch.txtf

	# Luckily, dynasty is one of the few sites that does ascending rather than descending order (txtf = forward, txtr = reversed)
	cat batch.txtf >> batch.txt

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtf

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
