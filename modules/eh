eh_longname="E-Hentai"
eh_url="http://e-hentai.org/"

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Mangareader
		return 1
	fi

	return 0
}

dl_eh() {

	sitepage=$1

  wget --quiet $sitepage -O tmp.1

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
  # the last sed deals with names like fate/stay night which are invalid
	folder="`cat tmp.1 | grep 'title>' - | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/E-Hentai Galleries/Rip/g' | sed 's|/|_|g'`"
	mkdir -p "$folder"
	cd "$folder"
	
	DATA=`cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/g' | sed 's/"><img alt.*//g' | grep 'url:'`
  rm ../tmp.1
	
	echo "$DATA"

  eval "urls=(`echo $DATA | tr '\n' ' ' | sed 's/url://g'`)"

  page="${urls[0]}"

	echo -n "[e-h] Downloading '$folder' "

  doneyet=0

	while [ $doneyet == 0 ]; do

    get=($(wget --quiet --no-cache -nc -t 0 -w 30 "$page" -O - | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'keystamp' -))

     extra=""

    if [ "${get[1]}" == "" ]; then
       get=($(wget --quiet --no-cache -nc -t 0 -w 30 "$page" -O - | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 '.php?f=' -))
       extra="-O $(echo ${get[1]} | sed 's/.*n=//')"
    fi

    #echo ${get[@]}

    if [ "${get[0]}" == "$page" ]; then
          doneyet=1
    fi

    echo wget --no-cache --quiet -nc -t 0 -w 30 "${get[1]}" $extra
    wget --no-cache --quiet -nc -t 0 -w 30 "${get[1]}" $extra

    if [ ! "${get[0]}" == "" ]; then
       page="${get[0]}"
    fi

    echo "next $page"

		spinner
	done
	
	done_spin

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	echo -n "[Mangapark] Scraping Chapters..."

	wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1" -O scrape.htm > /dev/null 2>&1

	grep 'class="ch sts"' scrape.htm > batch.txtr

	sed -i 's|^.*href="||g' batch.txtr
	sed -i 's|">.*||g' batch.txtr
	sed -i "s/^[[:space:]]*//" batch.txtr
	sed -i "s/[[:space:]]*$//" batch.txtr

	# URLS are local.
	sed -i "s|^|http://mangapark.com|g" batch.txtr

	# We need a specific type of URL - the 'all on one page' type. Remove specifiers.
	sed -i "s|/1$||g" batch.txtr
	sed -i "s|/3-1$||g" batch.txtr
	sed -i "s|/6-1$||g" batch.txtr
	sed -i "s|/10-1$||g" batch.txtr

	# Lines are reverse order. tac.
	# If whatever we're using has no tac, you're stuck with reverse order.
	tac batch.txtr >> batch.txt || cat batch.txtr >> batch.txt

	# We've scraped a batch file from the URL list. Clean up.
	rm scrape.htm batch.txtr

	echo -e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
