eh_longname="E-Hentai"
eh_url="http://e-hentai.org/"

auto_eh() {
	if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
		# Mangareader
		return 1
	fi

	return 0
}

dl_eh() {
	echo -e "[e-h] Warning, this is experimental and subject to random"
	echo -e "[e-h] failures. I think some of the 'nodes' don't respond"
	echo -e "[e-h] properly, so if an image fails; open a browser and click"
	echo -e "[e-h] the image not working button repeatedly. You should"
	echo -e "[e-h] get the fallback server next try."

	sitepage=$1

  wget --quiet $sitepage -O tmp.1

	# Unfortunately e-h has shit system. Time to code page extraction
	# I'm using pipes for clarity.
  # the last sed deals with names like fate/stay night which are invalid
	folder="`cat tmp.1 | grep 'title>' - | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/E-Hentai Galleries/Rip/g' | sed 's|/|_|g'`"
	mkdir -p "$folder"
	cd "$folder"
	
	DATA=`cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/g' | sed 's/"><img alt.*//g' | grep 'url:'`
  rm ../tmp.1
	
#	echo "$DATA"

  eval "urls=(`echo $DATA | tr '\n' ' ' | sed 's/url://g'`)"

  page="${urls[0]}"

	echo -n "[e-h] Downloading '$folder' "

  doneyet=0

	while [ $doneyet == 0 ]; do

    get=($(wget --quiet --no-cache -nc -t 0 -w 30 "$page" -O - | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'keystamp' -))

     extra=""

    if [ "${get[1]}" == "" ]; then
       get=($(wget --quiet --no-cache -nc -t 0 -w 30 "$page" -O - | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 '.php?f=' -))
       extra="-O $(echo ${get[1]} | sed 's/.*n=//')"
    fi

    # echo ${get[@]}

    if [ "${get[0]}" == "$page" ]; then
          doneyet=1
    fi

#    echo wget --no-cache --quiet -nc -t 0 -w 30 "${get[1]}" $extra
    wget --no-cache --quiet -nc -t 0 -w 30 "${get[1]}" $extra

    if [ ! "${get[0]}" == "" ]; then
       page="${get[0]}"
    fi

#   echo "next $page"

		spinner
	done
	
	done_spin

	cd ..

	cbz_make "$folder"
}

scrape_eh() {
	echo -n "[e-h] This isn't supported, considering there's really zero categorization here."
}
