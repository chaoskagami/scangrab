#!/bin/bash
o() {
echo "$@"
}
n() {
echo -ne "$@"
}
e() {
echo -e "$@"
}
type() {
n "\x1b[$1m"
}
color() {
n "\x1b[3$1m"
}
cbz_make() {
n "[Post] Making CBZ..."
zip -r "$1.zip" "$1" > /dev/null 2>&1
mv "$1.zip" "$1.cbz" > /dev/null 2>&1
n "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Post] Cleanup..."
rm -rf "$1"
e "\b\b\b\b\b\b\b\b\b\bFinished.    "
}
_SC="|"
_FIRST=1
ss() {
if [ $_FIRST = 1 ]; then
n "[$_SC]"
_FIRST=0
else
n "\b\b\b"
if [ "$_SC" = "|" ]; then
_SC="/"
elif [ "$_SC" = "/" ]; then
_SC="-"
elif [ "$_SC" = "-" ]; then
_SC="\\"
elif [ "$_SC" = "\\" ]; then
_SC="|"
fi
n "[$_SC]"
fi
}
se() {
e "\b\b\b[OK]"
}
MODS=(batoto dynsc eh fakku foolsl mpark mread)
batoto_l="Batoto"
batoto_u="http://bato.to/"
a_batoto() {
if [ -n "`echo $1 | grep 'bato.to/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_batoto() {
o "[Batoto] If this so happens to fail, they changed their page generator again."
folder="`echo $1 | sed -re 's/^.+\///'`"
mkdir -p $folder
cd $folder
IS_404=0
CUR=0
PAGES=0
o -n "[Batoto] Downloading '$folder' "
while [ $IS_404 = 0 ]; do
CUR=$(( CUR + 1 ))
wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1/$CUR" -O $CUR.htm > /dev/null 2>&1
o `file $CUR.htm` | grep 'gzip' - > /dev/null 2>&1
RET=$?
if [ $RET = 0 ]; then # Yeah, got a gz.
n "  :/\b\b\b\b"
mv $CUR.htm $CUR.htm.gz
gunzip $CUR.htm.gz
fi
grep -A 1 'z-index: 1002' $CUR.htm | tail -n1 > $CUR.tmp
sed -i "s/[[:space:]]*<img //g" $CUR.tmp
sed -i "s/ style=.*//g" $CUR.tmp
. $CUR.tmp
EXT="${src##*.}"
wget --no-cache -nc -t 0 -w 30 "$src" -O $CUR.$EXT > /dev/null 2>&1
IS_404=$?
ss
done
PAGES=$(( CUR - 1 ))
se
rm *.htm
rm *.tmp
cd ..
cbz_make $folder
}
s_batoto() {
o "[Batoto] If this so happens to fail, they changed their page generator again."
o -n "[Batoto] Scraping Chapters..."
wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1" -O scrape.htm > /dev/null 2>&1
o `file scrape.htm` | grep 'gzip' - > /dev/null 2>&1
RET=$?
if [ $RET = 0 ]; then # Yeah, got a gz.
n "  :/\b\b\b\b"
mv scrape.htm scrape.htm.gz
gunzip scrape.htm.gz
fi
grep -A 2 'Sort:' scrape.htm >> batch.txtr
sed -i "s|^[[:space:]]*</td>[[:space:]]*||g" batch.txtr
sed -i 's|^[[:space:]]*<td style="border-top:0;"><div title="||g' batch.txtr
sed -i 's|" style="display: inline-block; width:16px; height: 12px;.*$||g' batch.txtr
sed -i "s|<a href=\"||g" batch.txtr
sed -i "s|\" title=.*||g" batch.txtr
sed -i '/^[[:space:]]*$/d' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i 's|^--$||g' batch.txtr
sed -i '/^$/d' batch.txtr
tac batch.txtr > batch.txtf || tail -r batch.txtr > batch.txtf
if [ "$2" = "" ]; then
sed -ni '0~2p' batch.txtf
cat batch.txtf >> batch.txt
else
n "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Applying Language Filter '$2'..."
grep -A 1 "$2" batch.txtf > batch.txtf2
sed -i '/^[[:space:]]*--[[:space:]]*$/d' batch.txtf2
sed -i '/^$/d' batch.txtf2
sed -ni '0~2p' batch.txtf2
cat batch.txtf2 >> batch.txt
fi
rm scrape.htm batch.txtr batch.txtf*
en "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
for ((n=0;n < ${#2}; n++)); do
en '\b'
done
e " Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
dynsc_l="Dynasty Scans"
dynsc_u="http://dynasty-scans.com/"
a_dynsc() {
if [ -n "`echo $1 | grep 'dynasty-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_dynsc() {
folder="`echo $1 | sed -re 's/^.+\///'`"
mkdir -p $folder
cd $folder
PAGEDATA=`wget --quiet $1 -O - | grep "var pages" -`
PAGETMP=`echo $PAGEDATA | sed -e "s/\"image\"\://g" | sed -e "s/,\"name\"\:\"[[:alnum:]_-]*\"//g" | sed -e "s/\}\]/\)/g" | sed -e "s/{//g" | sed -e "s/}//g" | sed -e "s/;//g" | sed -e "s/ //g" | sed -e "s/varpages=\[/pages=\(/g" | sed -e "s/,/ /g"`
eval $PAGETMP
o -n "[DynastyScans] Downloading '$folder' "
for image in "${pages[@]}"; do
wget --no-cache -nc -t 0 -w 30 "http://dynasty-scans.com$image" > /dev/null 2>&1
ss
done
se
cd ..
cbz_make $folder
}
s_dynsc() {
o -n "[DynastyScans] Scraping Chapters..."
wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1" -O scrape.htm > /dev/null 2>&1
grep 'class="name"' scrape.htm > batch.txtf
sed -i 's|^.*href="||g' batch.txtf
sed -i 's|" class=.*||g' batch.txtf
sed -i "s/^[[:space:]]*//" batch.txtf
sed -i "s/[[:space:]]*$//" batch.txtf
sed -i "s|^|http://dynasty-scans.com|g" batch.txtf
cat batch.txtf >> batch.txt
rm scrape.htm batch.txtf
e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[DynastyScans] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
eh_l="E-Hentai"
eh_u="http://e-hentai.org/"
a_eh() {
if [ -n "`echo $1 | grep 'e-hentai.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_eh() {
e "[e-h] Warning, this is experimental and subject to random"
e "[e-h] failures. I think some of the 'nodes' don't respond"
e "[e-h] properly, so if an image fails; open a browser and click"
e "[e-h] the image not working button repeatedly. You should"
e "[e-h] get the fallback server next try."
sitepage=$1
wget --quiet $sitepage -O tmp.1
folder="`cat tmp.1 | grep 'title>' - | sed 's/<title>//g' | sed 's/<\/title>//g' | sed 's/E-Hentai Galleries/Rip/g' | sed 's|/|_|g'`"
mkdir -p "$folder"
cd "$folder"
DATA=`cat ../tmp.1 | sed 's/0 no-repeat\"><a href=\"/\nurl:/g' | sed 's/"><img alt.*//g' | grep 'url:'`
rm ../tmp.1
eval "urls=(`echo $DATA | tr '\n' ' ' | sed 's/url://g'`)"
page="${urls[0]}"
o -n "[e-h] Downloading '$folder' "
doneyet=0
while [ $doneyet == 0 ]; do
get=($(wget --quiet --no-cache -nc -t 0 -w 30 "$page" -O - | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 'keystamp' -))
extra=""
if [ "${get[1]}" == "" ]; then
get=($(wget --quiet --no-cache -nc -t 0 -w 30 "$page" -O - | tr '<' '\n' | grep -e 'a href' -e 'img src' - | sed 's/a href="//g' | sed 's/img src="//g' | sed 's/".*//g' | grep -B1 -A0 '.php?f=' -))
extra="-O $(echo ${get[1]} | sed 's/.*n=//')"
fi
if [ "${get[0]}" == "$page" ]; then
doneyet=1
fi
wget --no-cache --quiet -nc -t 0 -w 30 "${get[1]}" $extra
if [ ! "${get[0]}" == "" ]; then
page="${get[0]}"
fi
ss
done
se
cd ..
cbz_make "$folder"
}
s_eh() {
o -n "[e-h] This isn't supported, considering there's really zero categorization here."
}
fakku_l="FAKKU"
fakku_u="https://fakku.net/"
a_fakku() {
if [ -n "`echo $1 | grep 'fakku.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_fakku() {
folder="`echo $1 | sed -re 's/^.+\///'`"
mkdir -p $folder
cd $folder
wget "$1/read" -O page.htm > /dev/null 2>&1
o `grep "window.params.thumbs =" page.htm` > tmp.1
sed -i 's/\\\//\//g' tmp.1
sed -i 's/\];/)/g' tmp.1
sed -i 's/window.params.thumbs = \[/pages=(/g' tmp.1
sed -i 's/\.thumb//g' tmp.1
sed -i 's/","/" "/g' tmp.1
sed -i 's/\/thumbs\//\/images\//g' tmp.1
printf "`cat tmp.1`" > tmp.1
. tmp.1
o -n "[Fakku] Downloading '$folder' "
for image in "${pages[@]}"; do
wget --no-cache -nc -t 0 -w 30 "https:$image"
ss
done
se
rm tmp.1
rm page.htm
cd ..
cbz_make $folder
}
s_fakku() {
o -n "[Fakku] Scraping Chapters..."
wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1" -O scrape.htm > /dev/null 2>&1
grep 'class="content-title"' scrape.htm > batch.txtf
sed -i 's|^.*href="||g' batch.txtf
sed -i 's|" title=.*||g' batch.txtf
sed -i "s/^[[:space:]]*//" batch.txtf
sed -i "s/[[:space:]]*$//" batch.txtf
sed -i "s|^|https://fakku.net|g" batch.txtf
cat batch.txtf >> batch.txt
rm scrape.htm batch.txtf
e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Fakku] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
foolsl_l="FoolSlide"
foolsl_u="Generic, n/a."
a_foolsl() {
if [ -n "`echo $1 | grep 'foolrulez.org/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
elif [ -n "`echo $1 | grep 'vortex-scans.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_foolsl() {
LAZYURL=`echo $1 | sed "s|/read/|/download/|g"`
o -n "[FoolSlide] Attempting Lazy Download "
FAILED=0
wget --content-disposition --no-cache -nc -t 0 -w 30 "$LAZYURL" > /dev/null 2>&1 || export FAILED=1
if [ $FAILED = 1 ]; then
o "[FAIL]"
o "[Foolslide] Downloading via alternate method "
else
o "[OK]"
fi
}
s_foolsl() {
o -n "[Foolslide] Scraping Chapters..."
wget --no-cache -nc -t 0 -w 30 "$1" -O tmp.htm > /dev/null 2>&1
grep '<div class="title"><a href=' tmp.htm > batch.txtr
sed -i 's|<div class="title"><a href="||g' batch.txtr
sed -i 's|" title=.*||g' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
tac batch.txtr >> batch.txt || cat batch.txtr >> batc.txt
rm tmp.htm batch.txtr
e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Foolslide] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
mpark_l="MangaPark"
mpark_u="http://mangapark.com/"
a_mpark() {
if [ -n "`echo $1 | grep 'mangapark.com/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_mpark() {
sitepage=$1
sitepage=`echo $sitepage | sed "s|/1$||g" | sed "s|/3-1$||g" | sed "s|/6-1$||g" | sed "s|/10-1$||g"`
folder="`echo $sitepage | sed -e "s|http\:\/\/||g" | sed -e "s|mangapark\.com\/manga\/||g" | sed -e "s|\/|-|g"`"
mkdir -p $folder
cd $folder
DATA=`wget --quiet $sitepage -O - | grep 'a target="_blank"' - | sed 's|<em><a target="_blank" href=||g'`
o "$DATA" > tmp.1
sed -i "s/ title=.*//" tmp.1
eval "pages=(`cat tmp.1 | tr '\n' ' '`)"
o -n "[Mangapark] Downloading '$folder' "
for image in "${pages[@]}"; do
wget --no-cache -nc -t 0 -w 30 "$image" > /dev/null 2>&1
ss
done
rm tmp.1
se
cd ..
cbz_make $folder
}
s_mpark() {
o -n "[Mangapark] Scraping Chapters..."
wget --user-agent="" --header "Accept-Encoding: identity" --no-cache -nc -t 0 -w 30 "$1" -O scrape.htm > /dev/null 2>&1
grep 'class="ch sts"' scrape.htm > batch.txtr
sed -i 's|^.*href="||g' batch.txtr
sed -i 's|">.*||g' batch.txtr
sed -i "s/^[[:space:]]*//" batch.txtr
sed -i "s/[[:space:]]*$//" batch.txtr
sed -i "s|^|http://mangapark.com|g" batch.txtr
sed -i "s|/1$||g" batch.txtr
sed -i "s|/3-1$||g" batch.txtr
sed -i "s|/6-1$||g" batch.txtr
sed -i "s|/10-1$||g" batch.txtr
tac batch.txtr >> batch.txt || cat batch.txtr >> batch.txt
rm scrape.htm batch.txtr
e "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[Mangapark] Scraped chapters to batch.txt. You can modify this, or pass it to autobatch."
}
mread_l="Mangareader"
mread_u="http://www.mangareader.net/"
a_mread() {
if [ -n "`echo $1 | grep 'mangareader.net/' | sed -e 's/^ *//' -e 's/[[:space:]]*$//'`" ]; then
return 1
fi
return 0
}
d_mread() {
folder="`echo $1 | sed -e "s|http\:\/\/||g" | sed -e "s|www\.mangareader\.net\/||g" | sed -e "s|\/|-|g"`"
mkdir -p $folder
cd $folder
o -n "[Mangareader] Blindly Downloading '$folder' "
IS_404=0
CUR=0
PAGES=0
while [ $IS_404 = 0 ]; do
CUR=$(( CUR + 1 ))
PAGEDATA=`wget --quiet --no-cache -nc -t 0 -w 30 "$1/$CUR" -O $CUR.htm`
IS_404=$?
if [ $IS_404 = 0 ]; then
src=`grep '<div id="imgholder">' $CUR.htm | sed "s/^.*src=\"//g" | sed "s/\" alt=.*//g"`
o $src
EXT="${src##*.}"
wget --no-cache -nc -t 0 -w 30 "$src" -O $CUR.$EXT > /dev/null 2>&1
ss
fi
done
PAGES=$(( CUR - 1 ))
se
rm *.htm
cd ..
cbz_make $folder
}
s_mread() {
o "[MangaReader] NYI, sorry."
}
if [ "$1" = "auto" -o "$1" = "a" ]; then # Common operation - Automatic Module Select.
for module in ${MODS[@]}; do
eval a_$module $2
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
eval d_$module $2
exit 0
fi
done
elif [ "$1" = "batch" -o "$1" = "l" ]; then # Common operation - typed batch.
IFS=$'\n' read -d '' -r -a LINES < $2
NEW=""
for chunk in "${LINES[@]}"; do
NEW="$NEW$0 $chunk ;"
done
eval $NEW
elif [ "$1" = "autobatch" -o "$1" = "b" ]; then # Common operation - auto batch.
IFS=$'\n' read -d '' -r -a LINES < $2
NEW=""
for chunk in "${LINES[@]}"; do
NEW="$NEW$0 auto $chunk ;"
done
eval $NEW
elif [ "$1" = "scrape" -o "$1" = "s" ]; then # Link scraper.
for module in ${MODS[@]}; do
eval a_$module $2
RETCHECK=$?
if [ $RETCHECK = 1 ]; then
eval s_$module $2 $3
exit 0
fi
done
else # Not a common operation - either invalid or a module-op.
MATCH=""
for module in ${MODS[@]}; do
if [ "$1" = "$module" ]; then
MATCH="d_$module $2"
fi
done
if [ "$MATCH" = "" ]; then # All checks failed. Usage~
type 1
e "Usage:"
type 0
n "\t$0\t"
color 3
n "OPERATION\t"
color 5
e "[PARAMS]"
type 0
type 1
e "Operations:"
type 0
color 3
e "\tauto (a)"
type 0
e "\t\tChooses module based on URL"
color 3
e "\tbatch (l)"
type 0
e "\t\tTakes a file with a list of types and URLs"
color 3
e "\tautobatch (b)"
type 0
e "\t\tTakes a file with URLs which will be run with auto."
color 3
e "\tscrape (s)"
type 0
e "\t\tWill take a manga's page and scrape chapters to"
e "\t\ta file named batch.txt"
o ""
e "\tYou can also specify a module name followed by"
e "\tthe URL instead of using the auto-detect."
type 1
e "Download Modules:"
type 0
for mod in "${MODS[@]}"; do
longname=$(temp=\$${mod}_l && eval echo $temp)
url=$(temp=\$${mod}_u && eval echo $temp)
n "\tModule Name:\t\t"
color 3
e "$mod"
type 0
n "\t\tLong Name:\t\t"
color 4
e "$longname"
type 0
n "\t\tSite(s) Used with:\t"
color 5
e "$url"
type 0
o ""
done
type 1
e "Misc Info"
type 0
e "\tIf you see an emote in the output, it means we had to deal"
e "\twith a retrieval quirk."
e "\n\t[ :/ ]\tGiven GZip'd data even though we said it wasn't"
e "\t\tsupported in the GET."
type 2
e "\t\tThis happens frequently with batoto when doing"
e "\t\tmultiple fetches. :/"
type 0
o ""
e "\tSome modules accept an extra parameter. Usually, this"
e "\tis a filter. Try values like 'English' or 'French'."
else # Module operation.
eval $MATCH
fi
fi
